{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n",
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('video_transcription/final_file')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "db = Chroma.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is backpropagation performed in pytorch?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = db.similarity_search(query,k=5)\n",
    "context = \"\"\n",
    "for i in range(5):\n",
    "    context += ans[i].page_content\n",
    "    context += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"and actually everything will just work\\nbecause the API that I've developed here\\nis identical to the API that pytorch\\nuses and the implementation also is\\nbasically as far as I'm aware identical\\nto the one in pi torch\\nand number three I try to introduce you\\nto the diagnostic tools that you would\\nuse to understand whether your neural\\nnetwork is in a good State dynamically\\nso we are looking at the statistics and\\nhistograms and activation of the forward\\npass application activations the\\nbackward pass gradients and then also\\nwe're looking at the weights that are\\ngoing to be updated as part of\\nstochastic already in ascent and we're\\nlooking at their means standard\\ndeviations and also the ratio of\\ngradients to data or even better the\\nupdates to data and we saw that\\ntypically we don't actually look at it\\nas a single snapshot Frozen in time at\\nsome particular iteration typically\\npeople look at this as uh over time just\\nlike I've done here and they look at\\nthese updated data ratios and they make\\nput it all together and this is the full\\ncode of training this two layer MLP and\\nwe're going to basically insert our\\nmanual back prop and we're going to take\\nout lost it backward and you will\\nbasically see that you can get all the\\nsame results using fully your own code\\nand the only thing we're using from\\npytorch is the torch.tensor to make the\\ncalculations efficient but otherwise you\\nwill understand fully what it means to\\nforward and backward and neural net and\\ntrain it and I think that'll be awesome\\nso let's get to it\\nokay so I read all the cells of this\\nnotebook all the way up to here and I'm\\ngoing to erase this and I'm going to\\nstart implementing backward pass\\nstarting with d lock problems so we want\\nto understand what should go here to\\ncalculate the gradient of the loss with\\nrespect to all the elements of the log\\nprops tensor\\nnow I'm going to give away the answer\\nhere but I wanted to put a quick note\\nhere that I think would be most\\npedagogically useful for you is to\\nbefore we do lost that backward we need\\none more thing if you remember from\\nmicrograd\\npytorch actually requires\\nthat we pass in requires grad is true\\nso that when we tell\\npythorge that we are interested in\\ncalculating gradients for this leaf\\ntensor by default this is false\\nso let me recalculate with that\\nand then set to none and lost that\\nbackward\\nnow something magical happened when\\nlasted backward was run\\nbecause pytorch just like micrograd when\\nwe did the forward pass here\\nit keeps track of all the operations\\nunder the hood it builds a full\\ncomputational graph just like the graphs\\nwe've\\nproduced in micrograd those graphs exist\\ninside pi torch\\nand so it knows all the dependencies and\\nall the mathematical operations of\\neverything\\nand when you then calculate the loss\\nwe can call a dot backward on it\\nand that backward then fills in the\\ngradients of\\nall the intermediates\\nall the way back to w's which are the\\nparameters of our neural net so now we\\ncan do w grad and we see that it has\\nwe want to back prop into logits through\\nthis second branch\\nnow here of course we took legits and we\\ntook the max along all the rows and then\\nwe looked at its values here now the way\\nthis works is that in pytorch\\nthis thing here\\nthe max returns both the values and it\\nReturns the indices at which those\\nvalues to count the maximum value\\nnow in the forward pass we only used\\nvalues because that's all we needed but\\nin the backward pass it's extremely\\nuseful to know about where those maximum\\nvalues occurred and we have the indices\\nat which they occurred and this will of\\ncourse helps us to help us do the back\\npropagation because what should the\\nbackward pass be here in this case we\\nhave the largest tensor which is 32 by\\n27 and in each row we find the maximum\\nvalue and then that value gets plucked\\nout into loaded Maxis and so intuitively\\num basically the derivative flowing\\nthrough here then should be one\\ntimes the look of derivatives is 1 for\\nthe appropriate entry that was plucked\\nout\\nwould encounter in pi torch so you'll\\nsee that I will structure our code into\\nthese modules like a linear module and a\\nbachelor module and I'm putting the code\\ninside these modules so that we can\\nconstruct neural networks very much like\\nwe would construct them in pytorch and I\\nwill go through this in detail so we'll\\ncreate our neural net\\nthen we will do the optimization loop as\\nwe did before\\nand then the one more thing that I want\\nto do here is I want to look at the\\nactivation statistics both in the\\nforward pass and in the backward pass\\nand then here we have the evaluation and\\nsampling just like before\\nso let me rewind all the way up here and\\ngo a little bit slower\\nso here I'm creating a linear layer\\nyou'll notice that torch.nn has lots of\\ndifferent types of layers and one of\\nthose layers is the linear layer\\nlinear takes a number of input features\\noutput features whether or not we should\\nhave bias and then the device that we\\nwant to place this layer on and the data\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "template=\"\"\"Please use the following context to answer questions.\n",
    "Context: {ans}\n",
    "Question: {query}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Backpropagation in pytorch is performed by calling the .backward() method on the loss tensor. Pytorch keeps track of all the operations under the hood and builds a full computational graph. When the loss is calculated, the .backward() method fills in the gradients of all the intermediates all the way back to the parameters of the neural network.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"ans\",\"query\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain.predict(ans = ans, query = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
