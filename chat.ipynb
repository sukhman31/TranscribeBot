{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from pyllamacpp.model import Model\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('audio.mp3.wav.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-02 23:20:38,164] {posthog.py:15} INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "[2023-05-02 23:20:38,169] {__init__.py:80} INFO - Running Chroma using direct local API.\n",
      "[2023-05-02 23:20:39,265] {__init__.py:49} WARNING - Using embedded DuckDB without persistence: data will be transient\n",
      "[2023-05-02 23:20:39,341] {ctypes.py:22} INFO - Successfully imported ClickHouse Connect C data optimizations\n",
      "[2023-05-02 23:20:39,357] {ctypes.py:31} INFO - Successfully import ClickHouse Connect C/Numpy optimizations\n",
      "[2023-05-02 23:20:39,371] {json_impl.py:45} INFO - Using python library for writing JSON byte strings\n",
      "[2023-05-02 23:20:39,557] {Collection.py:52} WARNING - No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n",
      "[2023-05-02 23:20:47,225] {SentenceTransformer.py:66} INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "[2023-05-02 23:20:49,544] {SentenceTransformer.py:105} INFO - Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3c299d2eab49b3aa2c5dd77ecd91bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db = Chroma.from_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11100d84acf48a78d25546332bd88dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is Bach\"\n",
    "ans = db.similarity_search(query,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demos that have been released called Bach.\n"
     ]
    }
   ],
   "source": [
    "print(ans[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#Download the model\n",
    "hf_hub_download(repo_id=\"LLukas22/gpt4all-lora-quantized-ggjt\", filename=\"ggjt-model.bin\", local_dir=\".\")\n",
    "\n",
    "#Load the model\n",
    "model = Llama(model_path=\"ggjt-model.bin\")\n",
    "\n",
    "#Generate\n",
    "prompt=f'''Please use the following context to answer questions.\n",
    "Context: {ans}\n",
    "---\n",
    "Question: {query}\n",
    "Answer:'''\n",
    "\n",
    "result=model(prompt, max_tokens=100)\n",
    "#Print result with additional infos\n",
    "print(result)\n",
    "\n",
    "#Get only the generated text\n",
    "generated_text = result[\"choices\"][0][\"text\"]\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
