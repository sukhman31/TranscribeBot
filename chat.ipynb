{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n",
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader('video_transcription/final_file')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "db = Chroma.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the maths behind a neural network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = db.similarity_search(query,k=5)\n",
    "context = \"\"\n",
    "for i in range(5):\n",
    "    context += ans[i].page_content\n",
    "    context += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"an input and they take the weights of a\\nneural network as an input and it's a\\nmathematical expression and the output\\nare your predictions of your neural net\\nor the loss function we'll see this in a\\nbit but basically neural networks just\\nhappen to be a certain class of\\nmathematical expressions\\nbut back propagation is actually\\nsignificantly more general it doesn't\\nactually care about neural networks at\\nall it only tells us about arbitrary\\nmathematical expressions and then we\\nhappen to use that machinery for\\ntraining of neural networks now one more\\nnote i would like to make at this stage\\nis that as you see here micrograd is a\\nscalar valued auto grant engine so it's\\nworking on the you know level of\\nindividual scalars like negative four\\nand two and we're taking neural nets and\\nwe're breaking them down all the way to\\nthese atoms of individual scalars and\\nall the little pluses and times and it's\\njust excessive and so obviously you\\nwould never be doing any of this in\\nokay so now that we have some machinery\\nto build out pretty complicated\\nmathematical expressions we can also\\nstart building out neural nets and as i\\nmentioned neural nets are just a\\nspecific class of mathematical\\nexpressions\\nso we're going to start building out a\\nneural net piece by piece and eventually\\nwe'll build out a two-layer multi-layer\\nlayer perceptron as it's called and i'll\\nshow you exactly what that means\\nlet's start with a single individual\\nneuron we've implemented one here but\\nhere i'm going to implement one that\\nalso subscribes to the pytorch api in\\nhow it designs its neural network\\nmodules\\nso just like we saw that we can like\\nmatch the api of pytorch\\non the auto grad side we're going to try\\nto do that on the neural network modules\\nso here's class neuron\\nand just for the sake of efficiency i'm\\ngoing to copy paste some sections that\\nare relatively straightforward\\nso the constructor will take\\nnumber of inputs to this neuron which is\\nhow many inputs come to a neuron so this\\nvery well and we were only able to get\\naway with it because\\nthe problem is very simple\\nso let's now bring everything together\\nand summarize what we learned\\nwhat are neural nets neural nets are\\nthese mathematical expressions\\nfairly simple mathematical expressions\\nin the case of multi-layer perceptron\\nthat take\\ninput as the data and they take input\\nthe weights and the parameters of the\\nneural net mathematical expression for\\nthe forward pass followed by a loss\\nfunction and the loss function tries to\\nmeasure the accuracy of the predictions\\nand usually the loss will be low when\\nyour predictions are matching your\\ntargets or where the network is\\nbasically behaving well so we we\\nmanipulate the loss function so that\\nwhen the loss is low the network is\\ndoing what you want it to do on your\\nproblem\\nand then we backward the loss\\nuse backpropagation to get the gradient\\nand then we know how to tune all the\\nparameters to decrease the loss locally\\nbut then we have to iterate that process\\ndark arts back propagation and then have\\nan intuitive sense of how these\\ngradients flow through a neural net you\\nare looking at your distribution of 10h\\nactivations here and you are sweating\\nso let me show you why we have to keep\\nin mind that during that propagation\\njust like we saw in micrograd we are\\ndoing backward pass starting at the loss\\nand flowing through the network\\nbackwards in particular we're going to\\nback propagate through this tors.10h\\nand this layer here is made up of 200\\nneurons for each one of these examples\\nand it implements an element twice 10 H\\nso let's look at what happens in 10h in\\nthe backward pass\\nwe can actually go back to our previous\\nmicrograd code in the very first lecture\\nand see how we implement the 10h\\nwe saw that the input here was X and\\nthen we calculate T which is the 10h of\\nx\\nso that's T and T is between negative 1\\nand 1. it's the output of the 10h and\\nthen in the backward pass how do we back\\npropagate through a 10 h\\nwe take out that grad\\nthey are scalar valued along the way\\nand we can do this forward pass\\nand build out a mathematical expression\\nso we have multiple inputs here a b c\\nand f\\ngoing into a mathematical expression\\nthat produces a single output l\\nand this here is visualizing the forward\\npass so the output of the forward pass\\nis negative eight that's the value\\nnow what we'd like to do next is we'd\\nlike to run back propagation\\nand in back propagation we are going to\\nstart here at the end and we're going to\\nreverse\\nand calculate the gradient along along\\nall these intermediate values\\nand really what we're computing for\\nevery single value here\\num we're going to compute the derivative\\nof that node with respect to l\\nso\\nthe derivative of l with respect to l is\\njust uh one\\nand then we're going to derive what is\\nthe derivative of l with respect to f\\nwith respect to d with respect to c with\\nrespect to e\\nwith respect to b and with respect to a\\nand in the neural network setting you'd\\nbe very interested in the derivative of\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "template=\"\"\"Please use the following context to answer questions.\n",
    "Context: {ans}\n",
    "Question: {query}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Neural networks are mathematical expressions that take input as data and weights and parameters of the neural net. The output of the neural network is a prediction or a loss function. Backpropagation is used to calculate the gradient of the loss function with respect to the parameters of the neural network. This allows us to tune the parameters of the neural network to decrease the loss locally.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"ans\",\"query\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain.predict(ans = context, query = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
