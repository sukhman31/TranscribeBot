 Hello, my name is Andre and I've been training deep neural networks for a bit more than a decade and in this lecture
 I'd like to show you what neural network training looks like under the hood. So in particular
 we are going to start with a blank jubyter notebook and by the end of this lecture
 we will define and train in your own that in order to see everything that goes on under the hood and exactly
 sort of how that works in an intuitive level. Now specifically what I would like to do is I would like to take you through
 building of micrograd. Now micrograd is this library that I released on github about two years ago
 but at the time I only uploaded this source code and you'd have to go and buy yourself and really
 figure out how it works. So in this lecture I will take you through it step by step and kind of
 kind of call it in all the pieces of it. So what's micrograd and why is it interesting?
 Micrograd is basically an autograd engine. Autograd is short for automatic gradient and really
 what it does is it implements backpropagation. Now backpropagation is this algorithm that allows
 you to efficiently evaluate the gradient of some kind of a loss function with respect to the weights
 of a neural network and what that allows us to do then is we can iteratively tune the weights of
 that neural network to minimize the loss function and therefore improve the accuracy of the network.
 So backpropagation would be at the mathematical core of any modern deep neural network library
 like say PyTorch or Jax. So the functionality of micrograd is I think best illustrated by an
 example. So if we just scroll down here you'll see that micrograd basically allows you to build
 out mathematical expressions and here what we are doing is we have an expression that we're
 building out where you have two inputs A and B and you'll see that A and B are negative four and two
 but we are wrapping those values into this value object that we are going to build out as part of
 micrograd. So this value object will wrap the numbers themselves and then we are going to build
 out a mathematical expression here where A and B are transformed into C, D and eventually E, F and G
 and I'm showing some of the functionality of micrograd and the operations that it supports.
 So you can add two value objects, you can multiply them, you can raise them to a constant power,
 you can offset by one, negate, squash at zero, square, divide by constant, divide by hit, etc.
 And so we're building out an expression graph with these two inputs A and B and we're creating
 an output value of G and micrograd will in the background build out this entire mathematical
 expression. So it will for example know that C is also a value, C was a result of an addition
 operation and the child nodes of C are A and B because the and all maintain pointers to A and B
 value objects. So we'll basically know exactly how all of this is laid out. And then not only can
 we do what we call the forward pass where we actually look at the value of G of course that's
 pretty straightforward. We will access that using the dot data attribute. And so the output of the
 forward pass, the value of G is 24.7, it turns out. But the big deal is that we can also take this
 G value object and we can call dot backward. And this will basically initialize back propagation
 at the node G. And what back propagation is going to do is it's going to start at G and it's going
 to go backwards through that expression graph, and it's going to recursively apply the chain rule
 from calculus. And what that allows us to do then is we're going to evaluate basically the
 derivative of G with respect to all the internal nodes, like Ed and C, but also with respect to
 the inputs, a and B. And then we can actually query this derivative of G with respect to A,
 for example, that's a dot grad. In this case, it happens to be 138. And the derivative of G with
 respect to B, which also happens to be here 645. And this derivative we'll see soon is very
 important information, because it's telling us how a and B are affecting G through this mathematical
 expression. So in particular, a dot grad is 138. So if we slightly nudge a and make it slightly larger,
 138 is telling us that G will grow. And the slope of that growth is going to be 138.
 And the slope of growth of B is going to be 645. So that's going to tell us about how G will
 respond if A and B get tweaked a tiny amount in a positive direction. Okay. Now you might be
 confused about what this expression is that we built out here. And this expression, by the way,
 is completely meaningless. I just made it up. I'm just flexing about the kinds of operations
 that are supported by micrograd. What we actually really care about are neural networks.
 But it turns out that neural networks are just mathematical expressions, just like this one,
 but actually slightly bit less crazy even. Neural networks are just a mathematical expression.
 They take the input data as an input, and they take the weights of a neural network as an input.
 And some mathematical expression and the output are your predictions of your neural net or the
 loss function. We'll see this in a bit. But basically neural networks just happen to be a
 certain class of mathematical expressions. But back propagation is actually significantly more
 general. It doesn't actually care about neural networks at all. It only tells about arbitrary
 mathematical expressions. And then we happen to use that machinery for training of neural
 networks. Now one more note I would like to make at the stage is that as you see here,
 micrograd is a scalar valued autograd engine. So it's working on the level of individual scalars
 like negative four and two. And we're taking neural nets and we're breaking them down all the
 way to these atoms of individual scalars and all the little pluses and times. And it's just
 excessive. And so obviously you would never be doing any of this in production. It's really just
 put down for pedagogical reasons because it allows us to not have to deal with these
 and dimensional tensors that you would use in modern deep neural network library. So this is
 really done so that you understand and refactor out back propagation and chain rule and understanding
 of your training. And then if you actually want to train bigger networks, you have to be using
 these tensors, but none of the math changes. This is done purely for efficiency. We are basically
 taking scale value, all the scale values. We're packaging them up into tensors, which are just
 arrays of these scalars. And then because we have these large arrays, we're making operations on
 those large arrays that allows us to take advantage of the parallelism in a computer. And all those
 operations can be done in parallel. And then the whole thing runs faster. But really none of the
 math changes and that's done purely for efficiency. So I don't think that it's pedagogically useful
 to be dealing with tensors from scratch. And I think, and that's why I fundamentally wrote
 micrograd, because you can understand how things work at the fundamental level. And then you can
 speed it up later. Okay, so here's the fun part. My claim is that micrograd is what you need to
 train neural networks and everything else is just efficiency. So you'd think that micrograd would be
 a very complex piece of code. And that turns out to not be the case. So if we just go to micrograd,
 and you will see that there's only two files here in micrograd, this is the actual engine.
 It doesn't know anything about neural nets. And this is the entire neural nets library
 on top of micrograd. So engine and and and dot pi. So the actual back propagation autograd engine
 that gives you the power of neural networks is literally
 a hundred lines of code of like very simple Python, which we'll understand by the end of this lecture.
 And then and and dot pi, this neural network library built on top of the autograd engine,
 um, is like a joke. It's like we have to define what is a neuron, and then we have to define what
 is a layer of neurons. And then we define what is a multilateral perceptron, which is just a
 sequence of layers of neurons. And so it's just a total joke. So basically, um, there's a lot of
 power that comes from only 150 lines of code. And that's only need to understand to understand
 neural network training and everything else is just efficiency. And of course, there's a lot
 too efficiency. But fundamentally, that's all that's happening. Okay, so now let's dive right in and
 implement micro grad step by step. The first thing I'd like to do is I'd like to make sure that you
 have a very good understanding intuitively of what a derivative is and exactly what information
 it gives you. So let's start with some basic imports that I copy based in every Jupyter Notebook
 always. And let's define the function, scalar value function f of x as follows. So I just make
 this up randomly. I just wanted a scalar value function that takes a single scalar x and returns
 a single scalar y. And we can call this function, of course, so we can pass in say 3.0 and get 20
 back. Now we can also plot this function to get a sense of its shape. You can tell from the
 mathematical expression that this is probably a parabola, it's a quadratic. And so if we just
 create a set of, um, sk, scalar values that we can feed in using, for example, a range from
 negative five to five and steps up point two five. So this is, so x is just from negative five to five,
 not including five in steps of point two five. And we can actually call this function on this
 non-py array as well. So we get a set of y's if we call f on x's. And these y's are basically
 also applying, um, function on every one of these elements independently. And we can plot this
 using matplotlib. So the ulti dot plot x is in y's. And we get a nice parabola. So previously
 here we fed in 3.0 somewhere here, and we received 20 back, which is here the y coordinate. So now
 I'd like to think through what is the derivative of this function at any single input point x.
 Right. So what is the derivative at different points x of this function? Now if you remember
 back to your calculus class, you've probably derived derivatives. So we take this mathematical
 expression 3x square minus 4x plus five, and you would write out on a piece of paper and you would,
 you know, apply the product rule and all the other rules and derive the mathematical expression
 of the great derivative of the original function. And then you could plug in different taxes and
 see what the derivative is. We're not going to actually do that because no one in neural networks
 actually writes out the expression for neural net. It would be a massive expression. It would be,
 you know, thousands since thousands of terms. No one actually derives the derivative, of course.
 And so we're not going to take this kind of symbolic approach. Instead, what I'd like to do is I'd
 like to look at the definition of derivative and just make sure that we really understand
 what derivative is measuring, what is telling you about the function. And so if we just look up
 derivative, we see that. Okay. So this is not a very good definition of derivative. This is a
 definition of what it means to be differentiable. But if you remember from your calculus, it is
 the limit as h goes to zero of f of x plus h minus f of x over h. So basically what it's saying is
 if you slightly bump up, you're at some point x that you're interested in, or hey, and if you
 slightly bump up, you know, you slightly increase it by small number h. How does the function respond
 with what sensitivity does it respond? Where does the slope at that point? Does the function go up
 or does it go down and by how much? And that's the slope of that function, the slope of that response.
 At that point. And so we can basically evaluate the derivative here, numerically, by taking a very
 small h, of course, the definition would ask us to take h to zero, we're just going to pick a very
 small h 0.001. And let's say we're interested in 0.3.0. So we can look at f of x, of course, is 20.
 And now f of x plus h. So if we slightly nudge x in a positive direction, how is the function
 going to respond? And just looking at this, do you expect f of x plus h to be slightly greater than 20,
 or do you expect to be slightly lower than 20? And since this three is here, and this is 20,
 if we slightly go positively, the function will respond positively. So you'd expect this to be
 slightly greater than 20. And by how much is telling you the sort of the strength of that slope,
 right, the size of the slope. So f of x plus h from f of x, this is how much the function
 responded in the positive direction. And we have to normalize by the run. So we have the rise over
 run to get the slope. So this, of course, is just numerical approximation of the slope,
 because we have to make a very, very small to converge to the exact amount. Now, if I'm doing
 too many zeros, at some point, I'm going to get an incorrect answer, because we're using floating
 point arithmetic. And the representations of all these numbers in computer memory is finite,
 and at some point we get into trouble. So we can converge towards the right answer with this approach.
 But basically, at three, the slope is 14. And you can see that by taking 3x squared minus 4x plus 5,
 and differentiating it in our head. So 3x squared would be 6x minus 4. And then we plug in x equals
 3. So that's 18 minus 4 is 14. So this squared. So that's at three. Now how about the slope at,
 say, negative three? Would you expect, what would you expect for the slope? Now, telling the exact
 value is really hard. But what is the sign of that slope? So at negative three, if we slightly go in
 the positive direction at x, the function would actually go down. And so that tells you that the
 slope would be negative. So we'll get a slight number below 20. And so if we take the slope,
 we expect something negative, negative 22. Okay. And at some point here, of course, the slope would
 be zero. Now for this specific function, I looked it up previously, and it's at point two over three.
 So at roughly two over three, that's somewhere here, this derivative would be zero. So basically,
 at that precise point, yeah, at that precise point, if we nudge in a positive direction,
 the function doesn't respond, this stays the same almost. And so that's why the slope is zero.
 Okay, now let's look at a bit more complex case. So we're going to start, you know,
 complexifying a bit. So now we have a function here, with output variable d,
 there is a function of three scalar inputs, a, b and c. So a, b and c are some specific values,
 three inputs into our expression graph, and a single output d. And so if we just print d,
 we get four. And now what I have to do is I'd like to again look at the derivatives of d
 with respect to a, b and c. And think through, again, just the intuition of what this derivative
 is telling us. So in order to evaluate this derivative, we're going to get a bit tacky here.
 We're going to again have a very small value of h. And then we're gonna fix the inputs at some
 values that we're interested in. So these are the, this is the point a, b, c, at which we're going
 to be evaluating the derivative of d with respect to all a, b and c at that point. So there are
 the inputs. And now we have d one is that expression. And then we're going to, for example, look at
 the derivative of d with respect to a. So we'll take a and we'll bump it by h. And then we'll get
 d two to be the exact same function. And now we're going to print, you know, fun, f one, d one is d
 one, d two is d two, and print slope. So the derivative or slope here will be, of course,
 d two minus d one divided h. So d two minus d one is how much the function increased
 when we bumped the specific input that we're interested in by a tiny amount. And this is the
 normalized by h to get the slope. So yeah. So this, so I just from this, we're going to print
 d one, which we know is four. Now d two will be bumped a will be bumped by h. So let's just think
 through a little bit, what d two will be printed out here. In particular, d one will be four,
 will d two be a number slightly greater than four or slightly lower than four. And it's going to
 tell us the the sign of the derivative. So we're bumping a by h, b is minus three, c is 10. So you
 can just intuitive think through this derivative and what it's doing, a will be slightly more positive.
 And but b is a negative number. So if a is slightly more positive, because b is negative three,
 we're actually going to be adding less to d. So you'd actually expect that the value of the
 function will go down. So let's just see this. Yeah. And so we went from four to 3.9996. And that
 tells you that the slope will be negative. And then will be negative number, because we went down.
 And then the exact number of slope will be exact number of slope is negative three.
 And you can also convince yourself that negative three is the right answer mathematically and
 analytically, because if you have eight times B plus C, and you are, you know, you have calculus,
 then differentiating eight times B plus C with respect to a gives you just B. And indeed,
 the value of B is negative three, which is the derivative that we have. So you can tell that that's
 correct. So now if we do this with B, so if we bump B by a little bit in a positive direction,
 we'd get different slopes. So what is the influence of B on the output D? So if we bump B by
 tiny amount in a positive direction, then because A is positive, we'll be adding more to D.
 Right. So, and now what is the, what is the sensitivity? What is the slope of that addition?
 And it might not surprise you that this should be two. And why is it two? Because D of D by D B,
 differentiating with respect to B would be would give us A and the value of A is two.
 So that's also working well. And then if C gets bumped a tiny amount in H by H,
 then of course, A times B is unaffected. And now C becomes slightly bit higher. What does that do
 to the function? It makes it slightly bit higher, because we're simply adding C. And it makes a
 slightly bit higher by the exact same amount that we added to C. And so that tells you that the slope
 is one. That will be the, the rate at which D will increase as we scale C. Okay, so we now have some
 intuitive sense of what this derivative is telling you about the function. And we'd like to move to
 neural networks. Now, as I mentioned, neural networks will be pretty massive expressions,
 mathematical expressions. So we need some data structures that maintain these expressions. And
 that's what we're going to start to build out now. So we're going to build out this value object
 that I showed you in the read me page of micro grad. So let me copy paste a skeleton of the first
 very simple value object. So class value takes a single scalar value that it wraps and keeps track
 of. And that's it. So we can, for example, do value of 2.0, and then we can get, we can look at its
 content. And Python will internally use the wrapper function to return this string. So this is a
 value object with data equals two that we're creating here. Now we'd like to do is like, we'd
 like to be able to have not just like two values, but we'd like to do a wealthy, right, we'd like to
 add them. So currently, you get an error because Python doesn't know how to add two value objects.
 So we have to tell it. So here's addition. So you have to basically use these special double
 underscore methods in Python to define these operators for these objects. So if we call the,
 if we use this plus operator, Python will internally call a dot add of B. That's what will happen
 internally. And so B will be the other, and self will be a. And so we see that what we're going to
 return is a new value object. And it's just going to be wrapping the plus of their data. But remember
 now because data is the actual like numbered Python number. So this operator here is just
 the typical floating point plus addition now, it's not an addition of value objects. And we'll
 return a new value. So now a plus B should work. And it should print value of negative one,
 because that's two plus minus three. There we go. Okay, let's now implement multiply,
 just so we can recreate this expression here. So multiply, I think it won't surprise you,
 will be fairly similar. So instead of add, we're going to be using mall. And then here,
 of course, we want to do times. And so now we can create a C value object, which will be 10.0.
 And now we should be able to do a times B. Well, let's just do a times B first.
 That's value of negative six now. And by the way, I skipped over this a little bit.
 Suppose that I didn't have the wrapper function here, then it's just that you'll get some kind of
 an ugly expression. So what repar is doing is it's providing us a way to print out like a nicer
 looking expression in Python. So we don't just have something cryptic. We actually are, you know,
 it's value of negative six. So this gives us a times. And then this we should now be able to
 add C to it, because we've defined and told the Python how to do mall and add. And so this will
 call this will basically be equivalent to a dot mall of B. And then this new value object will be
 dot add of C. And so let's see if that work. Yep. So that worked. Well, that gave us four,
 which is what we expect from before. And I believe you can just call the manually as well.
 There we go. So yeah. Okay. So now what we are missing is the connected tissue of this expression.
 As I mentioned, we want to keep these expression graphs. So we need to know and keep pointers
 about what values produce what other values. So here, for example, we are going to
 introduce a new variable, which we'll call children. And by default, it will be an empty tuple. And
 then we're actually going to keep a slightly different variable in the class, which we'll call
 underscore private, which will be the set of children. This is how I done. I did it in the
 original micro grad looking at my code here. I can't remember exactly the reason I believe it was
 efficiency. But this underscore children will be a tuple for convenience. But then when we
 actually maintain it in the class, it will be just this set, I believe for efficiency. So now,
 when we are creating a value like this with a constructor, children will be empty and
 prep will be the empty set. But when we are creating a value through addition or multiplication,
 we're going to feed in the children of this value, which in this case is self another.
 So those are the children here. So now we can do D dot prep. And we'll see that the children of the
 we now know are this value of negative six and value of 10. And this, of course, is the value
 resulting from a times B and the C value, which is 10. Now the last piece of information we don't
 know. So we know that the children of every single value, but we don't know what operation
 created this value. So we need one more element here, let's call it underscore pop.
 And by default, this is the empty set for leaves. And then we'll just maintain it here.
 And now the operation will be just a simple string. And in the case of addition, it's plus
 in the case of multiplication is times. So now we not just have D dot prep, we also have a D dot
 up. And we know that D was produced by an addition of those two values. And so now we have the full
 mathematical expression. And we're building out this data structure. And we know exactly how each
 value came to be by word expression and from what other values. Now, because these expressions
 are about to get quite a bit larger, we'd like a way to nicely visualize these expressions that
 we're building out. So for that, I'm going to copy paste a bunch of slightly scary code
 that's going to visualize this, these expression graphs for us. So here's the code and I'll explain
 it in a bit. But first, let me just show you what this code does. Basically, what it does is it
 creates a new function draw dot that we can call on some root node. And then it's going to visualize
 it. So if we call draw dot on D, which is this final value here, that is a times B plus C.
 It creates something like this. So this is D. And you see that this is a times B creating an
 iterative value plus C gives us the output node D. So that's draw out of D. And I'm not going to go
 through this in complete detail. You can take a look at graphis and its API. Graphis is a open
 source graph visualization software. And what we're doing here is we're building out this graph in
 graphis API. And you can basically see that trace is this helper function that enumerates all the
 nodes and edges in the graph. So that just builds a set of all the nodes and edges. And then we
 iterate for all the nodes and we create special node objects for them in using dot node. And then
 we also create edges using dot dot edge. And the only thing that's like slightly tricky here is
 you'll notice that I basically add these fake nodes, which are these operation nodes. So for
 example, this node here is just like a plus node. And I create these special op nodes here. And I
 connect them accordingly. So these nodes, of course, are not actual nodes in the original graph.
 They're not actually a value object. The only value objects here are the things in squares.
 Those are actual value objects or representations thereof. And these op nodes are just created in
 this draw dot routine so that it looks nice. Let's also add labels to these graphs, just so we know
 what variables are where. So let's create a special underscore label. Or let's just do label
 equals empty by default, and save it to each node. And then here, we're going to do label is a
 label is the label is C. And then let's create a special equals a times B.
 And label will be. It's coming out. And E will be a plus C and a D dot label will be B.
 Okay, so nothing really changes. I just added this new function, new variable. And then here,
 when we are printing this, I'm going to print the label here. So this will be a percent S bar,
 and this will be end up labeled. And so now we have the label on the left here. So it says
 A be creating E, and then E plus C creates D, just like we have it here. And finally,
 let's make this expression just one layer deeper. So D will not be the final output node.
 Instead, after D, we are going to create a new value object called F. We're going to start
 running out of variables soon. F will be negative 2.0. And it's label. Of course, just D F. And then
 L capital L will be the output of our graph. And L will be P times F. Okay, so L will be negative
 8 is the output. So now we don't just draw a D, D draw L. Okay. And somehow the label of
 L is undefined. Oops. Although I will have to be explicitly given to it. There we go. So L is the
 output. So let's quickly recap what we've done so far. We are able to build out mathematical
 expressions using only plus and times so far. They are scalar valued along the way. And we can do
 this forward pass and build out a mathematical expression. So we have multiple inputs here,
 A, B, C, and F, going into a mathematical expression that produces a single output L.
 And this here is just rising the forward pass. So the output of the forward pass is negative 8.
 That's the value. Now what we'd like to do next is we'd like to run back propagation. And in
 back propagation, we are going to start here at the end. And we're going to reverse and calculate
 the gradient along along all these intermediate values. And really what we're computing for every
 single value here, we're going to compute the derivative of that node with respect to L.
 So the derivative of L with respect to L is just one. And then we're going to derive what is the
 derivative of L with respect to F with respect to D with respect to C with respect to E with
 respect to B and with respect to A. And in neural network setting, it'd be very interested in the
 derivative of basically this loss function L with respect to the weights of a neural network.
 And here of course we have just these variables A, B, C, and F. But some of these will eventually
 represent the weights of a neural net. And so we'll need to know how those weights are impacting
 the loss function. So we'll be interested basically in the derivative of the output
 with respect to some of its leaf nodes. And those leaf nodes will be the weights of the neural
 net. And the other leaf nodes of course will be the data itself. But usually we will not want or
 use the derivative of the loss function with respect to data, because the data is fixed.
 But the weights will be iterated on using the gradient information. So next we are going to
 create a variable inside the value class that maintains the derivative of L with respect to that
 value. And we will call this variable grad. So there's a data and there's a self that grad.
 And initially it will be zero. And remember that zero is basically means no effect. So at
 initialization we're assuming that every value does not impact does not affect the output.
 Right, because if the gradient zero, that means that changing this variable is not changing the
 loss function. So by default, we assume that the gradient is zero. And then now that we have grad,
 and it's 0.0, we are going to be able to visualize it here after data. So here grad is 0.4.
 And this will be in that grad. And now we are going to be showing both the data and the grad
 and initialize that zero. And we are just about getting ready to calculate the back
 propagation. And of course, this grad again, as I mentioned, is representing the derivative of
 the output. In this case, L with respect to this value. So with respect to, so this is the
 derivative of L with respect to F, respect to D and so on. So let's now fill in those gradients
 and actually do back propagation manually. So let's start filling in these gradients and start
 all the way at the end, as I mentioned here. First, we are interested to fill in this gradient here.
 So what is the derivative of L with respect to L? In other words, if I change L by a tiny amount
 H, how much does L change? It changes by H. So it's proportional and therefore the variable will be
 one. We can of course measure these or estimate these numerical gradients numerically, just like
 we've seen before. So if I take this expression and I create a def LLL function here, and let this
 here. Now the reason I'm creating a gating function LLL here is because I don't want to pollute or
 mess up the global scope here. This is just kind of like a little staging area. And as you know,
 in Python, all of these will be local variables to this function. So I'm not changing any of the
 global scope here. So here, L1 will be L. And then copy based on this expression, we're going to add
 a small amount H. In, for example, a, right, and this would be measuring the derivative of L with
 respect to a. So here, this will be L2. And then we want to print that derivative. So print L2
 minus L1, which is how much L changed, and then normalize it by H. So this is the rise over run.
 And we have to be careful because L is a value node. So we actually want its data.
 So that these are floats divided by H. And this should print the derivative of L with respect to
 A, because A is the one that we bumped a little bit by H. So what is the derivative of L with respect
 to a? It's six. Okay. And obviously, if we change L by H, then that would be here effectively.
 This looks really awkward, but changing L by H, you see the derivative here is one.
 That's kind of like the base case of what we are doing here. So basically, we can't come up here,
 and we can manually set L dot grad to one. This is our manual back propagation. L dot grad is one,
 and let's redraw. And we'll see that we filled in grad is one for L. We're now going to continue
 to back propagation. So let's here look at the derivatives of L with respect to D and F. Let's do
 a D first. So what we are interested in, if I create a markdown on here, is we'd like to know,
 basically, we have that L is D times F, and we'd like to know what is D L by D D.
 What is that? And if you know you're a calculus, L is D times F. So what is D L by D D? It would be F.
 And if you don't believe me, we can also just derive it because the proof would be fairly
 straightforward. We go to the definition of the derivative, which is F of X plus H minus F of X
 divided H as a limit of H goes to zero of this kind of expression. So when we have L is D times F,
 then increasing D by H would give us the output of D plus H times F. That's basically F of X plus H,
 minus D times F, and then divide H. And symbolically expanding out here, we would have basically D
 times F plus H times F minus D times F divided H. And then you see how the D F minus D F cancels.
 So you're left with H times F divided H, which is F. So in the limit, as H goes to zero of,
 derivative definition, we just get F in a case of D times F. So symmetrically, D L by D F
 will just be D. So what we have is that F dot grad we see now is just the value of D, which is four.
 And we see that D dot grad is just the value of F.
 And so the value of F is negative two. So we'll set those manually.
 Let me erase this markdown node, and then let's redraw what we have.
 Okay, and let's just make sure that these were correct. So we seem to think that D L by D D is
 negative two. So let's double check. Let me erase this plus H from before. And now we want to
 derivative with respect to F. So let's just come here when I create F and let's do a plus H here.
 And this should print a derivative of L with respect to F. So we expect to see four.
 Yeah, and this is four up to floating point funkiness. And then D L by D D should be F,
 which is negative two. grad is negative two. So if we again come here and we change D,
 D dot data plus equals H right here. So we expect, so we've added a little H and then we see how L
 changed. And we expect to print negative two. There we go.
 So we've numerically verified. What we're doing here is kind of like an inline gradient check.
 Gradient check is when we are deriving this like back propagation and getting the derivative
 with respect to all the intermediate results. And then numerical gradient is just, you know,
 estimating it using small step size. Now we're going to the crux back propagation. So this will
 be the most important node to understand, because if you understand the gradient for this node,
 you understand all of back propagation and all training of neural nets, basically.
 So we need to derive D L by D C. In other words, the derivative L with respect to C,
 because we've computed all these other gradients already. Now we're coming here and we're continuing
 the back propagation manually. So we want D L by D C, and then we'll also derive D L by D E.
 Now here's the problem. How do we derive D L by D C? We actually know the derivative L with respect
 to D. So we know how L is sensitive to D. But how is L sensitive to C? So if we wiggle C, how does
 that impact L through D? So we know D L by D C. And we also here know how C impacts D. And so
 just very intuitively, if you know the impact that C is having on D, and the impact that D is
 having on L, then you should be able to somehow put that information together to figure out how C
 impacts L. And indeed, this is what we can actually do. So in particular, we know just concentrating on
 D first. Let's look at how what is derivative basically of D with respect to C. So in other words,
 what is D D by D C? So here we know that D is C times C plus E. That's what we know. And now we're
 interested in D D by D C. If you just know your calculus again, and you remember that
 differentiating C plus E with respect to C, you know that that gives you 1 and 0. And we can also
 go back to the basics and derive this. Because again, we can go to our f of x plus H minus f of x
 derived by h. That's the definition of a derivative as h goes to 0. And so here, focusing on C and
 its effect on D, we can basically do the f of x plus h will be C is incremented by h plus E.
 That's the first evaluation of our function minus C plus E. And then divide h. And so what is this?
 Just expanding this out, this will be C plus H plus E minus C minus E divided h. And then you
 see here how C minus E cancels E minus E cancels were left with h over h, which is 1.0. And so
 by symmetry also, D D by D, E will be 1.0 as well. So basically the derivative of a sum
 expression is very simple. And this is the local derivative. So I call this the local derivative
 because we have the final output value all the way at the end of this graph. And we're now like
 a small node here. And this is a little plus node. And it, the little plus node, doesn't know anything
 about the rest of the graph that it's embedded in. All it knows is that it did it plus. It took
 a C and an E, added them and created a D. And this plus node also knows the local influence of C on D,
 rather than the derivative of D with respect to C. And it also knows the derivative of D with
 respect to E. But that's not what we want. That's just a local derivative. What we actually want
 is D L by D C. And L could L is here just one step away. But in a general case, this little plus
 node is could be embedded in like a massive graph. So again, we know how L impacts D. And now we know
 how C and E impact D. How do we put that information together to write D L by D C? And the answer,
 of course, is the chain rule in calculus. And so I pulled up a chain rule here from Kepedia.
 And I'm going to go through this very briefly. So chain rule, we could be there sometimes can
 be very confusing and calculus can can be very confusing. Like, this is the way I learned
 chain rule and was very confusing. Like what is happening? It's just complicated. So I like this
 expression much better. If a variable Z depends on a variable Y, which itself depends on a variable
 X, then Z depends on X as well, obviously, through the intermediate variable Y. And in this case,
 the chain rule is expressed as if you want D Z by DX, then you take the D Z by D Y and you multiply
 it by D Y by DX. So the chain rule fundamentally is telling you how we chain these derivatives
 together correctly. So to differentiate through a function composition, we have to apply a
 multiplication of those derivatives. So that's really what chain rule is telling us. And there's
 a nice little intuitive explanation here, which I also think is kind of cute. The chain rules
 says that knowing the instantaneous rate of change of Z with respect to Y and Y relative to X
 allows one to calculate the instantaneous rate of change of Z relative to X as a product of those
 two rates of change, simply the product of those two. So here's a good one. If a car travels twice
 as fast as bicycle, and the bicycle is four times as fast as walking men, then the car travels
 two times four, eight times as fast as the men. And so this makes it very clear that the correct
 thing to do sort of is to multiply. So cars twice as fast as bicycle and bicycle is four times as
 fast as men. So the car will be eight times as fast as the men. And so we can take these
 intermediate rates of change, if you will, and multiply them together. And that justifies the
 chain rule intuitively. So have a look at chain rule. But here, really what it means for us is
 there's a very simple recipe for deriving what we want, which is D L by D C. And what we have so far
 is we know one, and we know what is the impact of D on L. So we know D L by D D, the derivative
 L with respect to D D, we know that that's negative two. And now because of this local
 reasoning that we've done here, we know D D by D C. So how does C impact D? And in particular,
 this is a plus node. So the local derivative is simply 1.0. It's very simple. And so the chain
 rule tells us that D L by D C, going through this intermediate variable, will just be simply D L by
 D D times D D by D C. That's chain rule. So this is identical to what's happening here,
 except Z is R L, Y is R D, and X is R C. So we literally just have to multiply these. And because
 these local derivatives like D D by D C are just one, we basically just copy over D L by D D,
 because this is just times one. So what is it? So because D L by D D is negative two,
 what is D L by D C? Well, it's the local gradient 1.0 times D L by D D, which is negative two.
 So literally what a plus node does, you can look at it that way, is it literally just routes the
 gradient, because the plus nodes local derivatives are just one. And so in the chain rule, one times
 D L by D D is just D L by D D. And so that derivative just gets routed to both C
 and to E in the case. So basically, we have that E dot grad, or what's start with C, since that's
 the one we looked at, is negative two times one negative two. And in the same way, by symmetry,
 E dot grad will be negative two. Let's decline. So we can set those. We can redraw.
 And you see how we just assign negative to negative two. So this back propagating signal,
 which is carrying the information of like, what is the derivative of L with respect to all the
 intermediate nodes? We can imagine it almost like flowing backwards through the graph. And a plus
 node will simply distribute the derivative to all the leaf nodes, sorry, to all the children nodes
 of it. So this is the claim. And now let's verify it. So let me remove the plus H here from before.
 And now instead, what we're going to do is we want to incurrence C. So C data will be
 incremented by H. And when I run this, we expect to see negative two, negative two.
 And then of course, for E. So E data plus equals H, and we expect to see negative two. Simple.
 So those are the derivatives of these internal nodes. And now we're going to recurse our way
 backwards again. And we're again going to apply the chain rule. So here we go, our second application
 of chain rule. And we will apply it all the way through the graph, which just happened to only
 have one more node remaining. We have that D L by D E, as we have just calculated, is negative two.
 So we know that. So we know the derivative of L with respect to E. And now we want D L by D A,
 right? And the chain rule is telling us that that's just D L by D E,
 negative two, times the local gradient. So what is the local gradient? Basically D E by D A.
 We have to look at that. So I'm a little times node inside a massive graph. And I only know
 that I did A times B, and I produce an E. So now what is D E by D A, and D E by D B? That's the
 only thing that I sort of know about. That's my local gradient. So because we have that E is A times
 B, we're asking what is D E by D A. And of course, we just did that here. We had a times, so I'm not
 going to redrive it. But if you want to differentiate this with respect to A, you'll just get B, right?
 The value of B, which in this case is negative three point zero. So basically, we have that D L
 by D A. Well, let me just do it right here. We have that A dot grad. And we are applying chain
 rule here is D L by D E, which we see here is negative two times what is D E by D A. It's the
 value of B, which is negative three. That's it. And then we have B dot grad is again D L by D E,
 which is negative two, just the same way, times what is D E by D D B is the value of A, which is
 two dot two point zero, as the value of A. So these are our claimed derivatives. Let's redraw.
 And we see here that A dot grad turns out to be six because that is negative two times negative
 three. And B dot grad is negative four times, sorry, is negative two times two, which is negative four.
 So those are our claims. Let's delete this and let's verify them. We have A here, A dot data plus
 equals H. So the claim is that A dot grad is six. Let's verify six. And we have B data plus equals
 H. So nudging B by H and looking at what happens, we claim it's negative four. And indeed, it's
 negative four plus minus again, float, oddness. And that's it. This, that was the manual back
 propagation all the way from here to all the leaf nodes. And I'm done at piece by piece. And
 really all we've done is, as you saw, we iterated through all the nodes one by one and locally
 applied the chain rule. We always know what is the derivative of L with respect to this little output.
 And then we look at how this output was produced. This output was produced through some operation.
 And we have the pointers to the children nodes of this operation. And so in this little operation,
 we know what the local derivatives are. And we just multiply them onto the derivative always.
 So we just go through and recursively multiply on the local derivatives. And that's what
 back propagation is, is just a recursive application of chain rule backwards through the computation
 graph. Let's see this power in action, just very briefly. What we're going to do is we're going to
 notch our inputs to try to make L go up. So in particular, what we're doing is we want A dot
 data, we're going to change it. And if you want L to go up, that means we just have to go in the
 direction of the gradient. So A should increase in the direction of gradient,
 by like some small step amount. This is the step size. And we don't just want this for B, but also for B.
 Also for C. Also for F. Those are leaf nodes, which we usually have control over. And if we
 notch in direction of the gradient, we expect a positive influence on L. So we expect L to go up
 positively. So it should become less negative, it should go up to say negative, you know, six or
 something like that. It's hard to tell exactly. And we'd have to rerun the forward pass. So let me just
 do that here. This would be the forward pass. F would be unchanged. This is effectively the
 forward pass. And now if we print L dot data, we expect, because we nudged all the values,
 all the inputs in the direction of gradient, we expected a less negative L, we expected to go up.
 So maybe it's negative six or so, let's see what happens. Okay, negative seven. And this is
 basically one step of an optimization that will end up running. And really, this gradient just
 give us some power, because we know how to influence the final outcome. And this will be
 extremely useful for training, you know, that's as well as CMC. So now I would like to do one more
 example of manual backpropagation using a bit more complex and useful example. We are going to
 back propagate through a neuron. So we want to eventually build out neural networks. And in the
 simplest case, these are multilateral perceptrons, as they're called. So this is a two layer neural
 nut. And it's got these hidden layers made up of neurons. And these neurons are fully connected to
 each other. Now biologically, neurons are very complicated devices, but we have very simple
 mathematical models of them. And so this is a very simple mathematical model of a neuron.
 You have some inputs, axis. And then you have these synapses that have weights on them. So
 the W's are weights. And then the synapse interacts with the input to this neuron,
 multiplicatively. So what flows to the cell body of this neuron is W times X. But there's
 multiple inputs. So there's many W times axis flowing to the cell body. The cell body then has
 also like some bias. So this is kind of like the inert innate sort of trigger happiness of this
 neuron. So this bias can make it a bit more trigger happy or a bit less trigger happy,
 regardless of the input. But basically we're taking all the W times X of all the inputs,
 adding the bias. And then we take it through an activation function. And this activation
 function is usually some kind of a squash function, like a sigmoid or 10 H or something like that.
 So as an example, we're going to use the 10 H in this example. NumPy has a N P dot 10 H. So
 we can call it on a range, and we can plot it. Those are the 10 H function. And you see that
 the inputs as they come in, get squashed on the white coordinate here. So right at zero,
 we're going to get exactly zero. And then as you go more positive in the input,
 then you'll see that the function will only go up to one and then plateau out. And so if you pass
 in very positive inputs, we're going to cap it smoothly at one. And on the negative side,
 we're going to cap it smoothly to negative one. So that's 10 H. And that's the squashing function
 or an activation function. And what comes out of this neuron is just the activation function
 applied to the dot product of the weights and the inputs. So let's write one out.
 I'm going to copy based because I don't want to type too much. But okay, so here we have the inputs
 x1, x2. So this is a two dimensional neuron. So two inputs are going to come in. These are thought
 out as the weights of this neuron, weights w1, w2. And these weights again are the synaptic
 strengths for each input. And this is the bias of the neuron B. And now we want to do is,
 according to this model, we need to multiply x1 times w1 and x2 times w2. And then we need to
 add bias on top of it. And it gets a little messy here, but all we are trying to do is x1,
 w1 plus x2, w2 plus B. And these are multiplied here. Except I'm doing it in a small steps so that
 we actually have pointers to all these intermediate nodes. So we have x1, w1 variable, x times x2,
 w2 variable, and I'm also labeling them. So n is now the cell body raw activation without
 the activation function from now. And this should be enough to basically plot it. So draw a dot of n
 gives us x1 times w1, x2 times w2 being added. And the bias gets added on top of this. And this
 n is this sum. So we're now going to take it through an activation function. And let's say
 we use the 10H so that we produce the output. So what we'd like to do here is we'd like to
 do the output. And I'll call it o is n dot 10H. Okay, but we haven't yet written the 10H.
 Now the reason that we need to implement another 10H function here is that 10H is a hyperbolic
 function. And we've only so far implemented plus and the times. And you can't make a 10H
 out of just pluses and times. You also need exponentiation. So 10H is this kind of a formula
 here. You can use either one of these. And you see that there is exponentiation involved,
 which we have not implemented yet for our low value node here. So we're not going to be able
 to produce 10H yet and we have to go back up and implement something like it. Now one option here
 is we could actually implement exponentiation, right? And we could return the x of a value
 instead of a 10H of a value. Because if we have x, then we have everything else that we need
 so, because we know how to add and we know how to add and we know how to multiply. So we'll be
 able to create 10H if we knew how to exp. But for the purposes of this example, I specifically wanted
 to show you that we don't necessarily need to have the most atomic pieces in this value object.
 We can actually like create functions at arbitrary points of abstraction. They can be complicated
 functions, but they can be also very, very simple functions like a plus. And it's totally up to us.
 The only thing that matters is that we know how to differentiate through any one function. So
 we take some inputs and we make an output. The only thing that matters can be arbitrarily complex
 function as long as you know how to create the local derivative. If you know the local derivative
 of how the inputs impact the output, then that's all you need. So we're going to cluster up
 all of this expression and we're not going to break it down to its atomic pieces. We're just
 going to directly implement 10H. So let's do that. Death 10H. And then out will be a value
 of, and we need this expression here. So let me actually copy paste.
 Let's grab N, which is a cell data. And then this, I believe is the 10H. Math.x of
 2, no, N minus 1 over 2N plus 1. Maybe I can call this x. Just let it match it exactly.
 Okay. And now this will be T. And children of this node, they're just one child.
 And I'm wrapping it in a tuple. So this is a tuple of one object, just self.
 And here, the name of this operation will be 10H. And we're going to return that.
 Okay. So now values should be implementing 10H. And now we can scroll the way down here.
 And we can actually do n dot 10H. And that's going to return the 10H output of N.
 And now we should be able to draw that of, oh, not of N. So let's see how that worked.
 There we go. N went through 10H to produce this output. So now 10H is a sort of,
 our little micrograt supported node here as an operation.
 And as long as we know derivative of 10H, then we'll be able to back propagate through it.
 Now let's see this 10H in action. Currently it's not squashing too much,
 because the input to it is pretty low. So the bias was increased to say eight.
 Then we'll see that what's flowing into the 10H now is two.
 And 10H is squashing it to 0.96. So we're already hitting the tail of this 10H.
 And it will sort of smoothly go up to one and then plateau out over there.
 Okay. So now I'm going to do something slightly strange.
 I'm going to change this bias from eight to this number, 6.88, etc.
 And I'm going to do this for specific reasons, because we're about to start back propagation.
 And I want to make sure that our numbers come out nice.
 They're not like very crazy numbers. They're nice numbers that we can sort of
 understand in our head. Let me also add both label.
 Oh, a short four output here. So that's the R. Okay. So 0.88 flows into 10H comes out 0.7.
 So now we're going to do back propagation and we're going to fill in all the gradients.
 So what is the derivative O with respect to all the inputs here?
 And of course, in a typical neural network setting, what we really care about the most
 is the derivative of these neurons on the weights specifically, the W2 and W1,
 because those are the weights that we're going to be changing,
 part of the optimization. And the other thing that we have to remember is here,
 we have only a single neuron, but in the neural not used typically have many neurons and they're
 connected. So this is only like a one small neuron, a piece of a much bigger puzzle.
 And eventually there's a loss function that sort of measures the accuracy of the neural
 nut. And we're back propagating with respect to that accuracy and trying to increase it.
 So let's start off back propagation here and end.
 What is the derivative of O with respect to O? The base case sort of we know always is
 that the gradient is just 1.0. So let me fill it in. And then let me
 split out the drawing function here. And then here cell
 clear this output here. Okay. So now when we draw O, we'll see that O and the graph is 1.
 So now we're going to back propagate through the 10H. So to back propagate through 10H,
 we need to know the local derivative of 10H. So if we have that O is 10H of N, then what is
 DO by DN. Now what you could do is you could come here and you could take this expression and you
 could do your calculus derivative taking. And that would work. But we can also just scroll down
 with the PDI here into a section that hopefully tells us that derivative D by DX of 10H of X is
 any of these. I like this one. 1 minus 10H square of X. So this is 1 minus 10H of X squared.
 So basically what this is saying is that DO by DN is 1 minus 10H of N squared.
 And we already have 10H of N. It's just O. So it's 1 minus O squared. So O is the output here.
 So the output is this number. O dot data is this number. And then what this is saying is that DO by
 DN is 1 minus this squared. So 1 minus O dot data squared is 0.5 conveniently.
 So the local derivative of this 10H operation here is 0.5. And so that would be DO by DN.
 So we can fill in that in that grad is 0.5. We'll just fill it in.
 So this is exactly 0.5. 1/2. So now we're going to continue the back propagation.
 This is 0.5 and this is a plus node. So how is backdrop going to? What is backdrop going to do here?
 And if you remember our previous example, a plus is just a distributor of gradient.
 So this gradient will simply flow to both of these equally. And that's because the local
 derivative of this operation is 1 for every one of its nodes. So 1 times 0.5 is 0.5.
 So therefore, we know that this node here, which we called this, its grad is just 0.5.
 And we know that B dot grad is also 0.5. So let's set those and let's draw.
 So those are 0.5. Continuing, we have another plus. 0.5 again, we'll just distribute. So 0.5 will
 flow to both of these. So we can set theirs. X2W2 as well. That grad is 0.5.
 And let's redraw. Plus this are my favorite operations to back propagate through because
 it's very simple. So now it's flowing into these expressions, this 0.5. And so really, again,
 keep in mind what derivative is telling us at every point in time along here. This is saying that
 if we want the output of this neuron to increase, then the influence on these expressions is positive
 on the output. Both of them are positive
 contribution to the output. So now back propagating to X2W2 first. This is a times node. So we know
 that the local derivative is the other term. So if we want to calculate X2 dot grad, then
 can you think through what it's going to be?
 So X2 dot grad will be w2 dot data times this X2W2 dot grad. And w2 dot grad will be
 X2 dot data times X2W2 dot grad. So that's the local piece of chain rule.
 Let's set them and let's redraw. So here we see that the gradient on our weight
 2 is 0 because X2's data was 0. But X2 will have the gradient 0.5 because data here was 1.
 And so what's interesting here is because the input X2 was 0 and because of the way the times
 works, of course this gradient will be 0. And to think about intuitively why that is,
 derivative always tells us the influence of this on the final output. If I wiggle w2,
 how is the output changing? It's not changing because we're multiplying by 0.
 So because it's not changing, there is no derivative and 0 is the correct answer
 because we're splashing with that 0. And let's do it here. 0.5 should come here and flow through
 this times. And so we'll have that X1 dot grad is, can you think through a little bit what
 this should be? The local derivative of times with respect to X1 is going to be w1. So w1's data
 times X1 w1 dot grad and w1 dot grad will be X1 dot data times X1 w2 w1 dot grad.
 Let's see what those came out to be. So this is 0.5 so this would be negative 1.5 and this would be
 1. And we back propagate it through this expression. These are the actual final derivatives. So if we
 want this neurons output to increase, we know that what's necessary is that w2 we have no gradient.
 w2 doesn't actually matter to this neuron right now, but this neuron this weight should go up.
 So if this weight goes up, then this neurons output would have gone up and proportionally
 because the gradient is 1. Okay, so doing the back propagation manually is obviously ridiculous.
 So we are now going to put an end to this suffering and we're going to see how we can implement
 the backward pass a bit more automatically. We're not going to be doing all of it manually out here.
 It's now pretty obvious to us by example how these pluses and times are back propagating
 ingredients. So let's go up to the value object and we're going to start codifying what we've seen
 in the examples below. So we're going to do this by storing a special self that backward.
 And underscore backward. And this will be a function which is going to do that little piece of chain
 rule at each little node that compute that took inputs and produced output. We're going to store
 how we are going to chain the outputs gradient into the inputs gradients. So by default,
 this will be a function that doesn't do anything. So and you can also see that here in the value
 in micro grad. So with this backward function, by default, doesn't do anything. This is a empty
 function. And that would be sort of the case, for example, for leaf node, for leaf node, there's
 nothing to do. But now if when we're creating these out values, these out values are an addition
 of self and other. And so we're going to want to sell set outs backward to be the function that
 propagates the gradient. So let's define what should happen.
 And we're going to store it in a closure. Let's define what should happen when we call
 out grad. For an addition, our job is to take out grad and propagate it into self-scrad and
 other grad. So basically we want to solve self-grad to something. And we want to set others that grad
 to something. Okay. And the way we saw below how chain rule works, we want to take the local
 derivative times the sort of global derivative, I should call it, which is the derivative of the
 final output of the expression with respect to outs data, respect to out. So the local derivative
 of self in an addition is 1.0. So it's just 1.0 times outs grad. That's the chain rule. And others
 that grad will be 1.0 times out grad. And what you basically what you're seeing here is that
 outs grad will simply be copied onto self-scrad and others grad, as we saw happens for an addition
 operation. So we're going to later call this function to propagate the gradient having done an
 addition. Let's now do multiplication. We're going to also define that backward.
 And we're going to set its backward to be backward.
 And we want to chain out grad into self-that grad and others that grad.
 And this will be a little piece of chain rule for multiplication. So we'll have,
 so what should this be? Can you think through?
 So what is the local derivative? Here, the local derivative was others that data.
 And then others that data. And then times out that grad, that's chain rule. And here we have
 self-that data times out that grad. That's what we've been doing.
 And finally here for 10H, that backward. And then we want to set outs backwards to be just backward.
 And here we need to back propagate. We have out that grad and we want to chain it into self-that grad.
 And self-that grad will be the local derivative of this operation that we've done here,
 which is 10H. And so we saw that the local gradient is 1 minus the 10H of x squared,
 which here is t. That's the local derivative because that's t is the output of this 10H.
 So 1 minus t squared is the local derivative. And then gradients,
 um, as we multiplied because of the chain rule. So out grad is chained through the local gradient
 into self-that grad. And that should be basically it. So we're going to redefine our value node.
 We're going to swing all the way down here. And we're going to redefine our expression,
 make sure that all the grads are zero. Okay. But now we don't have to do this manually anymore.
 We are going to basically be calling the dot backward in the right order.
 So first we want to call o's dot backward.
 So o was the outcome of 10H. Right. So calling o's dot backward will be this function. This is
 what it will do. Now we have to be careful because there's a times out that grad. And out that grad
 remember is initialized to zero. So here we see grad zero. So as a base case, we need to set
 both that grad to 1.0 to initialize this with one.
 And then once this is one, we can call o dot backward. And what that should do is it should
 propagate this grad through 10H. So the local derivative times the global derivative,
 which is initialized at one. So this should
 a dough. So I thought about redoing it, but I figured I should just leave the error in here
 because it's pretty funny. Why is not object not callable? It's because I screwed up. We're
 trying to save these functions. So this is correct. This here, we don't want to call the function
 because that returns none. These functions return none. We just want to store the function.
 So let me redefine the value object. And then we're going to come back in redefine the expression
 draw dot. Everything is great. O dot grad is one. O dot grad is one. And now,
 now this should work, of course. Okay. So all that backward should have, this grad should now be
 0.5 if we redraw. And if everything went correctly, 0.5. Yay. Okay. So now we need to call n-stat-grad.
 And it's not backward, sorry. N-stat-forward. So that seems to have worked.
 So n-stat-backward, wrapped the gradient to both of these. So this is looking great.
 Now we can of course call V dot grad, V dot backward, sorry. What's going to happen? Well,
 B doesn't have it backward. B is backward because B is a leaf node. B is backward is by
 initialization, TMT function. So nothing would happen, but we can call it on it. But when we call
 this one, it's backward. Then we expect this point five to get further around it. Right. So
 there we go, 0.5. And then finally, we want to call it here on x2w2.
 And on x1w1. Let's do both of those. And there we go. So we get 0.5, negative 1.5, and 1,
 exactly as we did before. But now we've done it through calling that backward,
 sort of manually. So we have the one last piece to get rid of, which is us calling
 underscore backward manually. So let's think through what we are actually doing.
 We've laid out a mathematical expression, and now we're trying to go backwards through that
 expression. So going backwards through the expression just means that we never want to call a dot
 backward for any node before we've done sort of everything after it. So we have to do everything
 after it before ever going to call dot backward on any one node. We have to get all of its full
 dependencies. Everything that it depends on has to propagate to it before we can continue back
 propagation. So this ordering of graphs can be achieved using something called topological sort.
 So topological sort is basically a laying out of a graph such that all the edges go only from
 left to right, basically. So here we have a graph, it's a directory, a cyclic graph, a DAG.
 And this is two different topological orders of it, I believe, where basically you'll see that
 it's a laying out of the nodes such that all the edges go only one way from left to right.
 And implementing topological sort, you can look in Wikipedia and so on. I'm not going to go through
 it in detail. But basically, this is what builds a topological graph. We maintain a set of visited
 nodes. And then we are going through starting at some root node, which for us is oh, that's what
 we're going to start a topological sort. And starting at Oh, we go through all of its children,
 and we need to lay them out from left to right. And basically, this starts at Oh, if it's not
 visited, then it marks it as visited. And then it iterates through all of its children, and calls
 built topological on them. And then after it's gone through all the children, it adds itself.
 So basically, this node that we're going to call it on, like say, Oh, is only going to add itself
 to the topo list after all of the children have been processed. And that's how this function is
 guaranteeing that you're only going to be in the list once all your children are in the list.
 And that's the invariant that is being maintained. So if we built up on Oh, and then inspect this list,
 we're going to see that it ordered our value objects. And the last one is the value of 0.7,
 which is the output. So this is Oh, and then this is n. And then all the other nodes get laid out
 before it. So that built the topological graph. And really what we're doing now is we're just
 calling dot underscore backward on all of the nodes in a topological order. So if we just
 reset the gradients, they're all zero. What did we do? We started by setting Oh, that grad to be one.
 That's the base case. Then we built the topological order.
 And then we went for node in reversed off topo. Now, in the reverse order,
 because this list goes from, you know, we need to go through it in reversed order. So starting at
 Oh, node backward. And that should be it. There we go. Those are the correct derivatives.
 Finally, we are going to hide this functionality. So I'm going to copy this. And we're going to
 hide it inside the Valley class, because we don't want to have all that code lying around.
 So instead of an underscore backward, we're now going to define an actual backward. So that backward
 without the underscore. And that's going to do all the stuff that we just arrived. So let me just
 clean this up a little bit. So we're first going to build the topological graph, starting at self.
 So build topo of self will populate the topological order into the topo list,
 which is a local variable. Then we set self that grad to be one. And then for each node in the
 reversed list, so starting at us and going to all the children underscore backward.
 And that should be it. So save. Come down here, we define. Okay, all the grads are zero.
 And now what we can do is without backward, without the underscore. And
 there we go. And that's that's back propagation. Please for one neuron. We shouldn't be too
 happy with ourselves, actually, because we have a bad bug. And we have not surfaced the bug because
 of some specific conditions that we are have, we have to think about right now. So here's the
 simplest case that shows the bug. Say I create a single node a. And then I create a B that is a
 plus a. And then I call backward. So what's going to happen is a is three. And then a is a plus a.
 So there's two arrows on top of each other here. Then we can see that B is, of course,
 the forward pass works. B is just a plus a, which is six. But the gradient here is not actually
 correct that we calculate it automatically. And that's because, of course, just doing
 calculus in your head, the derivative of B with respect to a should be two. One plus one. It's not
 one. Intuitively, what's happening here, right? So B is the result of a plus a. And then we call
 backward on it. So let's go up and see what that does. B is a result of addition. So out is B.
 And then when we call backward, what happened is self that grad was set to one. And then other
 that grad was set to one. But because we're doing a plus a self and other are actually the exact same
 object. So we are overriding the gradient. We are setting it to one. And then we are setting it again
 to one. And that's why it stays at one. So that's a problem. There's another way to see this in a
 little bit more complicated expression. So here we have a and B. And then D will be the
 multiplication of the two. And E will be the addition of the two. And then we multiply it
 times D to get F. And then we call F that backward. And these gradients, if you check,
 will be incorrect. So fundamentally, what's happening here, again, is basically, we're going
 to see an issue anytime we use a variable more than once. Until now, in these expressions above,
 every variable is used exactly once. So we didn't see the issue. But here, if a variable is used
 more than once, what's going to happen during backward pass? We're back propagating from F to
 E to D. So far so good. But now E calls it backward. And it deposits its gradients to A and B. But
 then we come back to D and call backward. And it overrides those gradients at A and B. So that's
 obviously a problem. And the solution here, if you look at the multivariate case of the chain rule
 and its generalization there, the solution there is basically that we have to accumulate these
 gradients, these gradients add. And so instead of setting those gradients, we can simply do plus
 equals, we need to accumulate those gradients, plus equals, plus equals, plus equals, plus equals.
 And this will be okay, remember, because we are initializing them at zero. So they start at zero.
 And then any contribution that flows backwards, we'll simply add. So now if we redefine this one,
 because the plus equals this now works, because a dad grad started at zero. And we call beta backward,
 we deposit one and then we deposit one again. And now this is two, which is correct. And here,
 this will also work. And we'll get correct gradients. Because when we call it a backward,
 we will deposit the gradients from this branch. And then we get to back to detail backward,
 it will deposit its own gradients. And then those gradients simply add on top of each other. And so
 we just accumulate those gradients and that fixes the issue. Okay, now before we move on,
 let me actually do a bit of cleanup here and delete some of these, some of this intermediate work.
 So I'm not going to need any of this, now that we've derived all of it. We are going to keep this
 because I want to come back to it. Delete the 10 H, delete our morning example,
 delete the step, delete this, keep the code that draws, and then delete this example
 and leave behind only the definition of value. And now let's come back to this nonlinearity
 here that we implemented the 10 H. Now I told you that we could have broken down 10 H into its
 explicit atoms in terms of other expressions if we had the exp function. So if you remember,
 10 H is defined like this, and we chose to develop 10 H as a single function. And we can do that
 because we know it's derivative and we can back propagate through it. But we can also break down
 10 H into an expressive function of exp. And I would like to do that now because I want to prove
 to you that you get all the same results and all the same gradients. But also because it forces
 us to implement a few more expressions. It forces us to do exponentiation, addition, subtraction,
 division, and things like that. And I think it's a good exercise to go through a few more of these.
 Okay, so let's scroll up to the definition of value. And here, one thing that we currently
 can't do is we can do like a value of say 2.0. But we can't do, you know, here, for example,
 we want to add constant one, and we can't do something like this. And we can't do it because
 it says into object has no attribute data. That's because a plus one comes right here to add.
 And then other is the integer one. And then here Python is trying to access one dot data,
 and that's not a thing. That's because basically one is not a value object. And we only have addition
 from value objects. So as a matter of convenience, so that we can create expressions like this and
 make them make sense, we can simply do something like this. Basically, we let other alone, if other
 is an instance of value, but if it's not an instance of value, we're going to assume that it's a number
 like an integer or a float, and we're going to simply wrap it in in value. And then other will
 just become value of other, and then other will have a data attribute. And this should work. So if
 I just say this read a foreign value, then this should work. There we go. Okay, now let's do the
 exact same thing for multiply, because we can't do something like this. Again, for the exact same
 reason. So we just have to go to mall. And if other is not a value, then let's wrap it in value.
 Let's redefine value. And now this works. Now here's a kind of unfortunate and not obvious part.
 A times two works, we saw that, but two times a is that gonna work. You'd expect it to write,
 but actually it will not. And the reason it won't is because Python doesn't know. Like when you do
 a times two, basically, so a times two, Python will go and it will basically do something like a
 dot mall of two, that's basically what it will call. But to it, two times a is the same as two
 dot mall of a, and it doesn't to can't multiply value. And so it's really confused about that.
 So instead what happens is in Python, the way this works, is you are free to define
 something called the our mall. And our mall is kind of like a fallback. So if Python can't do two
 times a, it will check if if by any chance a knows how to multiply two, and that will be called into
 our mall. So because Python can't do two times a, it will check is there an our mall in value.
 And because there is, it will now call that. And what we'll do here is we will swap the order of
 the operands. So basically, two times a will redirect to our mall, and our mobile basically
 call a times two. And that's how that will work. So redefining that with our mall, two times a
 becomes four. Okay, now looking at the other elements that we still need, we need to know how
 to exponentiate and how to divide. So let's first the explanation to the exponentiation part. We're
 going to introduce a single function X here. And X is going to mirror 10 age, in the sense that
 it's a simple, single function that transform a single scalar value and outputs a single scalar
 value. So we pop out the Python number, we use method X to exponentiate it, create a new value
 object, everything that we've seen before. The tricky part of course is how do you back propagate
 through e to the X. And so here you can potentially pause the video and think about what should go
 here. Okay, so basically, we need to know what is the local derivative of e to the X. So d by dx
 of e to the X is famously just e to the X. And we've already just calculated e to the X,
 and it's inside out that data. So we can do out that data times, and out that grad, that's the
 chain. So we're just chaining on to the current running grad. And this is what the expression
 looks like. It looks a little confusing, but this is what it is, and that's the explanation.
 So redefining, we should not be able to call it a data exp, and hopefully the backward pass
 works as well. Okay, and the last thing we'd like to do, of course, is we'd like to be able to divide.
 Now, I actually will implement something slightly more powerful than division,
 because division is just a special case of something a bit more powerful. So in particular,
 just by rearranging, if we have some kind of a b equals value of 4.0 here, we'd like to basically
 be able to do a divided b, and we'd like this to be able to give us 0.5. Now, division actually
 can be reshuffled as follows. If we have a divided b, that's actually the same as a multiplying 1 over
 b, and that's the same as a multiplying b to the power of negative 1. And so what I'd like to do
 instead is I basically like to implement the operation of X to the k for some constant
 k. So it's an integer or a float, and we would like to be able to differentiate this. And then as a
 special case, negative 1 will be division. And so I'm doing that just because it's more general,
 and yeah, you might as well do it that way. So basically what I'm saying is we can redefine
 division, which we will put here somewhere. Yeah, we can put this here somewhere. What I'm saying
 is that we can redefine division. So self divide other can actually be rewritten as self times
 other to the power of negative 1. And now value raised to the power of negative 1, we have now
 defined that. So here's, so we need to implement the power function. Where am I going to put the
 power function maybe here somewhere? Let's just call it from Fort. So this function will be called
 when we try to raise a value to some power, and other will be that power. Now I'd like to make
 sure that other is only an int or a float. Usually other is some kind of a different value object,
 but here other will be forced to be an int or a float. Otherwise the math won't work for
 trying to achieve in the specific case. That would be a different derivative expression
 if we wanted other to be a value. So here we create the output value, which is just, you know,
 this data raised to the power of other and other here could be, for example, negative 1. That's what
 we are hoping to achieve. And then this is the backward stub. And this is the fun part, which is
 what is the chain rule expression here for back for back propagating through the power function,
 where the power is to the power of some kind of a constant. So this is the exercise and maybe
 pause the video here and see if you can figure it out yourself as to what we should put here.
 Okay, so you can actually go here and look at the derivative rules as an example. And we see
 lots of derivatives that you can hopefully know from calculus. In particular, what we're looking
 for is the power rule, because that's telling us that if we're trying to take d by dx of x to the
 n, which is what we're doing here, then that is just n times x to the n minus 1, right? Okay.
 So that's telling us about the local derivative of this power operation. So all we want here,
 basically n is now other and self dot data is x. And so this now becomes
 other, which is n times self dot data, which is now a Python int or a float. It's not a value
 object. We're accessing the data attribute raised to the power of other minus one or n minus one.
 I can put brackets around this, but this doesn't matter because power takes precedence over multiply
 and by him. So that would have been okay. And that's the local derivative only. But now we have to
 chain it. And we change it just simply by multiplying by a top grad, that's chain rule. And this should
 technically work. And we're gonna find out soon. But now if we do this, this should now work.
 And we get point five. So the forward pass works, but does the backward password. And I realized
 that we actually also have to know how to subtract. So right now, a minus b will not work. To make
 it work, we need one more piece of code here. And basically, this is the subtraction. And the way
 we're going to implement subtraction is we're going to implement it by addition of a negation.
 And then to implement negation, we're going to multiply by negative one. So just again,
 using the stuff we've already built and just expressing it in terms of what we have, and a minus b
 does not work. Okay, so now let's scroll again to this expression here for this neuron. And let's
 just compute the backward pass here once we've defined O. And let's draw it. So here's the
 gradients for all these leaf nodes for this two dimensional neuron that has a 10 H that we've seen
 before. So now what I'd like to do is I'd like to break up this 10 H into this expression here.
 So let me copy paste this here. And now instead of, we'll preserve the label, and we will change
 how we define O. So in particular, we're going to implement this formula here. So we need e to the
 two X minus one over e to the X plus one. So e to the two X, we need to take two times m,
 and we need to exponentiate it. That's e to the two X. And then because we're using it twice,
 let's create an intermediate variable, E, and then define O as E plus one over E minus one over E
 plus one, E minus one over E plus one. And that should be it. And then we should be able to draw
 that above. So now before I run this, what do we expect to see? Number one, we're expecting to see
 a much longer graph here, because we've broken up 10 H into a bunch of other operations. But
 those operations are mathematically equivalent. And so what we're expecting to see is number one,
 the same result here. So the forward pass works. And number two, because of that mathematical
 equivalence, we expect to see the same backward pass and the same gradients on these leaf nodes.
 So these gradients should be identical. So let's run this. So number one, let's verify that
 instead of a single 10 H node, we have now X, and we have plus, we have times negative one.
 This is the division. And we end up with the same forward pass here. And then the gradients,
 we have to be careful because they're in slightly different order, potentially. The gradients for
 W two X two should be zero and point five. W two and X two are zero and point five. And W one X
 one are one and negative one point five, one and negative one point five. So that means that both
 our forward passes and backward passes were correct, because this turned out to be equivalent to
 10 H before. And so the reason I wanted to go through this exercise is number one. We got to
 practice a few more operations and writing more backwards passes. And number two, I wanted to
 illustrate the point that the level at which you implement your operations is totally up to you.
 You can implement backward passes for tiny expressions like a single individual plus or a single times,
 or you can implement them for say 10 H, which is kind of a potentially you can see it as a
 composite operation because it's made up of all these more atomic operations. But really,
 all of this is kind of like a fake concept. All that matters is we have some kind of inputs
 and some kind of an output. And this output is a function of the inputs in some way. And as long
 as you can do forward pass and the backward pass of that little operation, it doesn't matter what
 that operation is and how composite it is. If you can write the local gradients, you can change
 the gradient and you can continue back propagation. So the design of what those functions are is
 completely up to you. So now I would like to show you how you can do the exact same thing by using
 a modern deep neural network library, like for example, PyTorch, which I've roughly modeled
 micro grad by. And so PyTorch is something you would use in production. And I'll show you how
 you can do the exact same thing, but in PyTorch API. So I'm just going to copy paste it in and
 walk you through it a little bit. This is what it looks like. So we're going to import PyTorch.
 And then we need to define these value objects like we have here. Now,
 micro grad is a scalar valued engine. So we only have scalar values like 2.0. But in PyTorch,
 everything is based around tensors. And like I mentioned, tensors are just
 end dimensional arrays of scalars. So that's why things get a little bit more complicated here.
 I just need a scalar valued tensor, a tensor with just a single element. But by default,
 when you work with PyTorch, you would use more complicated tensors like this. So if I import PyTorch,
 then I can create tensors like this. And this tensor, for example, is a 2x3 array of scalars
 in a single compact representation. So you can check its shape. We see that it's a 2x3 array.
 And so this is usually what you would work with in the actual libraries. So here I'm creating
 a tensor that has only a single element 2.0. And then I'm casting it to be double
 because Python is by default using double precision for its floating point numbers. So I'd like everything
 to be identical. By default, the data type of these tensors will be float 32. So it's only using a
 single precision float. So I'm casting it to double so that we have float 64 just like in Python.
 So I'm casting to double. And then we get something similar to value of 2. The next thing I have to
 do is because these are leaf nodes, by default, PyTorch assumes that they do not require gradients.
 So I need to explicitly say that all of these nodes require gradients.
 Okay, so this is going to construct scalar valued one element tensors. Make sure that PyTorch
 notes that they require gradients. Now, by default, these are set to false, by the way,
 because of efficiency reasons, because usually you would not want gradients for leaf nodes,
 like the inputs to the network. And this is just trying to be efficient in the most common cases.
 So once we've defined all of our values in PyTorch land, we can perform arithmetic,
 just like we can here in micro grad land. So this would just work. And then there's a torch.10h
 also. And when we get back is a tensor again. And we can just like in micro grad, it's got a
 data attribute and it's got grad attributes. So these tensor objects, just like in micro grad,
 have a dot data and a dot grad. And the only difference here is that we need to call a dot item,
 because otherwise PyTorch dot item basically takes a single tensor of one element,
 and it just returns that element stripping out the tensor. So let me just run this. And
 hopefully we are going to get this is going to print the forward pass, which is 0.707.
 And this will be the gradients, which hopefully are 0.50 negative 1.5 and 1. So if we just run this,
 there we go. 0.7. So the forward pass agrees. And then 0.50, 81.5 and 1. So PyTorch agrees with us.
 And just to show you here, basically, oh, here's a tensor with a single element.
 And it's a double. And we can call that item on it to just get the single number out.
 So that's what item does. And oh is a tensor object, like I mentioned,
 and it's got a backward function just like we've implemented. And then all of these also have
 a dot grad. So like X2, for example, on the grad, and it's a tensor. And we can pop out the individual
 number with dot item. So basically, tortious, torch can do what we did in micro grad as a special case
 when your tensors are all single element tensors. But the big deal with PyTorch is that everything
 is significantly more efficient because we are working with these tensor objects. And we can do
 lots of operations in parallel on all of these tensors. But otherwise, what we built very
 much agrees with the API of PyTorch. Okay, so now that we have some machinery to build out
 pretty complicated mathematical expressions, we can also start building up neural nets. And as I
 mentioned, neural nets are just a specific class of mathematical expressions. So we're going to start
 building out a neural net piece by piece, and eventually we'll build out a two layer multilayer
 layer perceptron, as it's called. And I'll show you exactly what that means. Let's start with a
 single individual neuron. We've implemented one here. But here I'm going to implement one that
 also subscribes to the PyTorch API and how it designs its neural network modules. So just like
 we saw that we can like match the API of PyTorch on the autograd side, we're going to try to do
 that on the neural network modules. So here's class neuron. And just for the sake of efficiency,
 I'm going to copy paste some sections that are relatively straightforward. So the constructor
 will take a number of inputs to this neuron, which is how many inputs come to a neuron. So this one
 for example has three inputs. And then it's going to create a weight that is some random number
 between negative one and one for every one of those inputs and a bias that controls the overall
 trigger happiness of this neuron. And then we're going to implement a depth underscore underscore
 call of self and x. So I'm going to put x. And really what we don't do here is w times x plus b,
 or w times x here is a dot product specifically. Now if you haven't seen call, let me just return
 0.0 here from now, the way this works now is we can have an x which is say like 2.0, 3.0,
 then we can initialize a neuron that is two dimensional, because these are two numbers.
 And then we can feed those two numbers into that neuron to get an output. And so when you use this
 notation, n of x Python will use call. So currently, call just returns 0.0. Now we'd like to actually
 do the forward pass of this neuron instead. So we're going to do here first is we need to
 basically multiply all of the elements of w with all of the elements of x pairwise, we need to
 multiply them. So the first thing we're going to do is we're going to zip up salta w and x.
 And in Python zip takes two iterators, and it creates a new iterator that iterates over the
 topples of their corresponding entries. So for example, just to show you we can print this list
 and still return 0.0 here. Sorry, I'm in my. So we see that these w's are paired up with the x's
 w with x. And now what we want to do is for w x i in, we want to multiply w times w i times
 x i, and then we want to sum all of that together to come up with an activation and add also salta
 b on top. So that's the raw activation. And then of course we need to pass that through a normality.
 So what we're going to be returning is act. h. And here's out. So now we see that we are getting
 some outputs, and we get a different output from your own each time, because we are initializing
 different weights and biases. And then to be a bit more efficient here, actually,
 sum by the way, takes a second optional parameter, which is the start. And by default, the start is
 zero. So these elements of this sum will be added on top of zero to begin with. But actually,
 we can just start with salta b. And then we just have an expression like this.
 And then the generator expression here must be parenticized by home. There we go.
 Yep. So now we can forward a single neuron. Next up, we're going to define a layer of neurons.
 So here we have a schematic for a mlp. So we see that these mlp's each layer, this is one layer,
 has actually a number of neurons, and they're not connected to each other. But all of them are
 fully connected to the input. So what is a layer of neurons? It's just it's just a set of neurons
 evaluated independently. So in the interest of time, I'm going to do something fairly straightforward
 here. It's literally a layer is just a list of neurons. And then how many neurons do we have?
 We take that as an input argument here, how many neurons do you want in your layer? Number of
 outputs in this layer. And so we just initialize completely independent neurons with this given
 dimensionality. And we call on it, we just independently evaluate them. So now instead of a neuron,
 we can make a layer of neurons. They are two dimensional neurons, and let's have three of them.
 And now we see that we have three independent evaluations of three different neurons.
 Okay, finally, let's complete this picture and define an entire multilateral perception or mlp.
 And as we can see here in an mlp, these layers just feed into each other sequentially.
 So let's come here, and I'm just going to copy the code here in interest of time.
 So an mlp is very similar. We're taking the number of inputs as before, but now instead of
 taking the single and out, which is number of neurons in a single layer, we're going to take a
 list of and outs. And this list defines the sizes of all the layers that we want in our mlp.
 So here we just put them all together, and then iterate over consecutive pairs of these sizes
 and create layer objects for them. And then in the call function, we are just calling them
 sequentially. So that's an mlp really. And let's actually re implement this picture. So we want
 three input neurons, and then two layers of four and an output unit. So we want
 three dimensional input, say this is an example, but we want three inputs into two layers of four
 and one output. And this, of course, is an mlp. And there we go. That's a forward pass of an mlp.
 To make this a little bit nicer, you see how we have just a single element, but it's wrapped in
 the list because layer always returns lists. Circum for convenience, return outs at zero if
 length out is exactly a single element else return full list. And this will allow us to just get a
 single value out at the last layer that only has a single neuron. And finally, we should be able to
 prod out of N of X. And as you might imagine, these expressions are now getting relatively
 involved. So this is an entire mlp that we're defining now. All the way until a single output.
 Okay. And so obviously you would never differentiate on pen and paper these expressions, but with
 micro grad, we will be able to back propagate all the way through this and back propagate
 into these weights of all these neurons. So let's see how that works. Okay, so let's create
 ourselves a very simple example data set here. So this data set has four examples. And so we have
 four possible inputs into the neural net. And we have four desired targets. So we like the
 neural net to assign our output 1, 1, 0 when it's fed this example, negative one when it's fed these
 examples, and one when it's fed this example. So it's a very simple binary classifier neural
 net, basically that we would like here. Now let's think what the neural net currently thinks about
 these four examples. We can just get their predictions. Basically, we can just call N of X for X in Xs.
 And then we can print. So these are the outputs of the neural net on those four examples. So
 the first one is 0.91, but we like it to be one. So we should push this one higher, this one we want
 to be higher. This one says 0.88 and we want this to be negative one. This is 0.8, we want it to be
 negative one. And this one is 0.8, we want it to be one. So how do we make the neural net and how do we
 tune the weights to better predict the desired targets? And the trick used in deep learning to
 achieve this is to calculate a single number that somehow measures the total performance of your
 neural net. And we call the single number the loss. So the loss first is a single number that we're
 going to define that basically measures how well the neural net is performing. Right now we have
 the intuitive sense that it's not performing very well, because we're not very much close to this.
 So the loss will be high, and we'll want to minimize the loss. So in particular, in this case,
 what we're going to do is we're going to implement the mean squared error loss. So this is doing is
 we're going to basically iterate for why ground truth and why output in zip of wise and life red.
 So we're going to pair up the ground truths with the predictions. And the zip iterates over
 tuples of them. And for each, why ground truth and why output we're going to subtract them.
 And square. So let's first see what these losses are. These are individual loss components.
 And so basically for each one of the four, we are taking the prediction and the ground truth.
 We are subtracting them and squaring them. So because this one is so close to its target,
 0.91 is almost one, subtracting them gives a very small number. So here we would get like a
 negative point one, and then squaring it just makes sure that regardless of whether we are
 more negative or more positive, we always get a positive number. Instead of squaring, we should
 know we could also take, for example, the absolute value. We need to discard the sign.
 And so you see that the expression is arranged so that you only get zero exactly when
 y out is equal to y ground truth. When those two are equal, so your prediction is exactly the target,
 you are going to get zero. And if your prediction is not the target, you are going to get some other
 number. So here, for example, we are way off. And so that's why the loss is quite high. And the more
 off we are, the greater the loss will be. So we don't want high loss, we want low loss. And so the
 final loss here will be just the sum of all of these numbers. So you see that this should be
 zero roughly plus zero roughly, but plus seven. So loss should be about seven here. And now we
 want to minimize the loss. We want the loss to be low, because if loss is low, then every one of the
 predictions is equal to its target. So the loss, the lowest it can be a zero. And the greater it is,
 the worse off, the neural net is predicting. So now, of course, if we do lost that backward,
 something magical happened when I hit enter. And the magical thing, of course, that happened is
 that we can look at and add layers that neuron and that layers at say, like the first layer,
 that neurons at zero. Because remember that MLP has the layers, which is a list. And each
 layer has neurons, which is a list. And that gives us individual neuron. And then it's got some weights.
 And so we can, for example, look at the weights at zero. Oops, it's not called weights, it's called
 w. And that's a value. But now this value also has a graph, because of the backward pass.
 And so we see that because this gradient here on this particular weight of this particular
 neuron of this particular layer is negative, we see that its influence on the loss is also negative.
 So slightly increasing this particular weight of this neuron of this layer would make the loss
 go down. And we actually have this information for every single one of our neurons and all
 their parameters. Actually, it's worth looking at also the draw dot loss, by the way. So previously,
 we looked at the draw dot of a single neural neural neural forward pass. And that was already a large
 expression. But what is this expression? We actually forwarded every one of those four examples. And
 then we have the loss on top of them with the mean squared error. And so this is a really massive
 graph. Because this graph that we built up now, oh my gosh, this graph that we built up now,
 we're just kind of excessive, it's excessive because it has four forward passes of a neural
 nut for every one of the examples. And then it has the loss on top. And it ends with the value of
 the loss, which was 7.12. And this loss will now back propagate through all the forward forward
 passes, all the way through just every single intermediate value of the neural net, all the way
 back to, of course, the parameters of the weights, which are the input. So these weight parameters
 here are inputs to this neural net. And these numbers here, these scalars are inputs to the
 neural net. So if we went around here, we will probably find some of these examples, this 1.0,
 potentially maybe this 1.0, or you know, some of the others. And you'll see that they all have
 gradients as well. The thing is these gradients on the input data are not that useful to us. And
 that's because the input data seems to be not changeable. It's, it's a given to the problem. And
 so it's a fixed input, we're not going to be changing it or messing with it, even though we do have
 gradients for it. But some of these gradients here will be for the neural network parameters,
 the Ws and the Bs. And those sweep, of course, we want to change. Okay, so now we're going to
 want some convenience codes to gather up all of the parameters of the neural net, so that we can
 operate on all of them simultaneously. And every one of them, we will nudge a tiny amount
 based on the gradient deformation. So let's collect the parameters of the neural net all in
 one array. So let's create a parameters of self that just returns,
 self that W, which is a list, concatenated with a list of self that B. So this will just return
 a list, list plus list just, you know, gives you a list. So that's parameters of neuron. And I'm
 calling it this way, because also PyTorch has a parameters on every single and in module. And
 it does exactly what we're doing here, it just returns the parameter tensors for us is the parameter
 scalars. Now layer is also a module. So it will have parameters, self. And basically, what we want
 to do here is something like this, like, params is here, and then for neuron in salt out neurons,
 we want to get neuron parameters. And we want to params that extend. So these are the parameters
 of this neuron. And then we want to put them on top of params. So params dot extend of piece.
 And then we want to return params. So this is way too much code. So actually, there's a way to
 simplify this, which is return p for neuron in self, neurons for p in neuron dot parameters.
 So it's a single less comprehension in Python, you can sort of nest them like this, and you can
 then create the desired array. So this is these are identical. We can take this out.
 And then let's do the same here.
 That parameters, self, and return a parameter for layer in self dot layers for p in layer dot
 parameters. And that should be good. Now let me pop out this. So we don't re initialize our
 network, because we need to re initialize our. Okay, so unfortunately, we will have to probably
 re initialize the network, because we just had functionality. Because this class, of course,
 we I want to get all the end up parameters. That's not going to work, because this is the old class.
 Okay. So unfortunately, we do have to re initialize the network, which will change some of the numbers.
 But let me do that so that we pick up the new API, we can now do end up parameters.
 And these are all the weights and biases inside the entire neural net. So in total, this MLP has 41
 parameters. And now we'll be able to change them. If we recalculate the loss here, we see that
 unfortunately, we have slightly different predictions and slightly different loss. But that's okay.
 Okay, so we see that this neurons gradient is slightly negative. We can also look at its data
 right now, which is 0.85. So this is the current value of this neuron. And this is its gradient
 on the loss. So what we want to do now is we want to iterate for every pn and that parameters.
 So for all the 41 parameters of this neural net, we actually want to change p data data
 slightly according to the gradient information. Okay, so data to do here. But this will be basically
 a tiny update in this gradient descent scheme. And gradient descent, we are thinking of the gradient
 as a vector pointing in the direction of increased loss. And so in gradient descent, we are modifying
 p data by a small step size in the direction of the gradient. So the step size as an example
 could be like a very small number of 0.01 is the step size times p dot grad, right? But we have
 to think through some of the signs here. So in particular, working with this specific example here,
 we see that if we just left it like this, then this neurons value would be currently increased
 by a tiny amount of the gradient. The gradient is negative. So this value of this neuron would
 go slightly down. It would become like 0.8, you know, four or something like that. But if this
 neurons value goes lower, that would actually increase the loss. That's because the derivative
 of this neuron is negative. So increasing this makes the loss go down. So increasing it is what
 we want to do instead of decreasing it. So basically what we're missing here is we're actually missing
 a negative sign. And again, this other interpretation, and that's because we want to minimize the loss,
 we don't want to maximize the loss, we want to decrease it. And the other interpretation,
 as I mentioned, is you can think of the gradient vector. So basically, just the vector of all
 the gradients as pointing in the direction of increasing the loss. But then we want to decrease
 it. So we actually want to go in the opposite direction. And so you can convince yourself that
 this sort of like does the right thing here with a negative because we want to minimize the loss.
 So if we notch all the parameters by tiny amount, then we'll see that this data will have changed
 a little bit. So now this neuron is a tiny amount greater value. So 0.854, once it's 0.857.
 And that's a good thing because slightly increasing this neuron data makes the loss go down,
 according to the gradient. And so the correcting has happened signwise. And so now what we would
 expect, of course, is that because we've changed all these parameters, we expect that the loss
 should have gone down a bit. So we want to reevaluate the loss. Let me basically,
 this is just a data definition that hasn't changed. But the forward pass here of the network,
 we can recalculate. And actually, let me do it outside here so that we can compare the two
 loss values. So here, if I recalculate the loss, we'd expect the new loss now to be slightly lower
 than this number. So hopefully what we're getting now is a tiny bit lower than 4.84.
 4.36. Okay. And remember, the way we've arranged this is that low loss means that our predictions
 are matching the targets. So our predictions now are probably slightly closer to the targets.
 And now all we have to do is we have to iterate this process. So again, we've done the forward pass,
 and this is the loss. Now we can lost that backward. Let me take these out. And we can do a step size.
 And now we should have a slightly lower loss. 4.36 goes to 3.9. And okay, so we've done the
 forward pass. Here's the backward pass, nudge. And now the loss is 3.66, 3.47. And you get the idea,
 we just continue doing this. And this is a gradient descent. We're just iteratively doing forward pass,
 backward pass update, forward pass, backward pass update. And the neural net is improving
 its predictions. So here, if we look at y-pred now, y-pred, we see that this value should be getting
 closer to one. So this value should be getting more positive. These should be getting more
 negative. And this one should be also getting more positive. So if we just iterate this a few
 more times, actually, we'll be able to afford to go a bit faster. Let's try a slightly higher learning
 rate. Oops, okay, there we go. So now we're at 0.31. If you go too fast, by the way, if you try to
 make it too big of a step, you may actually overstep over confidence. Because again, remember, we
 don't actually know exactly about the loss function. The loss function has all kinds of structure.
 And we only know about the very local dependence of all these parameters on the loss. But if we step
 too far, we may step into a part of the loss that is completely different. And that can
 destabilize training and make your loss actually blow up even. So the loss is now 0.04. So actually,
 the predictions should be really quite close. Let's take a look. So you see how this is almost one,
 almost negative one, almost one. We can continue going. So yep, backward update.
 Oops, there we go. So we went way too fast. And we actually overstepped. So we got to
 to eager, where are we now? Oops. Okay, seven in negative nine. So this is very, very low loss.
 And the predictions are basically perfect. So somehow we,
 basically we were doing way to the updates and we briefly exploded. But then somehow we ended up
 getting into a really good spot. So usually this learning rate and a tuning of it is a subtle
 art. You want to set your learning rate. If it's too low, you're going to take way too long to
 converge. But if it's too high, the whole thing gets unstable and you might actually even explode
 the loss, depending on your loss function. So finding the step size to be just right,
 it's it's a pretty subtle art sometimes when you're using sort of vanilla gradient descent.
 But we happen to get into a good spot. We can look at and parameters. So this is the setting of
 weights and biases that makes our network predict the desired targets very, very close.
 And basically we've successfully trained in neural net. Okay, let's make this a tiny bit more
 respectable and implement an actual training loop and what that looks like. So this is the data
 definition that stays. This is the forward pass. So for k in range, you know, we're going to
 take a bunch of steps. First, you do the forward pass. We evaluate the loss.
 Let's reinitialize the neural line from scratch. And here's the data. And we first do forward pass,
 then we do the backward pass. And then we do an update. That's gradient descent.
 And then we should be able to iterate this and we should be able to print the current step,
 the current loss. Let's just print the sort of number of the loss. And that should be it.
 And then the learning rate 0.01 is a little too small. 0.1 we saw is like a little bit dangerous
 with UI. Let's go somewhere in between and we'll optimize this for not 10 steps, but let's go for
 say 20 steps. Let me erase all of this junk. And let's run the optimization.
 And you see how we've actually converged slower in a more controlled manner and got to a loss
 that is very low. So I expect Y-Pred to be quite good. There we go.
 And that's it. Okay, so this is kind of embarrassing, but we actually have a really terrible bug
 in here. And it's a subtle bug and it's a very common bug. And I can't believe I've done it for
 the 20th time in my life, especially on camera. And I could have re-shot the whole thing, but I think
 it's pretty funny. And you get to appreciate a bit what working with neural nets maybe is like
 sometimes. We are guilty of a common bug. I've actually tweeted the most common neural mistakes
 a long time ago now. And I'm not really going to explain any of these except for we are guilty of
 number three. You forgot to zero grad before dot backward. What is that?
 Basically what's happening and it's a subtle bug and I'm not sure if you saw it is that all of
 these weights here have a dot data and a dot grad. And dot grad starts at zero. And then we do
 backward and we fill in the gradients. And then we do an update on the data, but we don't flush
 the grad. It stays there. So when we do the second forward pass and we do backward again,
 remember that all the backward operations do a plus equals on the grad. And so these gradients
 just add up and they never get reset to zero. So basically we didn't zero grad. So here's how
 we zero grad before backward. We need to iterate over all the parameters. And we need to make sure
 that p dot grad is set to zero. We need to reset it to zero just like it is in the constructor.
 So remember all the way here for all these value nodes, grad is reset to zero. And then all these
 backward passes do a plus equals from that grad. But we need to make sure that we reset
 these grads to zero. So that when we do backward, all of them start at zero and the actual backward
 pass accumulates the loss derivatives into the grads. So this is zero grad in PyTorch.
 And we will get a slightly different optimization. Let's reset the neural net. The data is the same.
 This is now I think correct. And we get a much more, you know, we get a much more
 slower descent. We still end up with pretty good results. And we can continue this a bit more
 to get down lower and lower and lower. Yeah. So the only reason that the previous thing
 worked, it's extremely buggy. The only reason that worked is that this is a very, very simple
 problem. And it's very easy for this neural net to fit this data. And so the grads ended up
 accumulating and it effectively gave us a massive step size. And it made us converge extremely fast.
 But basically now we have to do more steps to get to very low values of loss and get
 Y-pred to be really good. We can try to step a bit greater. Yeah, we're going to get closer and closer
 to one minus one. So we're going to do all that's sometimes tricky because you may have lots of
 bugs in the code and your network might actually work, just like ours worked. But chances are is
 that if we had a more complex problem, then actually this bug would have made us not optimize the
 loss very well. And we were only able to get away with it because the problem is very simple.
 So let's now bring everything together and summarize what we learned. What are neural nets?
 Neural nets are these mathematical expressions. Fairly simple mathematical expressions in the case
 of multi-layer perceptron that take input as the data and they take input the weights and the
 parameters of the neural net mathematical expression for the forward pass, followed by a loss function.
 And the loss function tries to measure the accuracy of the predictions. And usually the loss will be low
 when your predictions are matching your targets or where the new network is basically behaving well.
 So we manipulate the loss function so that when the loss is low, the network is doing what you
 wanted to do on your problem. And then we backward the loss, use back propagation to get the gradient.
 And then we know how to tune all the parameters to decrease the loss locally. But then we have to
 iterate that process many times in what's called the gradient descent. So we simply follow the
 gradient information and that minimizes the loss and the losses arranged so that when the loss is
 minimized, the network is doing what you want it to do. And yeah, so we just have a blob of neural
 stuff and we can make it do arbitrary things. And that's what gives neural net their power.
 This is a very tiny network with 41 parameters. But you can build significantly more complicated
 neural nets with billions at this point, almost trillions of parameters. And it's a massive blob
 of neural tissue, simulated neural tissue, roughly speaking. And you can make it do extremely complex
 problems. And these neural nets then have all kinds of very fascinating emergent properties.
 In when you try to make them do significantly hard problems, as in the case of GPT, for example,
 we have massive amounts of text from the internet. And we're trying to get a neural
 nets to predict to take like a few words and try to predict the next word in a sequence.
 That's the learning problem. And it turns out that when you train this on all of internet,
 the neural net actually has like really remarkable emergent properties. But that neural net would
 have hundreds of billions of parameters. But it works on fundamentally these exact same principles.
 The neural net, of course, will be a bit more complex. But otherwise, the value in the gradient
 is there and will be identical. And the gradient descent would be there and would be basically
 identical. But people usually use slightly different updates. This is a very simple stochastic
 gradient set update. And loss function would not be in mean squared error. They would be using
 something called the cross entropy loss for predicting the next token. So there's a few more details,
 but fundamentally, the neural network setup and neural network training is identical and pervasive.
 And now you understand intuitively how that works under the hood. In the beginning of this video,
 I told you that by the end of it, you would understand everything in micro grad and that
 would slowly build it up. Let me briefly prove that to you. So I'm going to step through all the
 code that is in micro grad as of today. Actually, potentially some of the code will change by the
 time you watch this video, because I intend to continue developing micro grad. But let's look
 at what we have so far at least in it that pie is empty. When you go to engine.py, that has the
 value, everything here you should mostly recognize. So we have the data data that grad attributes,
 we have the backward function, we have the previous set of children and the operation that produced
 this value. We have addition multiplication and raising to a scalar power. We have the
 relu nonlinearity, which is slightly different type of nonlinearity than 10 H that we used in
 this video. Both of them are nonlinearities. And notably, 10 H is not actually present in micro grad
 as of right now, but I intend to add it later. With the backward, which is identical, and then
 all of these other operations, which are built up on top of operations here. So values should
 be very recognizable, except for the nonlinearity used in this video. There's no massive difference
 between relu and 10 H and sigmoid and these other nonlinearities. They're all roughly
 equivalent and can be used in MLPs. So I use 10 H because it's a bit smoother, and because it's a
 little bit more complicated than relu. And therefore, it's stressed a little bit more the
 local gradients and working with those derivatives, which I thought would be useful.
 And in the pie is the neural networks library, as I mentioned. So you should recognize identical
 implementation of their own layer and MLP. Notably, or not so much, we have a class module here. There's
 a parent class of all these modules. I did that because there's an end up module class in PyTorch.
 And so this exactly matches that API. And end up module in PyTorch has also a zero grad,
 which I refactored out here. So that's the end of micro grad, really. Then there's a test,
 which you'll see basically creates two chunks of code, one in micro grad and one in PyTorch.
 And we'll make sure that the forward and the backward paths agree identically. For a slightly
 less complicated expression and slightly more complicated expression, everything agrees. So we
 agree with PyTorch and all of these operations. And finally, there's a demo that I pyy and b here.
 And it's a bit more complicated binary classification demo than the one I covered in this lecture.
 So we only had a tiny data set of four examples. Here, we have a bit more complicated example
 with lots of blue points and lots of red points. And we're trying to again, build a binary classifier
 to distinguish two dimensional points as red or blue. It's a bit more complicated MLP here with
 it's bigger MLP. The loss is a bit more complicated because it supports batches. So because our data
 set was so tiny, we always did a forward pass on the entire data set of four examples. But when
 your data set is like a million examples, what we usually do in practice is we basically pick
 out some random subset, we call that a batch. And then we only process the batch forward, backward,
 and update. So we don't have to forward the entire training set. So this supports batching
 because there's a lot more examples here. We do a forward pass. The loss is slightly more different.
 This is a max margin loss that I implement here. The one that we used was the mean squared error
 loss because it's simplest one. There's also the binary cross entropy loss. All of them can be
 used for binary classification and don't make too much of a difference in the simple examples
 that we looked at so far. There's something called L2 regularization used here. This has to do with
 generalization of the neural net and controls the overfitting in machine learning setting. But I did
 not cover these concepts in this video potentially later. And the training loop you should recognize.
 So forward, backward, with zero grad and update and so on. You'll notice that in the update here,
 the learning rate is scaled as a function of number of iterations and it shrinks. And this is
 something called learning rate decay. So in the beginning, you have a high learning rate and as
 the network sort of stabilizes near the end, you bring down the learning rate to get some of the
 fine details in the end. And in the end, we see the decision surface of the neural net. And we see
 that it learned to separate out the red and the blue area based on the data points. So that's
 the slightly more complicated example in the demo demo that I by YMB that you're free to go over.
 But yeah, as of today, that is micro grad. I also wanted to show you a little bit of real stuff so
 that you get to see how this is actually implemented in production grade library like PyTorch. So in
 particular, I wanted to show I wanted to find and show you the backward pass for 10h in PyTorch.
 So here in micro grad, we see that the backward pass for 10h is one minus t square, where t is the
 output of the 10h of x times out that grad, which is a chain rule. So we're looking for something
 that looks like this. Now, I went to PyTorch, which has an open source GitHub code base. And
 I looked through a lot of its code. And honestly, I spent about 15 minutes and I couldn't find 10h.
 And that's because these libraries, unfortunately, they grow in size and entropy. And if you just
 search for 10h, you get apparently 2800 results and 400 and 406 files. So I don't know what these
 files are doing, honestly. And why there are so many mentions of 10h. But unfortunately,
 these libraries are quite complex. They're meant to be used, not really inspected. Eventually,
 I did stumble on someone who tries to change the 10h backward code for some reason. And someone
 here pointed to the CPU kernel and the CUDA kernel for 10h backward. So this so basically depends on
 if you're using PyTorch on a CPU device or on a GPU, which these are different devices and I
 haven't covered this. But this is the 10h backward kernel for CPU. And the reason it's so large is
 that at number one, this is like, if you're using a complex type, which we haven't even talked about,
 if you're using a specific data type of B float 16, which we haven't talked about. And then if
 you're not, then this is the kernel and deep here, we see something that resembles our backward pass.
 So they have eight times one minus B square. So this B, B here must be the output of the 10h. And
 this is the output grad. So here we found it deep inside PyTorch on this location for some reason,
 inside binary ops kernel, when 10h is not actually binary op. And then this is the GPU kernel.
 We're not complex. We're here and here we go with one line of code. So we did find it, but
 basically, unfortunately, these code bases are very large. And micro grad is very, very simple.
 But if you actually want to use real stuff, finding the code for it, you'll actually find that
 difficult. I also wanted to show you a little example here, where PyTorch is showing you how
 you can register a new type of function that you want to add to PyTorch as a Lego building walk.
 So here, if you want to, for example, add a like, John der polynomial three. Here's how you
 could do it, you will register it as a class that, subclass is torch.org rather that function.
 And then you have to tell PyTorch how to forward your new function and how to backward through it.
 So as long as you can do the forward pass of this little function piece that you want to add,
 and as long as you know, the local derivative, local gradients, which are implemented in the
 backward PyTorch will be able to back propagate through your function. And then you can use this
 as a Lego block in a larger Lego castle of all the different Lego blocks that PyTorch already has.
 And so that's the only thing you have to tell PyTorch and everything would just work.
 And you can register new types of functions in this way following this example. And that is
 everything that I wanted to cover in this lecture. So I hope you enjoyed building out
 micrograd with me. I hope you find interesting, insightful, and yeah, I will post a lot of the
 links that are related to this video in the video description below. I will also probably post a
 link to a discussion forum or discussion group where you can ask questions related to this video.
 And then I can answer or someone else can answer your questions. And I may also do a follow-up video
 that answers some of the most common questions. But for now, that's it. I hope you enjoyed it.
 If you did, then please like and subscribe so that YouTube knows to feature this video to
 more people. And that's it for now. I'll see you later.
 Now here's the problem. We know dl by. Wait, what is the problem?
 And that's everything I wanted to cover in this lecture. So I hope you enjoyed us building
 out micrograd. Okay, now let's do the exact same thing for multiply because we can't do
 something like eight times two. Oops. I know what happened there.
 Hi everyone, hope you're well.
 And next up what I'd like to do is I'd like to build out MakeMore.
 Like Micrograd before it, MakeMore is a repository that I have on my GitHub webpage.
 You can look at it. But just like with Micrograd, I'm going to build it out step by step
 and I'm going to spell everything out. So we're going to build it out slowly and together.
 Now, what is MakeMore?
 MakeMore, as the name suggests, makes more of things that you give it.
 So here's an example. Names.txt is an example data set to MakeMore.
 And when you look at names.txt, you'll find that it's a very large data set of names.
 So here's lots of different types of names. In fact, I believe there are 32,000 names
 that I've sort of found randomly on the government website.
 And if you train MakeMore on this data set, it will learn to make more of things like this.
 And in particular, in this case, that will mean more things that sound name-like,
 but are actually unique names. And maybe if you have a baby and you're trying to assign a name,
 maybe you're looking for a cool new sounding unique name, MakeMore might help you.
 So here are some example generations from the neural network once we train it on our data set.
 So here's some example unique names that it will generate.
 Dontal, Iraq, Zendy, and so on.
 And so all these sort of sound name-like, but they're not, of course, names.
 So under the hood, MakeMore is a character-level language model.
 So what that means is that it is treating every single line here as an example.
 And within each example, it's treating them all as sequences of individual characters.
 So R-E-E-S-E is this example. And that's the sequence of characters.
 And that's the level in which we are building out MakeMore.
 And what it means to be a character-level language model then is that it's just sort of modeling those sequences of characters
 and it knows how to predict the next character in the sequence.
 Now, we're actually going to implement a large number of character-level language models
 in terms of the neural networks that are involved in predicting the next character in a sequence.
 So very simple, bygram and bag-of-word models, multi-level perceptrons,
 recurring neural networks, all the way to modern transformers.
 In fact, a transformer that we will build will be basically the equivalent transformer to GPT-2, if you have heard, of GPT.
 So that's kind of a big deal. It's a modern network.
 And by the end of the series, you will actually understand how that works on the level of characters.
 Now, to give you a sense of the extensions here, after characters,
 we will probably spend some time on the word level so that we can generate documents of words,
 not just little segments of characters, but we can generate entire large, much larger documents.
 And then we're probably going to go into images and image text networks, such as Dali, Stable Diffusion, and so on.
 But for now, we have to start here, character-level language modeling. Let's go.
 So like before, we are starting with a completely blank, duper-nold bit page.
 The first thing is I would like to basically load up the dataset, names.txt.
 So we're going to open up names.txt for reading.
 And we're going to read in everything into a massive string.
 And then because it's a massive string, we'd only like the individual words and put them in the list.
 So let's call split lines on that string to get all of our words as a Python list of strings.
 So basically, we can look at, for example, the first 10 words.
 And we have that it's a list of Emma, Olivia, Eva, and so on.
 And if we look at the top of the page here, that is indeed what we see.
 So that's good.
 This list actually makes me feel that this is probably sorted by frequency.
 But, okay, so these are the words.
 Now, we'd like to actually learn a little bit more about this dataset.
 Let's look at the total number of words.
 We expect this to be roughly 32,000.
 And then what is the, for example, shortest word?
 So min of, line of each word for w in words.
 So the shortest word will be length two.
 And max of 1w for w in words.
 So the longest word will be 15 characters.
 So let's now think through our very first language model.
 As I mentioned, a character level language model is predicting the next character in a sequence,
 given already some concrete sequence of characters before it.
 Now, what we have to realize here is that every single word here, like a zabella, is actually quite a few examples packed in to that single word.
 Because what is an existence of a word like a zabella in the dataset telling us really?
 It's saying that the character i is a very likely character to come first in a sequence of a name.
 The character s is likely to come after i.
 The character a is likely to come after i s.
 The character b is very likely to come after i s a.
 And so on all the way to a following a zabella.
 And then there's one more example actually packed in here.
 And that is that after there's a zabella, the word is very likely to end.
 So that's one more sort of explicit piece of information that we have here, that we have to be careful with.
 And so there's a lot packed into a single individual word in terms of the statistical structure of what's likely to follow in these character sequences.
 And then of course we don't have just an individual word.
 We actually have 32,000 of these.
 And so there's a lot of structure here to model.
 Now in the beginning what I'd like to start with is I'd like to start with building a by-gram language model.
 Now in a by-gram language model we're always working with just two characters at a time.
 So we're only looking at one character that we are given and we're trying to predict the next character in the sequence.
 So what characters are likely to follow are, what characters are likely to follow, a, and so on.
 And we're just modeling that kind of a little local structure.
 And we're forgetting the fact that we may have a lot more information.
 We're always just looking at the previous character to predict the next one.
 So it's a very simple and weak language model, but I think it's a great place to start.
 So now let's begin by looking at these by-grams in our dataset and what they look like.
 And these by-grams again are just two characters in a row.
 So for W and words, Hw here is an individual word string.
 We want to iterate for, we want to iterate this word with consecutive characters.
 So two characters at a time sliding in through the word.
 Now a interesting nice way, cute way to do this in Python by the way, is doing something like this.
 For character one, character two in zip of W and W at one.
 One call.
 Print, character one, character two.
 And let's not do all the words, let's just do the first three words.
 And I'm going to show you in a second how this works.
 But for now basically as an example, let's just do the very first word alone, Emma.
 You see how we have a Emma?
 And this will just print emm, emm, emm, emm.
 And the reason this works is because W is the string Emma.
 W at one column is the string Emma and zip takes two iterators and it pairs them up and then creates an iterator over the tuples of their consecutive entries.
 And if any one of these lists is shorter than the other, then it will just halt and return.
 So basically that's why we return EM/MM/MM/MA, but then because this iterator
 is second one here runs out of elements, Zip just ends and that's why we only get
 these tunnels. So pretty cute. So these are the consecutive elements in the first
 word. Now we have to be careful because we actually have more information here
 than just these three examples. As I mentioned, we know that E is very
 likely to come first and we know that A in this case is coming last. So what we
 need to do this is basically we're going to create a special array here,
 characters, and we're going to hallucinate a special start token here. I'm going to
 call it like special start. So this is a list of one element plus W and then
 plus a special end character. And the reason I'm wrapping the list of W here is
 because W is a string Emma, list of W will just have the individual characters in
 the list. And then doing this again now, but not iterating over W's but over the
 characters will give us something like this. So E is likely, so this is a
 by gram of the start character and E. And this is a by gram of the A in the
 special end character. And now we can look at, for example, what this looks like
 for Olivia or Eva. And indeed, we can actually, potentially this for the entire
 dataset, but we won't print that, that's going to be too much. But these are the
 individual character by grams and we can print them. Now in order to learn the
 statistics about which characters are likely to follow other characters, the
 simplest way in the by gram language models is to simply do it by counting.
 So we're basically just going to count how often any one of these combinations
 occurs in the training set in these words. So we're going to need some kind of
 a dictionary that's going to maintain some counts for every one of these
 by grams. So let's use a dictionary B and this will map these by grams. So
 by gram is a tuple of character on character two. And then B at by gram will
 be B dot get of by gram, which is basically the same as B at by gram. But in the case
 that by gram is not in the dictionary B, we would like to buy default return to
 zero plus one. So this will basically add up all the by grams and count how often
 they occur. Let's get rid of printing or rather, let's keep the printing and
 let's just inspect what B is in this case. And we see that many by grams occur just
 a single time. This one allegedly occurred three times. So A was an ending
 character three times. And that's true for all of these words. All of Emma, Olivia
 and Eva and with a. So that's why this occurred three times. Now let's do it for
 all the words. Oops, I should not have printed. I'm going to erase that. Let's
 kill this. Let's just run and now B will have the statistics of the entire
 data set. So these are the counts across all the words of the individual
 by grams. And we could, for example, look at some of the most common ones and least
 common ones. This kind of grows in Python, but the way to do this, the simplest way
 I like is we just use B dot items. B dot items returns the tuples of key value.
 In this case, the keys are the character by grams and the values are the counts.
 And so then what we want to do is we want to do a sorted of this. But by default
 sort is on the first item of a tuple. But we want to sort by the values which are
 the second element of a tuple that is the key value. So we want to use the key
 equals lambda that takes the key value and returns the key value at the one, not at
 zero, but at one, which is the count. So we want to sort by the count of these
 elements. And actually we wanted to go backwards. So here we have is the
 bygram Q and R occurs only a single time. DZ occurred only a single time. And when
 we sort this the other way around, we're going to see the most likely by grams. So
 we see that n was very often an ending character many, many times. And
 apparently n almost always follows an A. And that's a very likely combination as
 well. So this is kind of the individual counts that we achieve over the entire
 dataset. Now it's actually going to be significantly more convenient for us to
 keep this information in a two dimensional array instead of a Python
 dictionary. So we're going to store this information in a 2D array. And the rows
 are going to be the first character of the bygram and the columns are going to
 be the second character. And each entry in the two dimensional array will tell us
 how often that first character follows the second character in the dataset. So
 in particular the array representation that we're going to use or the library is
 that of PyTorch. And PyTorch is a deep learning neural work framework. But part
 of it is also this torch.tensor, which allows us to create multi-dimensional
 arrays and manipulate them very efficiently. So let's import PyTorch, which
 you can do by import torch. And then we can create arrays. So let's create a
 array of zeros. And we give it a size of this array. Let's create a 3x5 array as an
 example. And this is a 3x5 array of zeros. And by default you'll notice a.d type,
 which is short for data type, is float 32. So these are single precision floating
 numbers. Because we are going to represent counts, let's actually use d type as
 torch.in 32. So these are 32 bit integers. So now you see that we have integer data
 inside this tensor. Now tensors allow us to really manipulate all the individual
 entries and do it very efficiently. So for example, if we want to change this bit,
 we have to index into the tensor. And in particular, here, this is the first row
 and the, because it's zero indexed. So this is row index one and column index 0, 1,
 2, 3. So at 1, 3, we can set that to 1. And then a, we'll have a 1 over there.
 We can of course also do things like this. So now a will be 2 over there. Or 3.
 And also we can, for example, say a 0 0 is 5. And then a will have a 5 over here. So
 that's how we can index into the arrays. Now of course, the array that we are
 interested in is much, much bigger. So for our purposes, we have 26 letters of the
 alphabet. And then we have two special characters s and e. So we want 26 plus 2
 or 28 by 28 array. And let's call it the capital N, because it's going to
 represent sort of the counts. Let me erase this stuff. So that's the array that starts
 at zeros 28 by 28. And now let's copy paste this here. But instead of having
 a dictionary B, which we're going to erase, we have an N. Now the problem here is that
 we have these characters, which are strings, but we have to now basically
 index into a array, and we have to index using integers. So we need some kind of a
 lookup table from characters to integers. So let's construct such a character array.
 And the way we're going to do this is we're going to take all the words, which is a list
 of strings, we're going to concatenate all of it into a massive string. So this is just
 simply the entire data set as a single string. We're going to pass this to the set constructor,
 which takes this massive string and throws out duplicates, because sets do not allow
 duplicates. So set of this will just be the set of all the lowercase characters. And there
 should be a total of 26 of them. And now we actually don't want a set, we want a list.
 But we don't want a list sorted in some weird arbitrary way, we want it to be sorted from
 A to Z. So sorted list. So those are our characters. Now what we want is this lookup table, as
 I mentioned. So let's create a special s to I, I will call it s is string or character.
 And this will be an s to I mapping for is in enumerate of these characters. So enumerate
 basically gives us this iterator over the integer index and the actual element of the
 list. And then we are mapping the character to the integer. So s to I is a mapping from
 A to zero, B to one, etc, all the way from Z to 25. And that's going to be useful here.
 But we actually also have to specifically set that s will be 26. And s to I at E will
 be 27, right, because Z was 25. So those are the lookups. And now we can come here and
 we can map both character one and character two to their integers. So this will be s to
 I character one. And I X two will be s to I of character two. And now we should be able
 to do this line by using our array. So and at X one, I X two, this is the two dimensional
 array indexing I showed you before. And honestly, just plus equals one, because everything starts
 at zero. So this should work and give us a large 28 by 20 array of all these counts.
 So if we print n, this is the array, but of course it looks ugly. So let's erase this
 ugly mess and let's try to visualize it a bit more nicer. So for that, we're going
 to use a library called matplotlib. So matplotlib allows us to create figures. So we can do
 things like PLT, I am show of the counter a. So this is the 20 by 20 array. And this is
 a structure. But even this, I would say is still pretty ugly. So we're going to try
 to create a much nicer visualization of it. And I wrote a bunch of code for that. The
 first thing we're going to need is we're going to need to invert this array here, this dictionary.
 So s to I is a mapping from s to I. And in I to s, we're going to reverse this dictionary.
 So it rated with all the items and just reverse that array. So I to s maps inversely from
 zero to a want to be etc. So we'll need that. And then here's the code that I came up with
 to try to make this a little bit nicer. We create a figure, we plot n. And then we do
 and then we visualize much of things here. Let me just run it so you get a sense of what
 it says. Okay. So you see here that we have the array spaced out. And every one of these
 is basically like B follows G zero times B follows H 41 times. So a follows J 175 times.
 And so what you can see that I'm doing here is first I show that entire array. And then
 I iterate over all the individual little cells here. And I create a character string here,
 which is the inverse mapping I to s off the integer I and the integer J. So that's those
 are the biograms in a character representation. And then I plot just the by gram text. And
 then I plot the number of times that is by gram occurs. Now the reason that there's a
 dot item here is because when you index into these arrays, these are torch tensors. You
 see that we still get a tensor back. So the type of this thing, you think it would be
 just an integer 149, but it's actually a torch dot tensor. And so if you do dot item,
 then it will pop out that individual integer. So it'll just be 149. So that's what's happening
 there. And these are just some options to make it look nice. So what is the structure
 of this array? We have all these counts. And we see that some of them occur often, and
 some of them do not occur often. Now, if you scrutinize this carefully, you will notice
 that we're not actually being very clever. That's because when you come over here, you'll
 notice that for example, we have an entire row of completely zeros. And that's because
 the end character is never possibly going to be the first character of a by gram, because
 we're always placing these end tokens all at the end of a by gram. Similarly, we have
 entire column zeros here, because the s character will never possibly be the second element
 of a by gram, because we always start with s and we end with e, and we only have the words
 in between. So we have an entire column of zeros, an entire row of zeros. And in this
 little two by two matrix here as well, the only one that can possibly happen is if s directly
 follows e, that can be non zero, if we have a word that has no letters. So in that case,
 there's no letters in the word, it's an empty word, and we just have s follows e. But the
 other ones are just not possible. And so we're basically wasting space. And not only that,
 but the s and the e are getting very crowded here. I was using these brackets because there's
 convention in natural language processing to use these kinds of brackets to denote special
 tokens. But we're going to use something else. So let's fix all this and make it prettier.
 We're not actually going to have two special tokens, we're only going to have one special
 token. So we're going to have n by n array of 27 by set 27 instead. Instead of having two,
 we will just have one, and I will call it a dot. Okay, let me swing this over here. Now,
 one more thing that I would like to do is I would actually like to make this special
 character have position zero. And I would like to offset all the other letters off. I
 find that a little bit more pleasing. So we need a plus one here so that the first character,
 which is a, we'll start at one. So s to I will now be a starts at one and dot is zero. And
 I to s of course, we're not changing this because I to s just creates a reverse mapping.
 And this will work fine. So one is a two is B zero is dot. So we reverse that here. We
 have a dot and a dot. This should work fine. Make sure I started zeros count. And then
 here we don't go up to 28, we go up to 27. And this should just work. Okay. So we see
 that dot dot never happened. It's at zero because we don't have empty words. Then this
 row here now is just very simply the counts for all the first letters. So j starts a
 word, h starts a word, I start a word, etc. And then these are all the ending characters.
 And in between, we have the structure of what characters follow each other. So this is the
 counts array of our entire data set. So this array actually has all the information necessary
 for us to actually sample from this by gram character level language model. And roughly
 speaking, what we're going to do is we're just going to start following these probabilities
 and these counts. And we're going to start sampling from the from model. So in the beginning,
 of course, we start with the dot, the start token dot. So to sample the first character
 of a name, we're looking at this row here. So we see that we have the counts and those
 counts externally are telling us how often any one of these characters is to start a
 word. So if we take this n and we grab the first row, we can do that by using just indexing
 at zero, and then using this notation column for the rest of that row. So n zero column
 is indexing into the zero row, and then grabbing all the columns. And so this will give us a
 one dimensional array of the first row. So zero, four, four, ten, you know, zero, four,
 four, ten, one, three, oh, six, one, five, four, two, etc. It's just the first row. The
 shape of this is 27, it's just the row of 27. And the other way that you can do this
 also is you just you don't actually give this, you just grab the zero row like this. This
 is equivalent. Now these are the counts. And now what we'd like to do is we'd like to
 basically sample from this. Since these are two raw counts, we actually have to convert
 this to probabilities. So we create a probability vector. So we'll take n of zero, and we'll
 actually convert this to float first. Okay, so these integers are converted to float,
 floating point numbers. And the reason we're creating floats is because we're about to
 normalize these counts. So to create a probability distribution here, we want to divide, we basically
 want to do p p divide p dot sum. And now we get a vector of smaller numbers. And these
 are now probabilities. So of course, because we divided by the sum, the sum of p now is
 one. So this is a nice proper probability distribution. It sums to one. And this is
 giving us the probability for any single character to be the first character of a word. So now
 we can try to sample from this distribution. To sample from these distributions, we're
 going to use torched up multinomial, which I've pulled up here. So torched up multinomial
 returns samples from the multinomial probability distribution, which is a complicated way of
 saying you give me probabilities and I will give you integers, which are sampled according
 to the property distribution. So this is the signature of the method. And to make everything
 deterministic, we're going to use a generator object in PyTorch. So this makes everything
 deterministic. So you when you run this on your computer, you're going to get the exact
 same results that I'm getting here on my computer. So let me show you how this works.
 Here's the deterministic way of creating a torch generator object, seeding it with some
 number that we can agree on. So that seeds a generator gets gives us an object g. And
 then we can pass that g to a function that creates here random numbers, torched up random
 numbers, three of them. And it's using this generator object to as a source of randomness.
 So without normalizing it, I can just print. This is sort of like numbers between zero
 and one that are random according to this thing. And whenever I run it again, I'm always
 going to get the same result, because I keep using the same generator object, which I'm
 seeding here. And then if I divide to normalize, I'm going to get a nice probability distribution
 of just three elements. And then we can use torch of multinomial to draw samples from
 it. So this is what that looks like. Torched up multinomial will take the torch tensor
 of probability distributions. Then we can ask for a number of samples, let's say 20.
 Replacement equals true means that when we draw an element, we will we can draw it and
 then we can put it back into the list of eligible indices to draw again. And we have to specify
 replacement as true because by default, for some reason, it's false. And I think, you
 know, it's just something to be careful with. And the generator is passed in here. So we
 are going to always get deterministic results, the same results. So if I run these two, we're
 going to get a bunch of samples from this distribution. Now, you'll notice here that the
 probability for the first element in this tensor is 60%. So in these 20 samples, we'd expect
 60% of them to be zero. We'd expect 30% of them to be one. And because the the element
 index two has only 10% probability, very few of these samples should be two. And indeed,
 we only have a small number of twos. And we can sample as many as we'd like. And the
 more we sample, the more these numbers should roughly have the distribution here. So we
 should have lots of zeros, half as many, once. And we should have three times as few, sorry,
 as few once, and three times as few twos. So you see that we have very few twos, we have
 some ones and most of them are zero. So that's what Torxian Multiplaal is doing. For us here,
 we are interested in this row, we've created this p here. And now we can sample from it.
 So if we use the same seed, and then we sample from this distribution, let's just get one
 sample. Then we see that the sample is say 13. So this will be the index. And let's you
 see how it's a tensor that wraps 13, we again have to use that item to pop out that integer.
 And now index would be just the number 13. And of course, the we can do, we can map the
 I to S of I X to figure out exactly which character we're sampling here, we're sampling
 M. So we're saying that the first character is in our generation. And just looking at
 the row here, M was drawn. And you can see that M actually starts a large number of words.
 M started 2500 words out of 32,000 words. So almost a bit less than 10% of the words
 start with M. So this was actually fairly likely character to draw. So that would be
 the first character of our word. And now we can continue to sample more characters, because
 now we know that M started, M is already sampled. So now to draw the next character,
 we will come back here, and we will look for the row that starts with M. So you see M.
 And we have a row here. So we see that M dot is 516 ma is this many, and B is this many,
 etc. So these are the counts for the next row. And that's the next character that we
 are going to now generate. So I think we are ready to actually just write out the loop,
 because I think you're starting to get a sense of how this is going to go. The we always
 begin at index zero, because that's the start token. And then while true, we're going to
 grab the row corresponding to index that we're currently on. So that's P. So that's
 n array at I X, converted to float is our P. Then we normalize this P to sum to one.
 Accidentally, ran the infinite loop, we normalize P to sum to one. Then we need this generator
 object. We're going to initialize up here, and we're going to draw a single sample from
 this distribution. And then this is going to tell us what index is going to be next.
 If the index sampled is zero, then that's now the end token. So we will break. Otherwise,
 we are going to print s to I of X. I to s of X. And that's pretty much it. We're just
 this work. Okay, more. So that's the that's the name that we've sampled. We started with
 M, the next stop was O, then R, and then dot. And this dot we printed here as well. So
 let's not do this a few times. So let's actually create an out list here. And instead of printing,
 we're going to append. So out that append this character. And then here, let's just print
 it at the end. So let's just join up all the outs. And we're just going to print more.
 Okay, now we're always getting the same result because of the generator. So who want to do
 this a few times, we can go for high and range 10, we can sample 10 names. And we can just
 do that 10 times. And these are the names that we're getting out. Let's do 20. I'll be honest
 with you, this doesn't look right. So I started a few minutes to convince myself that it actually
 is right. The reason these samples are so terrible is that by gram language model is actually
 look just like really terrible. We can generate a few more here. And you can see that they're
 kind of like their name like a little bit like Yannu, Erile, etc. But they're just like totally
 messed up. And I mean, the reason that this is so bad, like we're generating H as a name.
 But you have to think through it from the model size, it doesn't know that this H is
 the very first H. All it knows is that H was previously. And now how likely is H the last
 character? Well, it's somewhat likely. And so it just makes it last character. It doesn't
 know that there were other things before it or there were not other things before it.
 And so that's why generating all these like nonsense names. Another way to do this is
 to convince yourself that it is actually doing something reasonable, even though it's so
 terrible is these little piece here are 27, right? Like 27. So how about if we did something
 like this? Instead of p having any structure whatsoever, how about if p was just torch
 dot ones of 27? By default, this is a float 32. So this is fine. Divide 27. So what I'm
 doing here is this is the uniform distribution, which will make everything equally likely.
 And we can sample from that. So let's see if that does any better. Okay. So it's, this
 is what you have from a model that is completely untrained where everything is equally likely.
 So it's obviously garbage. And then if we have a trained model, which is trained on
 just by grams, this is what we get. So you can see that it is more name like it is actually
 working. It's just by gram is so terrible. And we have to do better. Now next, I would
 like to fix an inefficiency that we have going on here. Because what we're doing here is
 we're always fetching a row of n from the counts matrix up ahead. And we're always doing
 the same things. We're converting to float and we're dividing. And we're doing this
 every single iteration of this loop. And we just keep renormalizing these rows over and
 over again, and it's extremely inefficient and wasteful. So what I'd like to do is I'd
 like to actually prepare a matrix capital P that will just have the probabilities in it.
 So in other words, it's going to be the same as the capital n matrix here of counts. But
 every single row will have the row of probabilities that is normalized to one, indicating the
 probability distribution for the next character, given the character before it, as defined
 by which row we're in. So basically, what we'd like to do is we'd like to just do it
 upfront here. And then we would like to just use that row here. So here, we would like
 to just do P equals P of I X instead. Okay. The other reason I want to do this is not
 just for efficiency, but also I would like us to practice these and dimensional tensors.
 And I'd like us to practice their manipulation, and especially something that's called broadcasting
 that we'll go into in a second. We're actually going to have to become very good at these
 tensor manipulations, because we're going to build out all the way to transformers. We're
 going to be doing some pretty complicated array operations for efficiency. And we need to
 really understand that and be very good at it. So intuitively, what we want to do is we
 first want to grab the floating point copy of N. And I'm mimicking the line here, basically.
 And then we want to divide all the rows so that they sum to one. So we'd like to do something
 like this, P divide P dot sum. But now we have to be careful, because P dot sum actually
 produces a sum, sorry, P equals N dot float copy. P dot sum produces a sums up all of
 the counts of this entire matrix N, and gives us a single number of just the summation of
 everything. So that's not the way we want to divide. We want to simultaneously and
 imperil divide all the rows by their respective sums. So what we have to do now is we have
 to go into documentation for towards dot sum. And we can scroll down here to a definition
 that is relevant to us, which is where we don't only provide an input array that we
 want to sum, but we also provide the dimension along which we want to sum. And in particular,
 we want to sum up over rows, right? Now, one more argument that I want you to pay attention
 to here is the keep them as false. If keep them is true, then the output tensor is of
 the same size as input, except of course, the dimension along which you summed, which will
 become just one. But if you pass in, keep them as false, then this dimension is squeezed
 out. And so towards dot sum, not only does the sum and collapses dimension to be of size
 one, but in addition, it does what's called a squeeze, where it squeezes out that dimension.
 So basically, what we want here is we instead want to do P dot sum of sum axis. And in
 particular, notice that P dot shape is 27 by 27. So when we sum up across axis zero, then
 we would be taking the zero dimension, and we would be summing across it. So we keep
 them as true. Then this thing will not only give us the counts across along the columns.
 But notice that basically the shape of this is one by 27, we just get a row vector. And
 the reason we get a row vector here again is because we've got 10 zero dimension. So this
 zero dimension becomes one, and we've done a sum. And we get a row. And so basically,
 we've done the sum this way vertically, and arrived at just a single one by 27 vector
 of counts. What happens when you take out, keep them, is that we just get 27. So it squeezes
 out that dimension, and we just get one dimensional vector of size 27. Now we don't actually want
 by 27 row vector, because that gives us the counts or the sums across the columns. We
 actually want to sum the other way along dimension one. And you'll see that the shape
 of this is 27 by one. So it's a column vector. It's a 27 by one vector of counts. Okay.
 And that's because what's happened here is that we're going horizontally, and this 27
 by 27 matrix becomes a 27 by one array. Now you'll notice by the way that the actual numbers
 of these counts are identical. And that's because this special array of counts here comes
 from by grams to the sticks. And actually, it just so happens by chance, or because of
 the way this array is constructed, that this sums along the columns or along the rows horizontally
 or vertically is identical. But actually, what we want to do in this case is we want
 to sum across the rows horizontally. So what we want here is be that some of one would
 keep them true, 27 by one column vector. And now what we want to do is we want to divide
 by that. Now we have to be careful here again. Is it possible to take what's a p dot shape
 you see here is 27 by 27? Is it possible to take a 27 by 27 array and divided by what
 is a 27 by one array? Is that a operation that you can do? And whether or not you can
 perform this operation is determined by what's called broadcasting rules. So if you just
 search broadcasting semantics in torch, you'll notice that there's a special definition for
 what's called broadcasting that for whether or not these two arrays can be combined in
 a binary operation like division. So the first condition is each tensor has at least one dimension,
 which is the case for us. And then when iterating over the dimension sizes, starting at the trailing
 dimension, the dimension sizes must either be equal, one of them is one or one of them
 does not exist. Okay, so let's do that. We need to align the two arrays and their shapes,
 which is very easy because both of these shapes have two elements. So they're aligned.
 Then we iterate over from the from the right and going to the left. Each dimension must
 be either equal, one of them is a one or one of them does not exist. So in this case, they're
 not equal, but one of them is a one. So this is fine. And then this dimension, they're
 both equal. So this is fine. So all the dimensions are fine. And therefore the this operation
 is broadcastable. So that means that this operation is allowed. And what is it that
 these arrays do when you divide 27 by 27 by 27 by one? What it does is that it takes this
 dimension one and it stretches it out, it copies it to match 27 here in this case. So
 in our case, it takes this column vector, which is 27 by one, and it copies it 27 times
 to make these both be 27 by 27 internally. You can think of it that way. And so it copies
 those counts. And then it does an element wise division, which is what we want because
 these counts, we want to divide by them on every single one of these columns in this
 matrix. So this actually we expect will normalize every single row. And we can check that this
 is true by taking the first row, for example, and taking it some, we expect this to be one
 because it's non normalized. And then we expect this now, because if we actually correctly
 normalize all the rows, we expect to get the exact same result here. So let's run this.
 It's the exact same result. So this is correct. So now I would like to scare you a little
 bit. You actually have to like, I basically encourage you very strongly to read through
 broadcasting semantics. And I encourage you to treat this with respect. And it's not
 something to play fast and loose with, it's something to really respect, really understand,
 and look up maybe some tutorials for broadcasting and practice it and be careful with it because
 you can very quickly run it two bucks. Let me show you what I mean. You see how here we
 have p dot sum of one keep them this true. The shape of this is 27 by one. Let me take
 out this line just so we have the n, and then we can see the counts. And we can see that
 this is a all the counts across all the rows. And it's 27 by one column vector, right? Now
 suppose that I tried to do the following, but I erase keep them just true here. What does
 that do? If keep them is not true, it's false. Then remember, according to documentation,
 it gets rid of this dimension one, it squeezes it out. So basically, we just get all the same
 counts, the same result, except the shape of it is not 27 by one, it is just 27, the one
 disappears. But all the counts are the same. So you'd think that this divide that would
 work. First of all, can we even write this and will it even is it even expected to run
 is it broadcastable? Let's determine if this result is broadcastable. P dot sum at one
 is shape is 27. This is 27 by 27. So 27 by 27 broadcasting into 27. So now, rules of broadcasting
 number one align all the dimensions on the right done. Now, it was over all the dimensions
 started from the right going to the left. All the dimensions must either be equal. One
 of them must be one or one that does not exist. So here they are all equal. Here, the dimension
 does not exist. So internally, what broadcasting will do is it will create a one here. And
 then we see that one of them is a one, and this will get copied. And this will run this
 will broadcast. Okay, so you'd expect this to work. Because we are the broadcast and
 this we can divide this. Now, if I run this, you'd expect it to work, but it doesn't. You
 actually get garbage. You get a wrong result, because this is actually a bug. This keep
 them equal equals true. Makes it work. This is a bug. In both cases, we are doing the
 correct counts. We are summing up across the rows, but keep them as saving us and making
 it work. So in this case, I'd like you to encourage you to potentially like pause this
 video at this point and try to think about why this is buggy and why the keep them was
 necessary here. Okay, so the reason to do for this is I'm trying to hint it here when
 I was sort of giving you a bit of a hint on how this works. This 27 vector internally
 inside the broadcasting, this becomes a one by 27. And one by 27 is a row vector, right?
 And now we are dividing 27 by 27 by one by 27, and torch will replicate this dimension.
 So basically, it will take this row vector and it will copy it vertically now 27 times.
 So the 27 by 27 lies exactly and element wise divides. And so basically what's happening
 here is we're actually normalizing the columns instead of normalizing the rows. So you can
 check that what's happening here is that p at zero, which is the first row of p dot sum
 is not one, it's seven. It is the first column as an example that sums to one. So to summarize,
 where does the issue come from? The issue comes from the silent adding of a dimension
 here, because in broadcasting rules, you align on the right and go from right to left. And
 if dimension doesn't exist, you create it. So that's where the problem happens. We still
 did the counts correctly. We did the counts across the rows, and we got the counts on
 the right here as a column vector. But because the key things was true, this this dimension
 was discarded. And now we just have a vector of 27. And because of broadcasting the way
 it works, this vector of 27 suddenly becomes a row vector. And then this row vector gets
 replicated vertically, and that every single point we are dividing by the count in the
 opposite direction. So this thing just doesn't work. This needs to be keep them as equals
 true in this case. So then we have that p at zero is normalized. And conversely, the first
 column you'd expect to potentially not be normalized. And this is what makes it work.
 So pretty subtle. And hopefully this helps to scare you that you should have respect for
 broadcasting. Be careful. Check your work and understand how it works under the hood and
 make sure that it's broadcasting in the direction that you like. Otherwise, you're going to
 introduce very subtle bugs, very hard to find bugs. And just be careful. One more note to
 an efficiency. We don't want to be doing this here, because this creates a completely new
 tensor that we store into P, we prefer to use in place operations if possible. So this would
 be an in place operation has the potential to be faster. It doesn't create new memory
 under the hood. And then let's erase this. We don't need it. And let's also just do fewer
 just so I'm not wasting space. Okay, so we're actually in a pretty good spot now. We trained
 a by gram language model, and we trained it really just by counting how frequently any
 pairing occurs and then normalizing so that we get a nice property distribution. So really
 these elements of this array P are really the parameters of our by gram language model,
 giving us and summarizing the statistics of these by grams. So we train the model, and
 then we know how to sample from the model. We just iteratively sampled the next character
 and feed it in each time and get a next character. Now what I'd like to do is I'd like to somehow
 evaluate the quality of this model. We'd like to somehow summarize the quality of this model
 into a single number. How good is it at predicting the training set? And as an example, so in
 the training set, we can evaluate now the training loss. And this training loss is telling us about
 sort of the quality of this model in a single number, just like we saw in micro grad.
 So let's try to think through the quality of the model and how we would evaluate it.
 Basically, what we're going to do is we're going to copy paste this code that we previously
 used for counting. Okay, and let me just print these by grams first. We're going to use F strings,
 and I'm going to print character one followed by character two. These are the by grams. And then
 I don't want to do it for all the words. Let's just do first three words. So here we have Emma,
 Olivia and Eva by grams. Now what we'd like to do is we'd like to basically look at the probability
 that the model assigns to every one of these by grams. So in other words, we can look at the
 probability, which is summarized in the matrix B of I X one, I X two. And then we can print it here
 as probability. And because these properties are way too large, let me present, or call in point
 for F to like truncate it a bit. So what do we have here, right? We're looking at the probabilities
 that the model assigns to everyone on these by grams in the data set. And so we can see some of them
 are 4%, 3%, etc. Just to have a measuring stick in our mind, by the way, we have 27 possible characters
 or tokens. And if everything was equally likely, then you'd expect all these probabilities to be
 4% roughly. So anything above 4% means that we've learned something useful from these by grams
 statistics. And you see that roughly some of these are 4%, but some of them are as high as 40%,
 35%. And so on. So you see that the model actually assigned a pretty high probability to whatever's
 in the training set. And so that that's a good thing. Basically, if you have a very good model,
 you'd expect that these probabilities should be near one, because that means that your model is
 correctly predicting what's going to come next, especially on the training set, where you,
 where you train your model. So now we'd like to think about how can we summarize these probabilities
 into a single number that measures the quality of this model. Now, when you look at the literature
 into maximum likelihood estimation and statistical modeling and so on, you'll see that what's
 typically used here is something called the likelihood. And the likelihood is the product
 of all of these probabilities. And so the product of all these probabilities is the likelihood.
 And it's really telling us about the probability of the entire data set assigned,
 assigned by the model that we've trained. And that is a measure of quality. So the product of these
 should be as high as possible. When you are training the model and when you have a good model,
 your product of these probabilities should be very high. Now, because the product of these
 probabilities is an unwieldy thing to work with, you can see that all of them are between 0 and 1.
 So your product of these probabilities will be a very tiny number. So for convenience,
 what people work with usually is not the likelihood, but they work with what's called the log likelihood.
 So the product of these is the likelihood. To get the log likelihood, we just have to take the log
 of the probability. And so the log of the probability here, the log of x from 0 to 1,
 the log is a, you see here monotonic transformation of the probability, where if you pass in
 1, you get 0. So probability 1 gets your log probability of 0. And then as you go lower and
 lower probability, the log will grow more and more negative until all the way to negative infinity at
 0. So here, we have a log problem, which is really just the torch dot log of probability.
 Let's print it out to get a sense of what that looks like. Log problem also 0.4f.
 Okay. So as you can see, when we plug in numbers that are very close, some of our higher numbers,
 we get closer and closer to 0. And then if we plug in very bad probabilities, we get more and more
 negative number. That's bad. So, and the reason we work with this is for a large extent convenience,
 right? Because we have mathematically that if you have some product a times b times c of all these
 probabilities, right? The likelihood is the product of all these probabilities. Then the log of these
 is just log of a plus log of b plus log of c. If you remember your logs from your high school or
 undergrad and so on. So we have that basically, the likelihood is the product of probabilities,
 the log likelihood is just the sum of the logs of the individual probabilities. So log likelihood
 starts at 0. And then log likelihood here, we can just accumulate simply.
 And then the end, we can print this.
 Print the log likelihood. F strings. Maybe you're familiar with this. So log likelihood is negative
 38. Okay. Now, we actually want. So how high can log likelihood get? It can go to 0. So when
 all the probabilities are one, log likelihood will be 0. And then when all the probabilities are lower,
 this will grow more and more negative. Now, we don't actually like this because what we'd like
 is a loss function. And a loss function has the semantics that low is good, because we're trying
 to minimize the loss. So we actually need to invert this. And that's what gives us something called
 the negative log likelihood. Negative log likelihood is just negative of the log likelihood.
 These are F strings, by the way, if you'd like to look this up, negative log likelihood equals.
 So negative log likelihood, now it's just negative of it. And so the negative log
 likelihood is a very nice loss function, because the lowest it can get is zero. And the higher it
 is, the worse off the predictions are that you're making. And then one more modification to this
 that sometimes people do is that for convenience, they actually like to normalize by they like to
 make it an average instead of a sum. And so here, let's just keep some counts as well. So n plus
 equals one starts at zero. And then here, we can have sort of like a normalized log likelihood.
 If we just normalize it by the count, then we will sort of get the average log likelihood.
 So this would be usually our loss function here, this is what we would use. So our loss
 function for the training set assigned by the model is 2.4. That's the quality of this model.
 And the lower it is, the better off we are. And the higher it is, the worse off we are.
 And the job of our training is to find the parameters that minimize the negative log likelihood loss.
 And that would be like a high quality model. Okay, so to summarize, I actually wrote it out here.
 So our goal is to maximize likelihood, which is the product of all the probabilities
 assigned by the model. And we want to maximize this likelihood with respect to the model parameters.
 And in our case, the model parameters here are defined in the table. These numbers, the
 probabilities are the model parameters sort of in our Breguem language models so far.
 But you have to keep in mind that here we are storing everything in a table format, the probabilities.
 But what's coming up as a brief preview is that these numbers will not be kept explicitly,
 but these numbers will be calculated by a neural network. So that's coming up.
 And we want to change and tune the parameters of these neural arcs. We want to change these
 parameters to maximize the likelihood, the product of the probabilities. Now, maximizing the likelihood
 is equivalent to maximizing the log likelihood, because log is a monotonic function.
 Here's the graph of log. And basically, all it is doing is it's just scaling your,
 you can look at it as just a scaling of the loss function. And so the optimization problem here,
 and here are actually equivalent, because this is just a scaling, you can look at it that way.
 And so these are two identical optimization problems.
 Maximizing the log likelihood is equivalent to minimizing the negative log likelihood.
 And then in practice, people actually minimize the average negative log likelihood to get numbers
 like 2.4. And then this summarizes the quality of your model. And we'd like to minimize it and
 make it as small as possible. And the lowest it can get is zero. And the lower it is, the better
 off your model is, because it's signing, it's assigning high probabilities to your data.
 Now let's estimate the probability over the entire training set, just to make sure that we get
 something around 2.4. Let's run this over the entire, oops, let's take out the print statement as well.
 But I do point four five were the entire training set.
 Now what I'd like to show you is that you can actually evaluate the probability for any word
 that you want. Like for example, if we just test a single word Andre and bring back the print statement,
 then you see that Andre is actually kind of like an unlikely word, like on average,
 we take three log probability to represent it. And roughly that's because EJ apparently is very
 uncommon as an example. Now think through this, when I take Andre and I append q, and I test the
 probability of it under a q, we actually get infinity. And that's because JQ has a zero percent
 probability according to our model. So the log likelihood, so the log of zero will be negative
 infinity, we get infinite loss. So this is kind of undesirable, right, because we plugged in a
 string that could be like a somewhat reasonable name. But basically what this is saying is that
 this model is exactly 0% likely to to predict this name. And our loss is infinity on this example.
 And really what the reason for that is that J is followed by q zero times, where is q? JQ is zero.
 And so JQ is zero percent likely. So it's actually kind of gross. And people don't like this too much.
 To fix this, there's a very simple fix that people like to do to sort of like smooth out your model
 a little bit and it's called model smoothing. And roughly what's happening is that we will
 eight, we will add some fake accounts. So imagine adding a count of one to everything.
 So we add a count of one like this, and then we recalculate the probabilities.
 And that's model smoothing. And you can add as much as you like, you can add five, and it will
 give you a smoother model. And the more you add here, the more uniform model you're going to have.
 And the less you add, the more peaked model you're going to have, of course. So one is like a pretty
 decent count to add. And that will ensure that there will be no zeros in our probability matrix p.
 And so this will of course change the generations a little bit. In this case, it didn't, but it
 in principle, it could. But what that's going to do now is that nothing will be infinity unlikely.
 So now our model will predict some other probability. And we see that JQ now has a very small
 probability. So the model still finds it very surprising that this was a word or a diagram,
 but we don't get negative infinity. So it's kind of like a nice fix that people like to apply
 sometimes and it's called models moving. Okay, so we've now trained a respectable
 bygram character level language model. And we saw that we both sort of trained the model by
 looking at the counts of all the bygrams and normalizing the rows to get probability distributions.
 We saw that we can also then use those parameters of this model to perform sampling of new words.
 So we sample new names according to those distributions. And we also saw that we can evaluate the quality
 of this model. And the quality of this model is summarized in a single number, which is the
 negative log likelihood. And the lower this number is, the better the model is, because it is giving
 high probabilities to the actual next characters and all the bygrams in our training set.
 So that's all well and good. But we've arrived at this model explicitly by doing something that
 felt sensible. We were just performing counts. And then we were normalizing those counts.
 Now what I would like to do is I would like to take an alternative approach. We will end up in a
 very, very similar position, but the approach will look very different. Because I would like to cast
 the problem of bygram character level language modeling into the neural network framework.
 And in the neural network framework, we're going to approach things slightly differently, but again,
 end up in a very similar spot. I'll go into that later. Now, our neural network is going to be a
 still a bygram character level language model. So it receives a single character as an input.
 Then there's neural network with some weights or some parameters w. And it's going to output
 the probability distribution over the next character in a sequence. It's going to make guesses as to
 what is likely to follow this character that was input to the model. And then in addition to that,
 we're going to be able to evaluate any setting of the parameters of the neural net, because we
 have the loss function, the negative, a lot likelihood. So we're going to take a look at its probability
 distributions. And we're going to use the labels, which are basically just the identity of the
 next character in that bygram, the second character. So knowing what second character actually comes
 next in the bygram allows us to then look at what how high of probability the model assigns to that
 character. And then we, of course, want the probability to be very high. And that is another
 way of saying that the loss is low. So we're going to use gradient based optimization, then,
 to tune the parameters of this network, because we have the loss function, and we're going to
 minimize it. So we're going to tune the weights so that the neural net is correctly predicting the
 probabilities for the next character. So let's get started. The first thing I want to do is I
 want to compile the training set of this neural network, right? So create the training set of all
 the bygrams. Okay. And here, I'm going to copy paste this code, because this code iterates over
 all the bygrams. So here we start with the words, we iterate over all the bygrams. And previously,
 as you recall, we did the counts. But now we're not going to do counts, we're just creating a
 training set. Now this training set will be made up of two lists. We have the inputs and the targets,
 the the links. And these by grams will denote x, y, those are the characters, right? And so
 we're given the first character of the bygram. And then we're trying to predict the next one.
 Both of these are going to be integers. So here, we'll take x is data, pen is just
 x one, why is that a pen? I x two. And then here, we actually don't want lists of integers. We will
 create tensors out of these. So x is torch dot tensor of x is and wise is torch dot tensor of y's.
 And then we don't actually want to take all the words just yet, because I want everything to be
 manageable. So let's just do the first word, which is Emma. And then it's clear what these
 x's and y's would be. Here, let me print character one character to just so you see what's going on
 here. So the by grams of these characters is dot E, E, M, M, M, A, dot. So this single word,
 as I mentioned, has one, two, three, four, five examples for a neural network. There are five
 separate examples in Emma. And those examples are from us here. When the input to the neural network
 is integer zero, the desired label is integer five, which corresponds to E. When the input to the
 neural network is five, we want its weights to be arranged so that 13 gets a very high probability.
 When 13 is put in, we want 13 to have a high probability. When 13 is put in, we also want one
 to have a high probability. When one is input, we want zero to have a very high probability.
 So there are five separate input examples to a neural net in this data set.
 I wanted to add a tangent of a note of caution to be careful with a lot of the APIs of some of
 these frameworks. You saw me silently use torch dot tensor with a lowercase t and the output looked
 right. But you should be aware that there's actually two ways of constructing a tensor.
 There's a torch dot lowercase tensor and there's also a torch dot capital tensor class, which you
 can also construct. So you can actually call both. You can also do torch dot capital tensor.
 And you get an X as in Y as well. So that's not confusing at all. There are threads on what is
 the difference between these two. And unfortunately, the docs are just like not clear on the difference.
 And when you look at the docs of lowercase tensor, construct tensor with no autograph history by
 copying data. It's just like it doesn't, it doesn't make sense. So the actual difference, as far as I
 can tell, is explained eventually in this random thread that you can Google. And really, it comes
 down to, I believe, that where is this? Torch dot tensor in first the D type, the data type
 automatically. Well, torch dot tensor just returns a float tensor. I would recommend stick to torch
 dot lowercase tensor. So indeed, we see that when I construct this with a capital T, the data type
 here of X is float 32. But torch dot lowercase tensor, you see how it's now X dot D type is now
 integer. So it's advised that you use lowercase T and you can read more about it if you like in
 some of these threads. But basically, I'm pointing out some of these things because I want to caution
 you and I want you to read get used to reading a lot of documentation and reading through a lot of
 Q and A's and threads like this. And you know, some of the stuff is unfortunately not easy and
 not very well documented. And you have to be careful out there. What we want here is integers,
 because that's what makes sense. And so lowercase tensor is what we are using. Okay, now we want
 to think through how we're going to feed in these examples into a neural network. Now it's not
 quite as straightforward as plugging it in, because these examples right now are integers. So there's
 like a zero five or 13, it gives us the index of the character, and you can't just plug an integer
 index into a neural net. These neural nets, right, are sort of made up of these neurons. And these
 neurons have weights. And as you saw in micro grad, these weights act multiplicatively on the inputs
 w x plus b, there's 10 Hs and so on. And so it doesn't really make sense to make an input neuron
 take on integer values that you feed in and then multiply on with weights. So instead, a common way
 of encoding integers is what's called one hot encoding. In one hot encoding, we take an integer
 like 13, and we create a vector that is all zeros, except for the 13th dimension, which we turn to a
 one. And then that vector can feed into in your own net. Now, conveniently, PyTorch actually has
 something called the one hot function inside torch and functional. It takes a tensor made up of integers
 long as a is an integer. And it also takes a number of classes, which is how large you want
 your tensor, your vector to be. So here, let's import torch dot and that functional s f. This is a
 common way of importing it. And then let's do f dot one hot. And we feed in the integers that we
 want to encode. So we can actually feed in the entire array of access. And we can tell it that
 num classes is 27. So it doesn't have to try to guess it, it may have guessed that it's only 13
 and would give us an incorrect result. So this is the one hot. Let's call this x in for x encoded.
 And then we see that x encoded that shape is five by 27. And we can also visualize it P l t
 dot i'm show of x and to make it a little bit more clear, because this is a little messy.
 So we see that we've encoded all the five examples into vectors. We have five examples,
 so we have five rows. And each row here is now an example into a neural mat. And we see that the
 appropriate bit is turned on as a one and everything else is zero. So here, for example, the zero
 of bit is turned on the fifth bit is turned on 13th bits are turned on for both of these examples.
 And the first bit here is turned on. So that's how we can encode integers into vectors. And then
 these vectors can feed in to neural nets. One more issue to be careful with here, by the way, is
 let's look at the data type of vacuum coding. We always want to be careful with data types.
 What would you expect x encodings data type to be when we're plugging numbers into neural
 nets, we don't want them to be integers. We want them to be floating point numbers that can take on
 various values. But the D type here is actually 64 bit integer. And the reason for that, I suspect,
 is that one hot received a 64 bit integer here, and it returned the same data type. And when you
 look at the signature of one hot, it doesn't even take a D type, a desired data type of the output
 tensor. And so we can't in a lot of functions in torture, we'll be able to do something like D
 type equals store dot float 32, which is what we want. But one hot does not support that.
 So instead, we're going to want to cast this to float like this. So that these, everything is the
 same. Everything looks the same. But the D type is float 32, and floats can feed into
 neural nets. So now let's construct our first neuron. This neuron will look at these input vectors.
 And as you remember from micro grad, these neurons basically perform a very simple function w x plus
 b, where w x is a dot product. Right. So we can achieve the same thing here. Let's first define
 the weights of this neuron, basically, where are the initial weights at initialization for this
 neuron? Let's initialize them with touch dot random. Torsh dot random is fills a tensor with random
 numbers drawn from a normal distribution. And a normal distribution has a probability density
 function like this. And so most of the numbers drawn from this distribution will be around zero.
 But some of them will be as high as almost three and so on. And very few numbers will be above
 three in magnitude. So we need to take size as an input here. And I'm going to use size to be 27 by
 1. So 27 by 1, and then let's visualize w. So w is a column vector of 27 numbers.
 And these weights are then multiplied by the inputs. So now to perform this multiplication,
 we can take x encoding, and we can multiply it with w. This is a matrix multiplication operator
 in PyTorch. And the output of this operation is five by one. The reason is five by one is the following.
 We took x encoding, which is five by 27, and we multiplied it by 27 by one.
 And in matrix multiplication, you see that the output will become five by one, because these 27
 will multiply and add. So basically, what we're seeing here out of this operation is we are seeing
 the five activations of this neuron on these five inputs. And we've evaluated all of them in parallel.
 We didn't feed in just a single input to the single neuron. We fed in simultaneously all the
 five inputs into the same neuron. And in parallel, PyTorch has evaluated the wx plus b, but here is
 just wx. There's no bias. It has valued w times x for all of them independently. Now, instead of a
 single neuron, though, I would like to have 27 neurons. And I'll show you in a second why I want
 27 neurons. So instead of having just a one here, which is indicating this presence of one single
 neuron, we can use 27. And then when w is 27 by 27, this will in parallel evaluate all the 27 neurons
 on all the five inputs, giving us a much better, much, much bigger result. So now what we've done
 is five by 27 multiplied 27 by 27. And the output of this is now five by 27. So we can see that the
 shape of this is five by 27. So what is every element here telling us, right? It's telling us for every
 one of 27 neurons that we created. What is the firing rate of those neurons on every one of those
 five examples? So the element, for example, three comma 13 is giving us the firing rate of the 13th
 neuron, looking at the third input. And the way this was achieved is by a dot product
 between the third input and the 13th column of this w matrix here. Okay, so using matrix
 multiplication, we can very efficiently evaluate the dot product between lots of input examples in
 a batch. And lots of neurons, where all those neurons have weights in the columns, those w's.
 And in matrix multiplication, we're just doing those dot products and in parallel. Just to show
 you that this is the case, we can take x and we can take the third row. And we can take the w and
 take its 13th column. And then we can do x and get three element wise multiply with w 13
 and sum that up, that's w x plus b. Well, there's no plus b, it's just w x dot product. And that's
 this number. So you see that this is just being done efficiently by the matrix multiplication
 operation for all the input examples and for all the output neurons of this first layer.
 Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has 27 neurons,
 right? So we have 27 inputs and now we have 27 neurons. These neurons perform w times x,
 they don't have a bias and they don't have a nonlinearity like 10h. We're going to leave them
 to be a linear layer. In addition to that, we're not going to have any other layers. This is going
 to be it. It's just going to be the dumbest, smallest, simplest neural net, which is just a single linear
 layer. And now I'd like to explain what I want those 27 outputs to be. Intuitively, what we're
 trying to produce here for every single input example is we're trying to produce some kind of a
 probability distribution for the next character in a sequence. And there's 27 of them. But we have
 to come up with like precise semantics for exactly how we're going to interpret these 27 numbers
 that these neurons take on. Now intuitively, you see here that these numbers are negative and some
 of them are positive, etc. And that's because these are coming out of a neural net layer,
 initialized with these normal distribution parameters. But what we want is we want something like we
 had here, like each row here told us the counts, and then we normalize the counts to get probabilities.
 And we want something similar to come out of a neural net. But what we just have right now is
 just some negative and positive numbers. Now we want those numbers to somehow represent the
 probabilities for the next character. But you see that probabilities, they have a special structure,
 they're positive numbers, and they sum to one. And so that doesn't just come out of a neural net.
 And then they can't be counts, because these counts are positive and counts are integers.
 So counts are also not really a good thing to output from a neural net. So instead, what the
 neural net is going to output, and how we are going to interpret the 27 numbers, is that these
 27 numbers are giving us log counts, basically. So instead of giving us counts directly,
 like in this table, they're giving us log counts. And to get the counts, we're going to take the
 log counts, and we're going to exponentiate them. Now, exponentiation takes the following form.
 It takes numbers that are negative, or they are positive. It takes the entire real line. And then
 if you plug in negative numbers, you're going to get e to the x, which is always below one.
 So you're getting numbers lower than one. And if you plug in numbers greater than zero,
 you're getting numbers greater than one, all the way, growing to the infinity. And this here grows
 to zero. So basically, we're going to take these numbers here. And instead of them being positive
 and negative in all over the place, we're going to interpret them as log counts. And then we're
 going to element wise, exponentiate these numbers, exponentiating them now gives us something like this.
 And you see that these numbers now, because of that, they went through an exponent, all the
 negative numbers turned into numbers below one, like 0.338. And all the positive numbers,
 originally, turned into even more positive numbers, so we're greater than one. So like,
 for example, seven is some positive number over here that is greater than zero. But exponentiated
 outputs here, basically give us something that we can use and interpret as the equivalent of counts
 originally. So you see these counts here, 112, 751, 1, etc. The neural net is kind of now predicting
 counts. And these counts are positive numbers, they can never be below zero. So that makes sense.
 And they can now take on various values, depending on the settings of W.
 So let me break this down. We're going to interpret these to be the log counts.
 In other words, for this, that is often used is so called logits. These are logits log counts.
 Then these will be sort of the counts, logis exponentiated. And this is equivalent to the
 n matrix, sort of the n array that we used previously. Remember, this was the n.
 This is the array of counts. And each row here are the counts for the next character, sort of.
 So those are the counts. And now the probabilities are just the counts normalized.
 And so I'm not going to find the same, but basically, I'm not going to scroll all the
 place. We've already done this. We want to counts that sum along the first dimension.
 And we want to keep them is true. We've went over this, and this is how we normalize the rows
 of our counts matrix to get our probabilities.
 So now these are the probabilities. And these are the counts that we have currently.
 And now when I show the probabilities, you see that every row here, of course,
 will sum to one, because they're normalized. And the shape of this is five by 27.
 And so really what we've achieved is for every one of our five examples,
 we now have a row that came out of a neural net. And because of the transformations here,
 we made sure that this output of this neural net now are probabilities, or we can interpret to be
 probabilities. So our W X here gave us logits. And then we interpret those to be log counts.
 We exponentiate to get something that looks like counts. And then we normalize those counts to get
 a probability distribution. And all of these are differentiable operations. So what we've done now,
 is we are taking inputs, we have differentiable operations that we can back propagate through,
 and we're getting out probability distributions. So, for example, for the 0th example that fed in,
 which was the 0th example here was a one-out vector of 0. And it basically corresponded to
 feeding in this example here. So we're feeding an dot into a neural net. And the way we fed the
 dot into a neural net is that we first got its index, then we one-hot encoded it,
 then it went into the neural net, and out came this distribution of probabilities. And its shape
 is 27. There's 27 numbers. And we're going to interpret this as the neural net's assignment
 for how likely every one of these characters, the 27 characters, are to come next. And as we
 tune the weights w, we're going to be, of course, getting different probabilities out
 for any character that you input. And so now the question is just, can we optimize and find a good
 w such that the probabilities coming out are pretty good. And the way we measure pretty good
 is by the loss function. Okay, so I organized everything into a single summary so that hopefully
 it's a bit more clear. So it starts here with an input data set. We have some inputs to the neural
 net, and we have some labels for the correct next character in a sequence. These are integers.
 Here I'm using a torch generators now so that you see the same numbers that I see. And I'm generating
 27 neurons weights, and each neuron here receives 27 inputs.
 Then here we're going to plug in all the input examples, x's, into a neural net. So here, this
 is a forward pass. First, we have to encode all of the inputs into one-half representations.
 So we have 27 classes, we pass in these integers, and x, inc, becomes a array that is 5 by 27,
 zeros, except for a few ones. We then multiply this in the first layer of a neural net to get
 logits, exponentiate the logits to get fake counts, sort of, and normalize these counts to get
 probabilities. So these last two lines, by the way, here are called the softmax, which I pulled up
 here. Softmax is a very often used layer in a neural net that takes these z's, which are logits,
 exponentiates them, and divides and normalizes. It's a way of taking outputs of a neural net layer,
 and these outputs can be positive or negative, and it outputs probability distributions. It outputs
 something that is always sums to one, and are positive numbers, just like probabilities.
 So this is kind of like a normalization function, if you want to think of it that way.
 And you can put it on top of any other linear layer inside a neural net, and it basically makes a
 neural net output probabilities that's very often used, and we used it as well here.
 So this is the forward pass, and that's how we made a neural net output probability.
 Now, you'll notice that all of these, this entire forward pass is made up of differentiable
 layers. Everything here, we can back propagate through, and we saw some of the back propagation
 in micrograd. This is just multiplication and addition. All that's happening here is just
 multiplying and add, and we know how to back propagate through them. Exponentiation, we know
 how to back propagate through. And then here, we are summing, and sum is as easily back propagated
 as well, and division as well. So everything here is a differentiable operation, and we can
 back propagate through. Now, we achieve these probabilities, which are five by 27. For every
 single example, we have a vector of probabilities that's under one. And then here, I wrote a
 bunch of stuff to sort of like break down the examples. So we have five examples making up
 Emma, right? And there are five by grams inside Emma. So by gram example, by gram example one
 is that E is the beginning character, right after dot. And the indexes for these are zero and five.
 So then we feed in a zero, that's the input of the neural net. We get probabilities from the
 neural net that are 27 numbers. And then the label is five, because E actually comes after
 dot. So that's the label. And then we use this label five to index into the probability distribution
 here. So this index five here is 012345. It's this number here, which is here. So that's basically
 the probability assigned by the neural net to the actual correct character. You see that the net
 work currently thinks that this next character that E following dot is only 1% likely, which is of
 course not very good, right? Because this actually is a training example. And the network thinks
 that is currently very, very unlikely. But that's just because we didn't get very lucky in generating
 a good setting of W. So right now this network thinks this is unlikely. And 0.01 is not a good
 outcome. So the log likelihood then is very negative. And the negative log likelihood is
 very positive. And so four is a very high negative log likelihood. And that means we're going to
 have a high loss, because what is the loss? The loss is just the average negative log likelihood.
 So the second character is EM. And you see here that also the network thought that M following E
 is very unlikely 1%. For M following M, it thought it was 2%. And for A following M,
 it actually thought it was 7% likely. So just by chance, this one actually has a pretty good
 probability and therefore a pretty low negative log likelihood. And finally here, it thought this
 was 1% likely. So overall, our average negative log likelihood, which is the loss, the total loss
 that summarizes basically the how well this network currently works, at least on this one word,
 not on the full data set, just the one word is 3.76, which is actually very fairly high loss.
 This is not a very good setting of W's. Now here's what we can do. We're currently getting 3.76.
 We can actually come here and we can change our W, we can resample it. So let me just add one to
 have a different seed. And then we get a different W. And then we can rerun this. And with this
 different C with this different setting of W's, we now get 3.37. So this is a much better W, right?
 And that, and it's better because the probabilities just happen to come out higher for the, for the
 characters that actually are next. And so you can imagine actually just resampling this, you know,
 we can try to. So, okay, this was not very good. Let's try one more. We can try three.
 Okay, this was terrible setting because we have a very high loss. So anyway, I'm going to erase this.
 What I'm doing here, which is just guess and check of randomly assigning parameters and seeing if
 the network is good, that is amateur hour. That's not how you optimize a neural net.
 The way you optimize a neural net is you start with some random guess. And we're going to commit
 to this one, even though it's not very good. But now the big deal is we have a loss function.
 So this loss is made up only of differentiable operations. And we can minimize the loss by tuning
 W's by computing the gradients of the loss with respect to these W matrices.
 And so then we can tune W to minimize the loss and find a good setting of W using gradient based
 optimization. So let's see how that will work. Now things are actually going to look almost
 identical to what we had with micro grad. So here, I pulled up the lecture from micro grad,
 the notebook. It's from this repository. And when I scroll all the way to the end where we left
 up with micro grad, we had something very, very similar. We had a number of input examples. In
 this case, we had four input examples inside Xs. And we had their targets, these are targets.
 Just like here, we have our Xs now, but we have five of them. And they're now integers
 instead of vectors. But we're going to convert our integers to vectors,
 except our vectors will be 27 large instead of three large. And then here, what we did is first,
 we did a forward pass where we ran a neural net on all of the inputs to get predictions.
 Our neural net at the time, this n of X was a net, a multi layer perceptron. Our neural net is
 going to look different, because our neural net is just a single layer, single linear layer,
 followed by a softmax. So that's our neural net. And the loss here was the mean squared error.
 So we simply subtracted the prediction from the ground truth and squared it and summed it all up.
 And that was the loss. And loss was the single number that summarized the quality of the neural
 net. And when loss is low, like almost zero, that means the neural net is predicting correctly.
 So we had a single number that that summarized the performance of the neural net. And everything
 here was differentiable and was stored in massive compute graph. And then we iterated over all the
 parameters, we made sure that the gradients are set to zero. And we called lost up backward.
 And lost up backward initiated back propagation at the final output node of loss. Right. So
 yeah, remember these expressions, we had lost all the way at the end, we start back propagation,
 and we went all the way back. And we made sure that we populated all the parameters dot grad.
 So that grad started at zero, but back propagation filled it in. And then in the update, we iterated
 all the parameters. And we simply did a parameter update, where every single element of our parameters
 was not in the opposite direction of the gradient. And so we're going to do the exact same thing
 here. So I'm going to pull this up on the side here. So then we have it available. And we're actually
 going to do the exact same thing. So this was the forward pass. So where we did this. And problems
 is our white bread. So now we have to evaluate the loss, but we're not using the mean squared error.
 We're using the negative log likelihood, because we are doing classification, we're not doing
 regression as it's called. So here, we want to calculate loss. Now the way we calculate it is
 is just this average negative log likelihood. Now this problems here has a shape of five by 27.
 And so to get all the, we basically want to pluck out the probabilities at the correct indices here.
 So in particular, because the labels are stored here in the array wise, basically what we're after
 is for the first example, we're looking at probability of five, right, at index five. For the second
 example, at the second row, or row index one, we are interested in the probability of size to
 index 13. At the second example, we also have 13. At the third row, we want one. And at the last row,
 which is four, we want zero. So these are the probabilities we're interested in, right? And you
 can see that they're not amazing as we saw above. So these are the probabilities we want, but we
 want like a more efficient way to access these probabilities, not just listing them out in a
 tuple like this. So it turns out that the way to do this in PyTorch, one of the ways at least,
 is we can basically pass in all of these integers in vectors. So these ones, you see how they're just
 0, 1, 2, 3, 4, we can actually create that using empty, not empty, sorry, torch dot arrange of five,
 0, 1, 2, 3, 4. So we can index here with torch dot arrange of five. And here we index with wise.
 And you see that that gives us exactly these numbers.
 So that plucks out the probabilities of that the neural network assigns to the correct
 next character. Now we take those probabilities, and we don't, we actually look at the log probability.
 So we want to dot log. And then we want to just average that up. So take the mean of all that.
 And then it's the negative average log likelihood, that is the loss. So the loss here is 3.7 something.
 And you see that this loss 3.76, 3.76 is exactly as we've obtained before. But this is a vectorized
 form of that expression. So we get the same loss. And the same loss, we can consider sort of as part
 of this forward pass. And we've achieved here now loss. Okay, so we made our way all the way to loss.
 We defined the forward pass. We forwarded the network and the loss. Now we're ready to do backward pass.
 So backward pass. We want to first make sure that all the gradients are reset. So they're at 0.
 Now in PyTorch, you can set the gradients to be 0. But you can also just set it to none. And
 setting it to none is more efficient. And PyTorch will interpret none as like a lack of a gradient.
 And it's the same as 0. So this is a way to set to 0, the gradient. And now we do loss.backward.
 Before we do loss.backward, we need one more thing. If you remember from micro grad,
 PyTorch actually requires that we pass in requires grad is true. So that we tell PyTorch that we
 are interested in calculating gradients for this leaf tensor by default. This is false. So let me
 recalculate with that. And then setting none and loss.backward. Now something magical happened when loss
 to backward was run. Because PyTorch, just like micro grad, when we did the forward pass here,
 it keeps track of all the operations under the hood. It builds a full computational graph. Just
 like the graphs we could produce in micro grad, those graphs exist inside PyTorch. And so it
 knows all the dependencies and all the mathematical operations of everything. And when you then
 calculate the loss, we can call a backward on it. And that backward then fills in the gradients
 of all the intermediates all the way back to W's, which are the parameters of our neural net.
 So now we can do WL grad. And we see that it has structure. There's stuff inside it.
 And these gradients, every single element here, so W dot shape is 27 by 27, W grads shape is the
 same 27 by 27. And every element of W dot grad is telling us the influence of that weight on the
 loss function. So for example, this number all the way here, if this element, the zero, zero element
 of W, because the gradient is positive, it's telling us that this has a positive influence on the loss,
 slightly nudging W slightly taking W zero zero, and adding a small h to it would increase the loss
 mildly, because this gradient is positive. Some of these gradients are also negative.
 So that's telling us about the gradient information. And we can use this gradient
 information to update the weights of this neural network. So let's not do the update.
 It's going to be very similar to what we had in micro grad. We need no loop over all the parameters,
 because we only have one parameter tensor and that is W. So we simply do W dot data plus equals
 the we can actually copy this almost exactly negative 0.1 times W dot grad.
 And that would be the update to the tensor. So that updates the tensor.
 And because the tensor is updated, we would expect that now the loss should decrease. So here, if I
 print loss that item, it was 3.76, right? So we've updated the W here. So if I recalculate forward
 pass, loss now should be slightly lower. So 3.76 goes to 3.74. And then we can again set to
 set grad to none and backward update. And now the parameters changed again. So if we recalculate
 the forward pass, we expect a lower loss again, 3.72. Okay, and this is again doing the we're now
 doing reading the set. And when we achieve a low loss, that will mean that the network
 is assigning high probabilities to the correct next characters. Okay, so I rearranged everything
 and I put it all together from scratch. So here is where we construct our data set of diagrams.
 You see that we are still iterating along over the first word Emma. I'm going to change that in a
 second. I added a number that counts the number of elements in Xs so that we explicitly see that
 number of examples is five, because currently we were just working with Emma, there's five
 by yarns there. And here I added a loop of exactly what we had before. So we had 10 iterations of
 very neat descent of forward pass, backward pass and update. And so running these two cells,
 initialization and gradient descent gives us some improvement on the loss function.
 But now I want to use all the words. And there's not five but 228,000
 by grams now. However, this should require no modification whatsoever. Everything should
 just run because all the code we wrote doesn't care if there's five by grams or 228,000
 by grams and with everything we should just work. So you see that this will just run.
 But now we are optimizing over the entire training set of all the by grams. And you see now that we
 are decreasing very slightly. So actually we can probably afford a larger learning rate.
 And probably afford even larger learning rate.
 Even 50 seems to work on this very, very simple example. So let me re-inertilize and let's run
 100 iterations. See what happens. Okay. We seem to be coming up to some pretty good losses here,
 2.47. Let me run 100 more. What is the number that we expect by the way in the loss? We expect
 to get something around what we had originally actually. So all the way back, if you remember,
 in the beginning of this video, when we optimized just by counting, our loss was roughly 2.47
 after we added smoothing. But before smoothing, we had roughly 2.45, likely at
 sorry loss. And so that's actually roughly the vicinity of what we expect to achieve.
 But before we achieved it by counting, and here we are achieving the roughly the same result,
 but with gradient based optimization. So we come to about 2.46, 2.45, etc. And that makes sense,
 because fundamentally we're not taking in any additional information. We're still just taking
 in the previous character and trying to predict the next one. But instead of doing it explicitly
 by counting and normalizing, we are doing it with gradient based learning. And it just so happens
 that the explicit approach happens to very well optimize the loss function without any need for
 gradient based optimization, because the setup for bygram language models is so straightforward,
 that's so simple, we can just afford to estimate those probabilities directly and maintain them
 in a table. But the gradient based approach is significantly more flexible. So we've actually
 gained a lot because what we can do now is we can expand this approach and complexify the neural
 net. So currently we're just taking a single character and feeding into a neural net, and the
 neural that's extremely simple. But we're about to iterate on this substantially. We're going to be
 taking multiple previous characters, and we're going to be feeding them into increasingly more
 complex neural nets. But fundamentally, the output of the neural net will always just be logits.
 And those logits will go through the exact same transformation. We are going to take them through
 softmax, calculate the loss function, and the negative log likelihood, and do gradient based
 optimization. And so actually, as we complexify the neural nets and work all the way up to transformers,
 none of this will really fundamentally change. None of this will fundamentally change. The only
 thing that will change is the way we do the forward pass, where we've taken some previous
 characters and calculate the logits for the next character in a sequence. That will become more
 complex. And I will use the same machinery to optimize it. And it's not obvious how we would have extended
 this by grammar approach into the case where there are many more characters at the input,
 because eventually these tables will get way too large, because there's way too many combinations
 of what previous characters could be. If you only have one previous character, we can just keep
 everything in a table accounts. But if you have the last 10 characters that are in, but we can't
 actually keep everything in a table anymore. So this is fundamentally an unscalable approach.
 And the neural network approach is significantly more scalable. And it's something that actually
 we can improve on over time. So that's where we will be digging next. I wanted to point out two
 more things. Number one, I want you to notice that this X and here, this is made up of one hot vectors.
 And then those one hot vectors are multiplied by this w matrix. And we think of this as a multiple
 neurons being forwarded in a fully connected manner. But actually what's happening here is that,
 for example, if you have a one hot vector here that has a one, let's say the fifth dimension,
 then because of the way the matrix multiplication works, multiplying that one hot vector with w
 actually ends up plucking out the fifth row of w. Logits would become just a fifth row of w.
 And that's because of the way the matrix multiplication works.
 So that's actually what ends up happening. So, but that's actually exactly what happened before.
 Because remember all the way up here, we have a bygram, we took the first character,
 and then that first character indexed into a row of this array here. And that row gave us the
 probability distribution for the next character. So the first character was used as a lookup into a
 matrix here to get the probability distribution. Well, that's actually exactly what's happening
 here, because we're taking the index, we're encoding it as one hot and multiplying it by w.
 So logits literally becomes the appropriate row of w. And that gets just as before
 exponentiated to create the counts, and then normalized and becomes probability.
 So this w here is literally the same as this array here. But w, remember, is the log counts,
 not the counts. So it's more precise to say that w exponentiated w dot exp is this array.
 But this array was filled in by counting and by basically populating the counts of bygrams,
 whereas in the gradient base framework, we initialize it randomly, and then we let the loss
 guide us to arrive at the exact same array. So this array exactly here is basically the array w
 at the end of optimization, except we arrived at it piece by piece by following the loss.
 And that's why we also obtain the same loss function at the end. And the second notice,
 if I come here, remember the smoothing where we added fake counts to our counts in order to
 smooth out and make more uniform the distributions of these probabilities.
 And that prevented us from assigning zero probability to any one bygram. Now, if I increase the count
 here, what's happening to the probability? As I increase the count, probability becomes more and
 more uniform, right? Because these counts go only up to like 900 or whatever. So if I'm adding
 plus a million to every single number here, you can see how the row and its probability
 then when we divide, it's just going to become more and more close to exactly even probability
 uniform distribution. It turns out that the gradient base framework has an equivalent to
 smoothing. In particular, think through these w's here, which we initialize randomly. We could
 also think about initializing w's to be zero. If all the entries of w are zero, then you'll see
 that logits will become all zero. And then exponentiating those logits becomes all one.
 And then the probabilities turn out to be exactly uniform. So basically, when w's are all equal to
 each other, or say, especially zero, then the probabilities come out completely uniform. So
 trying to incentivize w to be near zero is basically equivalent to label smoothing.
 And the more you incentivize that in a loss function, the more smooth distribution you're
 going to achieve. So this brings us to something that's called regularization, where we can actually
 augment the loss function to have a small component that we call a regularization loss.
 In particular, what we're going to do is we can take w and we can, for example, square all of its
 entries. And then we can, whoops, sorry about that, we can take all the entries of w and we can sum them.
 And because we're squaring, there will be no signs anymore.
 Natives and positives all get squashed to be positive numbers. And then the way this works is,
 you achieve zero loss if w is exactly or zero. But if w has non zero numbers, you accumulate loss.
 And so we can actually take this and we can add it on here. So we can do something like loss, plus
 w square dot sum, or let's actually, instead of sum, let's take a mean, because otherwise the
 sum gets too large. So mean is like a little bit more manageable. And then we have a regularization
 loss here, like say, 0.01 times, or something like that, you can choose the regularization
 strength. And then we can just optimize this. And now this optimization actually has two components,
 not only is it trying to make all the probabilities work out, but in addition to that, there's an
 additional component that simultaneously tries to make all w's be zero, because if w's are non zero,
 you feel a loss. And so minimizing this, the only way to achieve that is for w to be zero.
 And so you can think of this as adding like a spring force, or like a gravity force,
 that that pushes w to be zero. So w wants to be zero, and the probabilities want to be uniform.
 But they also simultaneously want to match up your probabilities as indicated by the data.
 And so the strength of this regularization is exactly controlling the amount of counts
 that you add here, adding a lot more counts here, corresponds to increasing this number,
 because the more you increase it, the more this part of the loss function dominates this part.
 And the more these these weights will be unable to grow, because as they grow, they accumulate
 way too much loss. And so if this is strong enough, then we are not able to overcome the force of
 this loss. And we will never, and basically everything will be uniform predictions.
 So I thought that's kind of cool. Okay. And lastly, before we wrap up, I wanted to show you how you
 would sample from this neural net model. And I copy pasted the sampling code from before,
 where remember that we sampled five times. And all we did, we started zero, we grabbed the current
 Ix row of P, and that was our probability row, from which we sampled the next index, and just
 accumulated that and break when zero. And running this gave us these results. I still have the P in
 memory. So this is fine. Now, this P doesn't come from the row of P, instead it comes from this
 neural net. First, we take Ix, and we encode it into a one hot row of X, Seng. This X Seng multiplies
 Rw, which really just plucks out the row of W corresponding to Ix. Really, that's what's happening. And
 that gets our logits. And then we normalize those logits, exponentiate to get counts, and then normalize
 to get the distribution. And then we can sample from the distribution. So if I run this,
 kind of anticlimatic or climatic, depending how you look at it, but we get the exact same result.
 And that's because this is the identical model. Not only does it achieve the same loss, but as I
 mentioned, these are identical models. And this W is the log counts of what we've estimated before.
 But we came to this answer in a very different way. And it's got a very different interpretation.
 But fundamentally, this is basically the same model and gives the same samples here. And so
 that's kind of cool. Okay, so we've actually covered a lot of ground. We introduced the
 Bagram character level language model. We saw how we can train the model, how we can sample from the
 model, and how we can evaluate the quality of the model using the negative log likelihood loss.
 And then we actually trained the model in two completely different ways that actually get the
 same result and the same model. In the first way, we just counted up the frequency of all the
 diagrams and normalized. In the second way, we used the negative log likelihood loss as a guide
 to optimizing the counts matrix or the counts array so that the loss is minimized in the in
 a gradient based framework. And we saw that both of them give the same result. And that's it.
 Now the second one of these, the gradient based framework is much more flexible. And right now,
 our neural network is super simple. We're taking a single previous character, and we're taking it
 through a single linear layer to calculate the logits. This is about to complexify. So in the
 follow up videos, we're going to be taking more and more of these characters. And we're going to
 be feeding them into a neural net. But this neural net will still output the exact same thing. The
 neural net will output logits. And these logits will still be normalized in the exact same way,
 and all the loss and everything else and the gradient, gradient based framework, everything
 stays identical. It's just that this neural net will now complexify all the way to transformers.
 So that's going to be pretty awesome. And I'm looking forward to it for now. Bye.
 Hi everyone. Today we are continuing our implementation of Makemore. Now in the
 last lecture we implemented the PyGram language model and we implemented both
 using counts and also using a super simple neural network that has a single
 linear layer. Now this is the Jupyter notebook that we both
 out last lecture and we saw that the way we approached this is that we looked at
 only the single previous character and we predicted the distribution for the
 character that would go mixed in the sequence and we did that by taking counts
 and normalizing them into probabilities so that each row here sums to one. Now
 this is all well and good if you only have one character of previous context and
 this works and it's approachable. The problem with this model of course is that
 the predictions from this model are not very good because you only take one
 character of context so the model didn't produce very name like sounding things.
 Now the problem with this approach though is that if we are to take more
 context into account when predicting the next character in a sequence things
 quickly blow up and this table the size of this table grows and in fact it grows
 exponentially with the length of the context because if we only take a single
 character at a time that's 27 possibilities of context but if we take two
 characters in the past and try to predict the third one suddenly the number of rows
 in this matrix you can look at it that way is 27 times 27 so there's 729
 possibilities for what could have come in the context. If we take three
 characters as the context suddenly we have 20,000 possibilities of context and
 so there's just way too many rows of this matrix it's way too few counts for
 each possibility and the whole thing just kind of explodes and doesn't work
 very well. So that's why today we're going to move on to this bullet point here
 and we're going to implement a multi-layer perceptron model to predict
 the next character in a sequence and this modeling approach that we're going to
 adopt follows this paper Ben-Jue et al 2003. So I have the paper pulled up here.
 Now this isn't the very first paper that proposed the use of multi-layer
 perceptrons or neural networks to predict the next character or token in a
 sequence but it's definitely one that was very influential around that time
 it is very often cited to stand in for this idea
 and I think it's a very nice write-up and so this is the paper that we're going
 to first look at and then implement. Now this paper has 19 pages so we don't
 have time to go into the full detail of this paper but I invite you to read it
 it's very readable interesting and has a lot of interesting ideas in it as well.
 In the introduction they describe the exact same problem I just described
 and then to address it they propose the following model.
 Now keep in mind that we are building a character level language model so we're
 working on a level of characters. In this paper they have a vocabulary of
 17,000 possible words and they instead build a word level language model
 but we're going to still stick with the characters but we'll take the same
 modeling approach. Now what they do is basically they propose to take
 every one of these words 17,000 words and they're going to associate
 to each word a say 30-dimensional feature vector.
 So every word is now embedded into a 30-dimensional space you can think of it
 that way. So we have 17,000 points or vectors
 in a 30-dimensional space and that's you might imagine that's very crowded
 that's a lot of points for a very small space.
 Now in the beginning these words are initialized completely randomly so
 they're spread out that random but then we're going to tune
 these embeddings of these words using that propagation.
 So during the course of training of this neural network these points or vectors
 are going to basically move around in this space and you might imagine that
 for example words that have very similar meanings or there are indeed
 synonyms of each other might end up in a very similar part of the space
 and conversely words that mean very different things would go somewhere
 else in the space. Now their modeling approach otherwise
 is identical to ours. They are using a multilayer neural network to predict the
 next word given the previous words and to train the neural network they are
 maximizing the log likelihood of the training data just like we did.
 So the modeling approach itself is identical. Now here they have a concrete
 example of this intuition. Why does it work? Basically suppose that for example
 you are trying to predict a dog was running in a blank.
 Now suppose that the exact phrase a dog was running in a
 has never occurred in a training data and here you are at sort of test time
 later when the model is deployed somewhere and it's trying to
 make a sentence and it's saying a dog was running in a blank
 and because it's never encountered this exact phrase in the training set
 you're out of distribution as we say like you don't have fundamentally any
 reason to suspect what might come next. But this approach actually allows you to
 get around that because maybe you didn't see the exact
 phrase a dog was running in a something but maybe you've seen similar phrases
 maybe you've seen the phrase the dog was running in a
 blank and maybe your network has learned that a and the
 are like frequently are interchangeable with each other and so maybe it took the
 embedding for a and the embedding for the and it actually put them like nearby
 each other in the space and so you can transfer knowledge
 through that embedding and you can generalize in that way.
 Similarly the network could know that cats and dogs are animals and they
 co-occur in lots of very similar contexts and so even though you haven't seen this
 exact phrase or if you haven't seen exactly walking or
 running you can through the embedding space
 transfer knowledge and you can generalize to novel
 scenarios. So let's now scroll down to the diagram of the neural network
 they have a nice diagram here and in this example we are taking
 three previous words and we are trying to predict the fourth word
 in a sequence. Now these three previous words as I mentioned
 we have a vocabulary of 17,000 possible words so every one of these
 basically are the index of the incoming word and because there are 17,000 words
 this is an integer between 0 and 16,999.
 Now there's also a lookup table that they call C.
 This lookup table is a matrix that is 17,000 by say 30
 and basically what we're doing here is we're treating this as a lookup table
 and so every index is plucking out a row of this embedding matrix
 so that each index is converted to the 30-dimensional
 vector that corresponds to the embedding vector for that word.
 So here we have the input layer of 30 neurons for three words making up 90
 neurons in total and here they're saying that this
 matrix C is shared across all the words so we're always
 indexing into the same matrix C over and over
 for each one of these words. Next up is the hidden layer of this neural
 network. The size of this hidden neural layer of this neural net is a
 hop parameter so we use the word hyperparameter when it's kind of like a
 design choice up to the designer of the neural net and this can be as large as
 you'd like or as small as you'd like so for example the size could be 100
 and we are going to go over multiple choices of the size of this hidden layer
 or going to evaluate how well they work. So say there were 100 neurons here
 all of them would be fully connected to the 90 words or 90
 numbers that make up these three words. So this is a fully connected layer
 then there's a 10-inch long linearity and then there's this output layer
 and because there are 17,000 possible words that could come next
 this layer has 17,000 neurons and all of them are fully connected
 to all of these neurons in the hidden layer. So there's a lot of parameters
 here because there's a lot of words so most computation is here this is the
 expensive layer. Now there are 17,000 logits here so on
 top of there we have the softmax layer which we've seen in our previous video
 as well. So every one of these logits is
 exponentiated and then everything is normalized to sum to one so that we
 have a nice probability distribution for the next
 word in the sequence. Now of course during training we actually have the
 label we have the identity of the next word in a sequence
 that word or its index is used to pluck out the probability of that word
 and then we are maximizing the probability of that word
 with respect to the parameters of this neural net. So the parameters are the
 weights and biases of this output layer, the weights and biases of the
 hidden layer and the embedding lookup table C and all of that is optimized
 using back propagation and these dashed arrows
 ignore those that represents a variation of a neural net that we are not going to
 explore in this video. So that's the setup and now let's
 implement it. Okay so I started a brand new notebook for this
 lecture. We are importing PyTorch and we are importing
 matplotlib so we can create figures. Then I am reading all the names
 into a list of words like I did before and I'm showing the first eight
 right here. Keep in mind that we have a 32,000 in total
 these are just the first eight and in here I'm building out the vocabulary
 of characters and all the mappings from the characters as
 strings to integers and vice versa. Now the first thing we want to do is we
 want to compile the dataset for the neural network
 and I had to rewrite this code, I'll show you in a second what it looks like.
 So this is the code that I created for the dataset creation so let me first run
 it and then I'll briefly explain how this works.
 So first we're going to define something called block size and this is basically
 the context length of how many characters do we take to predict the next one.
 So here in this example we're taking three characters to predict the fourth one
 so we have a block size of three that's the size of the block
 that supports the prediction. Then here I'm building out the
 x and y. The x are the input to the neural net
 and the y are the labels for each example inside x.
 Then I'm erring over the first five words. I'm doing first five just for
 efficiency while we are developing all the code but then later we're going to
 come here and erase this so that we use the
 entire training set. So here I'm printing the word
 Emma and here I'm basically showing the examples that we can generate,
 the five examples that we can generate out of the single
 sort of word Emma. So when we are given the context of just
 dot dot dot the first character in a sequence is
 e. In this context the label is m. When the context is this the label is m
 and so forth. And so the way I build this out is first I start with a padded
 context of just zero tokens. Then I iterate over all the characters.
 I get the character in the sequence and I basically build out the array y of
 this current character and the array x which stores the current running context.
 And then here see I print everything and here I
 crop the context and enter the new character in a sequence. So this is kind
 of like a rolling window of context. Now we can change the block size here to
 for example four and in that case we would be predicting
 the fifth character given the previous four or it can be five
 and then it would look like this or it can be say ten
 and then it would look something like this. We're taking 10 characters to
 predict the 11th one and we're always padding with dots.
 So let me bring this back to three just so that we have what we have here in the
 paper. And finally the data set right now looks
 as follows. From these five words we have created a
 data set of 32 examples and each input of the neural
 hat is three integers and we have a label that is also an integer
 y. So x looks like this. These are the individual examples
 and then y are the labels. So given this let's not write a neural
 network that takes these x's and predicts two y's. First let's build the
 embedding lookup table c. So we have 27 possible characters
 and we're going to embed them in a lower dimensional space.
 In the paper they have 17,000 words and they embed them in
 spaces as small dimensional as 30. So they cram 17,000
 words into 30 dimensional space. In our case we have only 27 possible
 characters so let's cram them in something as small as to start with for
 example a two dimensional space. So this lookup table will be random numbers
 and we'll have 27 rows and we'll have two columns.
 Right so each 20 each one of 27 characters will have a two-dimensional
 embedding. So that's our matrix c of embeddings
 in the beginning initialized randomly. Now before we embed all of the
 integers inside the input x using this lookup table c
 let me actually just try to embed a single individual integer like say
 five. So we get a sense of how this works. Now one way this works of
 course is we can just take the c and we can index into row five
 and that gives us a vector the fifth row of c
 and this is one way to do it. The other way that I presented in the previous
 lecture is actually seemingly different but actually identical.
 So in the previous lecture what we did is we took these integers and we used
 the one hot encoding to first encode them. So if that one hot
 we want to encode integer five and we want to tell it that the number of
 classes is 27. So that's the 26-dimensional vector of all zeros
 except the fifth bit is turned on. Now this actually doesn't work.
 The reason is that this input actually must be a torch dot tensor.
 And I'm making some of these errors intentionally just so you get to see
 some errors and how to fix them. So this must be a tensor not an int
 fairly straightforward to fix. We get a one-hot vector the fifth dimension is
 one and the shape of this is 27. And now notice that
 just as I briefly alluded to in a previous video if we take this one hot
 vector and we multiply it by c
 then what would you expect? Well number one
 first you'd expect an error because expected scalar type long but found
 float. So a little bit confusing but the problem here is that one hot the
 data type of it is long it's a 64-bit integer but this
 is a float tensor and so PyTorf doesn't know how to multiply
 and int with a float and that's why we had to explicitly cast this to a float
 so that we can multiply. Now the output actually here
 is identical and that it's identical because of the way the matrix
 multiplication here works. We have the one hot
 vector multiplying columns of c and because of all the zeros
 they actually end up masking out everything in c except for the fifth row
 which is blocked out. And so we actually arrive at the same result
 and that tells you that here we can interpret this first piece here
 this embedding of the integer. We can either think of it as the integer
 indexing into a lookup table c but equivalently we can also think of
 this little piece here as a first layer of this bigger neural nut.
 This layer here has neurons that have no nonlinearity there's no tanh
 they're just linear neurons and their weight matrix is c
 and then we are encoding integers into one hot and feeding those into a neural
 nut and this first layer basically embeds them.
 So those are two equivalent ways of doing the same thing. We're just going to
 index because it's much much faster and we're going to discard this
 interpretation of one hot inputs into neural nets
 and we're just going to index integers and create and use embedding tables.
 Now embedding a single integer like five is easy enough.
 We can simply ask by torch to retrieve the fifth row of c
 or the row index five of c. But how do we simultaneously embed all of
 these 32 by three integers stored in array x?
 Luckily by torch indexing is fairly flexible and quite powerful.
 So it doesn't just work to ask for a single element five like this.
 You can actually index using lists. So for example we can get the rows five
 six and seven and this will just work like this. We can index with a list.
 It doesn't just have to be a list it can also be a actually tensor of
 integers and we can index with that. So this is a
 integer tensor five six seven and this will just work as well.
 In fact we can also for example repeat row seven and retrieve it multiple times
 and that same index will just get embedded multiple times here.
 So here we are indexing with a one-dimensional tensor of integers
 but it turns out that you can also index with multi-dimensional tensors of
 integers. Here we have a two-dimensional tensor of
 integers. So we can simply just do c at x
 and this just works. And the shape of this is 32 by 3
 which is the original shape. And now for every one of those 32 by 3
 integers we've retrieved the embedding vector
 here. So basically we have that as an example.
 The 13th or example index 13 the second dimension is the integer one as an
 example. And so here if we do c of x which gives us that
 array and then we index into 13 by 2 of that
 array then we get the embedding here. And you can verify that
 c at one which is the integer at that location
 is indeed equal to this. You see they're equal. So basically long story short
 pytorch indexing is awesome and to embed
 simultaneously all of the integers in x we can simply do c of x
 and that is our embedding and that just works. Now let's construct this layer
 here the hidden layer. So we have that w1 as I'll call it
 are these weights which we will initialize randomly.
 Now the number of inputs to this layer is going to be
 three times two right because we have two dimensional embeddings and we have
 three of them. So the number of inputs is six and the number of neurons in this
 layer is a variable up to us. Let's use 100 neurons as an example.
 And then biases will be also initialized randomly as an example
 and let's and we just need 100 of them. Now the problem with this is we can't
 simply normally we would take the input in this case that's
 embedding and we'd like to multiply it with these weights
 and then we would like to add the bias. This is roughly what we want to do.
 But the problem here is that these embeddings are stacked up in the dimensions
 of this input tensor. So this will not work this matrix
 multiplication because this is a shape 32 by 3 by 2
 and I can't multiply that by 6 by 100. So somehow we need to concatenate
 these inputs here together so that we can do something along these lines which
 currently does not work. So how do we transform this 32 by 3 by 2
 into a 32 by 6 so that we can actually perform
 this multiplication over here. I'd like to show you that there are usually
 many ways of implementing what you'd like to do in Torch
 and some of them will be faster, better, shorter, etc.
 And that's because Torch is a very large library and it's got lots and lots of
 functions. So if we just go to the documentation
 and click on Torch, you'll see that my slider here is very tiny
 and that's because there are so many functions that you can call on these
 tensors to transform them, create them, multiply them, add them,
 perform all kinds of different operations on them.
 And so this is kind of like the space of possibility if you will.
 Now one of the things that you can do is we can control here
 control f for concatenate and we see that there's a function
 Torq.cat, sure for concatenate. And this concatenate is given a sequence of
 tensors in a given dimension and these tensors must have the same shape,
 etc. So we can use the concatenate operation to
 in a naive way concatenate these three embeddings
 for each input. So in this case we have
 amp of the shape and really what we want to do is we want to
 retrieve these three parts and concatenate them.
 So we want to grab all the examples, we want to grab
 first the 0th index and then all of this. So this
 plucks out the 32 by 2 embeddings of just the first
 word here. And so basically we want this guy,
 we want the first dimension and we want the second dimension
 and these are the three pieces individually.
 And then we want to treat this as a sequence and we want to torch.cat
 on that sequence. So this is the list.
 torch.cat takes a sequence of tensors and then we have to tell it along which
 dimension to concatenate. So in this case all these are 32 by 2
 and we want to concatenate not across dimension 0 but across dimension 1.
 So passing in 1 gives us the result that the shape of this is 32 by 6
 exactly as we'd like. So that basically took 32 and squashed
 these by concatenating them into 32 by 6. Now this is kind of ugly because
 this code would not generalize if we want to later change the block size.
 Right now we have three inputs, three words, but what if we had five
 then here we would have to change the code because I'm indexing directly.
 Well torch comes to rescue again because there turns out to be
 a function called unbind and it removes a tensor dimension.
 So it removes a tensor dimension, returns a tuple of all slices along a given
 dimension without it. So this is exactly what we need.
 And basically when we call torch.unbind
 torch.unbind of m and passing dimension
 1 index 1 this gives us a list of tensors exactly equivalent to this.
 So running this gives us a length 3 and it's exactly this list.
 And so we can call torch.cat on it and along the first dimension
 and this works and this shape is the same. But now this is
 it doesn't matter if we have block size 3 or 5 or 10 this will just work.
 So this is one way to do it but it turns out that in this case
 there's actually a significantly better and more efficient way.
 And this gives me an opportunity to hint at some of the
 internals of torch.tensor. So let's create an array here
 of elements from 0 to 17 and the shape of this
 is just 18. It's a single vector of 18 numbers.
 It turns out that we can very quickly re-represent this
 as different sized and dimensional tensors. We do this by calling
 a view and we can say that actually this is not a single vector of 18.
 This is a 2 by 9 tensor or alternatively this is a 9 by 2 tensor.
 Or this is actually a 3 by 3 by 2 tensor. As long as the total number of
 elements here multiply to be the same this will just work.
 And in pytorch this operation calling.view is extremely efficient.
 And the reason for that is that in each tensor there's something called the
 underlying storage. And the storage is just the numbers
 always as a one-dimensional vector. And this is how this tensor has represented
 in the computer memory. It's always a one-dimensional
 vector. But when we call that view we are manipulating some of
 attributes of that tensor that dictate how this one-dimensional sequence
 is interpreted to be an n-dimensional tensor.
 And so what's happening here is that no memory is being changed, copied, moved,
 or created when we call that view. The storage
 is identical. But when you call that view some of the internal
 attributes of the view of this tensor are being manipulated and changed.
 In particular that's something there's something called a storage offset,
 strides, and shapes. And those are manipulated so that this one-dimensional
 sequence of bytes is seen as different and dimensional
 arrays. There's a blog post here from Eric called
 Pytorch internals where he goes into some of this with respect to tensor and how
 the view of a tensor is represented. And this is really just like a logical
 construct of representing the physical memory.
 And so this is a pretty good blog post that you can go into.
 I might also create an entire video on the internals of Torch Tensor and how this
 works. For here we just note that this is an extremely
 efficient operation. And if I delete this and come back to our
 mb, we see that the shape of our mb is 32 by 3 by 2.
 But we can simply ask for Pytorch to view this instead as a 32 by 6.
 And the way this gets flattened into a 32 by 6 array
 just happens that these two get stacked up in a single row.
 And so that's basically the concatenation operation that we're after.
 And you can verify that this actually gives the exact same result as what we
 had before. So this is an element y equals and you can see that all the
 elements of these two tensors are the same. And so we get the exact same
 result. So long story short, we can actually just
 come here. And if we just view this as a 32 by 6
 instead, then this multiplication will work and give us the hidden states that
 we're after. So if this is h, then h slash
 shape is now the 100 dimensional activations
 for every one of our 32 examples. And this gives the desired result.
 Let me do two things here. Number one, let's not use 32. We can, for example, do
 something like m dot shape at zero so that we don't hardcode these numbers.
 And this would work for any size of this m. Or alternatively, we can also do
 negative one. When we do negative one, pytorch will
 infer what this should be. Because the number of elements must be the same.
 And we're saying that this is six, pytorch will derive that this must be
 32 or whatever else it is if m is of different size.
 The other thing is here, one more thing I'd like to point out is
 here when we do the concatenation, this actually is much less efficient
 because this concatenation would create a whole new tensor with a whole new
 storage. So new memory is being created because there's no way to concatenate
 tensors just by manipulating the view attributes.
 So this is inefficient and creates all kinds of new memory.
 So let me delete this now. We don't need this. And here to calculate h, we want
 to also dot 10h of this to get our, oops, to get our h.
 So these are now numbers between negative one and one because of the 10h
 and we have that the shape is 32 by 100. And that is basically this hidden layer
 of activations here for every one of our 32 examples.
 Now there's one more thing I glossed over that we have to be very careful with
 and that this, and that's this plus here. In particular, we want to make sure that
 the broadcasting will do what we like. The shape of this is 32 by 100
 and the one's shape is 100. So we see that the addition here will
 broadcast these two and in particular we have 32 by 100
 broadcasting to 100. So broadcasting will align on the right,
 create a fake dimension here. So this will become a one by 100 row vector
 and then it will copy vertically for every one of these rows of 32
 and do an element-wise addition. So in this case the correcting will be
 happening because the same bias vector will be added to all the rows
 of this matrix. So that is correct. That's what we'd like
 and it's always good practice. Just make sure
 so that you don't treat yourself in the foot. And finally, let's create the final
 layer here. So let's create W2 and B2.
 The input now is 100 and the output number of neurons
 will be for us 27 because we have 27 possible
 characters that come next. So the biases will be 27 as well.
 So therefore the logits which are the outputs of this neural net
 are going to be h multiplied by W2 plus B2.
 Logits that shape is 32 by 27 and the logits look good. Now exactly as we saw
 in the previous video, we want to take these logits and we want to
 first exponentiate them to get our fake counts and then we want to normalize
 them into a probability. So prob is counts divide
 and now counts that sum along the first dimension
 and keep them as true exactly as in the previous video.
 And so prob that shape now is 32 by 27
 and you'll see that every row of prob sums to one so it's normalized.
 So that gives us the probabilities. Now of course we have the actual error that
 comes next and that comes from this array
 y which we created during the data separation.
 So y is this last piece here which is the identity of the next character in a
 sequence that we'd like to now predict. So what we'd like to do now is just as
 in the previous video we'd like to index into the rows of prob
 and in each row we'd like to pluck out the probability assigned to the correct
 character as given here. So first we have
 Torched dot a range of 32 which is kind of like an iterator over
 numbers from 0 to 31 and then we can index into prob in the following way.
 Prob in Torched dot a range of 32 which iterates the rows
 and then in each row we'd like to grab this call as given by y.
 So this gives the current probabilities as assigned by this neural network with
 this setting of its weights to the correct character
 in the sequence. And you can see here that this looks okay for some of these
 characters like this is basically 0.2 but it doesn't look very good at all for
 many other characters like this is 0.0 7 0's 1
 probability and so the network thinks that some of these are extremely
 unlikely but of course we haven't trained the neural network
 yet so this will improve and ideally all of these
 numbers here of course are 1 because then we are correctly predicting the
 next character. Now just as in the previous video
 we want to take these probabilities we want to look at the log probability
 and then we want to look at the average log probability
 and then negative of it to create the negative log likelihood loss.
 So the loss here is 17 and this is the loss that we'd like to minimize
 to get the network to predict the correct character in the sequence.
 Okay so I rewrote everything here and made it a bit more respectable
 so here's our dataset here's all the parameters that we defined.
 I'm now using a generator to make it reproducible.
 I clustered all the parameters into a single list of parameters
 so that for example it's easy to count them and see that in total we currently
 have about 3,400 parameters and this is the forward pass as we
 developed it and we arrive at a single number here
 the loss that is currently expressing how well
 this neural network works with the current setting of parameters.
 Now I would like to make it even more respectable so in particular CVs lies
 here where we take the logits and we calculate a loss.
 We're not actually reinventing the wheel here this is just
 classification and many people use classification and that's why there is
 a functional.cross entropy function in PyTorch to calculate this much more
 efficiently. So we could just simply call f.cross entropy
 and we can pass in the logits and we can pass in the array of targets y
 and this calculates the exact same loss. So in fact we can simply
 put this here and erase these three lines and we're going to get the
 exact same result. Now there are actually many good reasons to prefer f.cross entropy
 over rolling your own implementation like this. I did this for educational
 reasons but you'd never use this in practice. Why is that?
 Number one when you use f.cross entropy PyTorch will not actually create
 all these intermediate tensors because these are all new tensors in memory
 and all this is fairly inefficient to run like this.
 Instead PyTorch will cluster up all these operations and very often
 create have a fused kernels that very efficiently evaluate these expressions
 that are sort of like clustered mathematical operations.
 Number two the backward pass can be made much more efficient
 and not just because it's a fused kernel but also analytically and
 mathematically it's much it's often a very much simpler
 backward pass to implement. We actually saw this with micrograd.
 You see here when we implemented 10H the forward pass of this operation to
 calculate the 10H was actually a fairly complicated mathematical
 expression but because it's a clustered mathematical
 expression when we did the backward pass we didn't individually backward
 through the x and the two times and the minus one and division etc.
 We just said it's 1 minus t squared and that's a much simpler mathematical
 expression and we were able to do this because we're able to reuse
 calculations and because we are able to mathematically and analytically derive
 the derivative and often that expression simplifies
 mathematically and so there's much less to implement.
 So not only can it be made more efficient because it runs in a fused kernel
 but also because the expressions can take a much simpler form mathematically.
 So that's number one. Number two under the hood f dot cross entropy can also be
 significantly more numerically well behaved. Let me show you an example of
 how this works.
 Suppose we have a logit of negative two three negative three zero and five
 and then we are taking the exponent of it and normalizing it to sum to one.
 So when logits take on this values everything is well and good and we get
 a nice probability distribution. Now consider what happens when some of
 these logits take on more extreme values and that can happen during
 optimization of a neural network. Suppose that some of these numbers grow
 very negative like say negative 100 then actually everything will come out
 fine. We still get a probability that you know
 are well behaved and they sum to one and everything is great
 but because of the way the exports if you have very positive logits let's say
 positive 100 in here you actually start to run into trouble
 and we get not a number here. And the reason for that is that these counts
 have an mth here. So if you pass in a very negative number to exp
 you just get a very negative sorry not negative but very small number very
 very near zero and that's fine. But if you pass in a very positive number
 suddenly we run out of range in our floating point number
 that represents these counts. So basically we're taking e and we're
 raising it to the power of 100 and that gives us mth
 because we run out of dynamic range on this floating point number that is
 count. And so we cannot pass very large
 logits through this expression. Now let me reset these numbers to
 something reasonable. The way PyTorch solved this
 is that you see how we have a really well behaved result here.
 It turns out that because of the normalization here you can actually offset
 logits by any arbitrary constant value that you want.
 So if I add one here you actually get the exact same result
 or if I add two or if I subtract three. Any offset will produce the exact same
 probabilities. So because negative numbers are okay
 but positive numbers can actually overflow this exp. What PyTorch does is it
 internally calculates the maximum value that occurs in the logits
 and it subtracts it. So in this case it would subtract five.
 And so therefore the greatest number in logits will become zero
 and all the other numbers will become some negative numbers.
 And then the result of this is always well behaved.
 So even if we have 100 here previously not good
 but because PyTorch will subtract 100 this will work.
 And so there's many good reasons to call cross entropy.
 Number one the forward pass can be much more efficient. The backward pass can be
 much more efficient and also things can be much more numerically well
 behaved. Okay so let's now set up the training of this neural mat.
 We have the forward pass.
 We don't need these because that we have the loss is equal to the
 fact that cross entropy does the forward pass.
 Then we need the backward pass. First we want to set the gradients to be
 zero. So for p-in parameters we want to make sure that p.grad
 is none which is the same as setting it to zero in PyTorch.
 And then loss the backward to populate those gradients.
 Once we have the gradients we can do the parameter update.
 So for p-in parameters we want to take all the
 data and we want to nudge it learning rate times p.grad.
 And then we want to repeat this a few times.
 And let's print the loss here as well.
 Now this won't suffice and it will create an error because we also have to go for
 p-in parameters and we have to make sure that p.grad
 is set to true in PyTorch. And this should just work.
 Okay so we started off with loss of 17 and we're
 decreasing it. Lots run longer. And you see how the loss decreases
 a lot here. So
 if we just run for a thousand times we get a very very low loss.
 And that means that we're making very good predictions.
 Now the reason that this is so straightforward right now
 is because we're only overfitting 32 examples.
 So we only have 32 examples of the first five words.
 And therefore it's very easy to make this neural mat fit.
 Only these 32 examples because we have 3,400 parameters
 and only 32 examples. So we're doing what's called overfitting
 a single batch of the data and getting a very low loss and good predictions.
 But that's just because we have so many parameters for so few examples.
 So it's easy to make this be very low.
 Now we're not able to achieve exactly zero.
 And the reason for that is we can for example look at logits which are being predicted.
 And we can look at the max along the first dimension.
 And in PyTorch max reports both the actual values that take on the maximum number,
 but also the indices of these.
 And you'll see that the indices are very close to the labels.
 But in some cases they differ.
 For example in this very first example the predicted index is 19 but the label is 5.
 And we're not able to make loss be zero and fundamentally that's because here
 the very first or the zero index is the example where dot dot dot is supposed to predict e.
 But you see how dot dot dot is also supposed to predict an o.
 And dot dot is also supposed to predict an i and then s as well.
 And so basically e o a or s are all possible outcomes in a training set for the exact same input.
 So we're not able to completely overfit and make the loss be exactly zero.
 But we're getting very close in the cases where there's a unique input for a unique output.
 In those cases we do what's called overfit and we basically get the exact same and the exact
 correct result. So now all we have to do is we just need to make sure that we read in the full
 data set and optimize the neural lot. Okay so let's swing back up where we created the data set.
 And we see that here we only use the first five words.
 So let me now erase this and let me erase the print statements otherwise we'd be printing
 way too much. And so when we process the full data set of all the words we now had 228,000
 examples instead of just 32. So let's now scroll back down to this is much larger.
 Re-initialize the weights the same number of parameters they all require gradients.
 And then let's push this print a lost that item to be here.
 And let's just see how the optimization goes if we run this.
 Okay so we started with a fairly high loss and then as we're optimizing the loss is coming down.
 But you'll notice that it takes quite a bit of time for every single iteration.
 So let's actually address that because we're doing way too much work forwarding and back
 running 220,000 examples. In practice what people usually do is they perform forward and
 backward pass an update on many batches of the data. So what we will want to do is we want to
 randomly select some portion of the data set and that's a mini batch and then only forward,
 backward and update on that little mini batch and then we integrate on those many batches.
 So in PyTorch we can for example use Torstop_brandint. We can generate numbers between 0 and 5 and make 32 of them.
 I believe the size has to be a tuple in PyTorch. So we can have a tuple 32 of numbers between 0 and 5.
 But actually we want x.shape of 0 here. And so this creates integers that index into our data set
 and there's 32 of them. So if our mini batch size is 32 then we can come here and we can first do
 mini batch construct. So integers that we want to optimize in this
 single iteration are in IX and then we want to index into x with IX to only grab those rows.
 So we're only getting 32 rows of x and therefore embeddings will again be 32 by 3 by 2,
 not 200,000 by 3 by 2. And then this IX has to be used not just to index into x but also to index into
 y. And now this should be mini batches and this should be much much faster. So it's instant almost.
 So this way we can run many many examples nearly instantly and decrease the loss much much faster.
 Now because we're only dealing with mini batches the quality of our gradient is lower. So the direction
 is not as reliable. It's not the actual gradient direction. But the gradient direction is good enough
 even when it's estimating on only 32 examples that it is useful. And so it's much better to have an
 approximate gradient and just make more steps than it is to evaluate the exact gradient and take
 fewer steps. So that's why in practice this works quite well. So let's now continue the optimization.
 Let me take out this lost item from here and place it over here at the end.
 Okay, so we're hovering around 2.5 or so. However, this is only the loss for that mini batch. So let's
 actually evaluate the loss here for all of X and for all of Y, just so we have a full sense of exactly
 how all the model is doing right now. So right now we're at about 2.7 on the entire training set.
 So let's run the optimization for a while. Okay, we're at 2.6, 2.57, 2.53.
 Okay, so one issue of course is we don't know if we're stepping too slow or too fast.
 So this point one I just guessed it. So one question is how do you determine this learning rate?
 And how do we gain confidence that we're stepping in the right sort of speed? So I'll show you one
 way to determine a reasonable learning rate. It works as follows. Let's reset our parameters
 to the initial settings. And now let's print in every step. But let's only do 10 steps or so.
 Or maybe maybe 100 steps. We want to find like a very reasonable search range, if you will. So for
 example, this is like very low, then we see that the loss is barely decreasing. So that's not,
 that's like too low, basically. So let's try this one. Okay, so we're decreasing the loss,
 but like not very quickly. So that's a pretty good low range. Now let's reset it again.
 And now let's try to find the place at which the loss kind of explodes. So maybe at negative one.
 Okay, we see that we're minimizing the loss, but you see how it's kind of unstable. It goes up
 and down quite a bit. So negative one is probably like a fast learning rate. Let's try negative 10.
 Okay, so this isn't optimizing. This is not working very well. So negative 10 is way too big.
 Negative one was already kind of big. So therefore, negative one was like somewhat reasonable if I
 reset. So I'm thinking that the right learning rate is somewhere between negative 0.001 and
 negative one. So the way we can do this here is we can use Torx Shotland space.
 And we want to basically do something like this between 0 and 1, but a number of steps is one
 more parameter that's required. Let's do a thousand steps. This creates 1000 numbers between 0.001
 and 1. But it doesn't really make sense to step between these linearly. So instead, let me create
 learning rate exponent. And instead of 0.001, this will be a negative three, and this will be a zero.
 And then the actual lars that we want to search over are going to be 10 to the power of LRE.
 So now what we're doing is we're stepping linearly between the exponents of these learning rates.
 This is 0.001. And this is one because 10 to the power of zero is one. And therefore, we are spaced
 exponentially in this interval. So these are the candidate learning rates that we want to serve
 like search over roughly. So now what we're going to do is here, we are going to run the
 optimization for 1000 steps. And instead of using a fixed number, we are going to use learning rate
 indexing into here, lars of i and make this i. So basically, let me be set this to be again,
 starting from random, creating these learning rates between 0.001 and 1, but exponentially
 stepped. And here, what we're doing is we're iterating 1000 times, we're going to use the learning rate
 that's in the beginning very, very low, in the beginning is going to be 0.001. But by the end,
 it's going to be 1. And then we're going to step with that learning rate. And now what we want to do
 is we want to keep track of the learning rates that we used. And we want to look at the losses
 that resulted. And so here, let me track stats. So lri.append lr. And loss i.append loss that item.
 Okay, so again, reset everything and then run. And so basically, we started with a very low learning
 rate, and we went all the way up to a learning rate of negative one. And now what we can do is we
 can pay all T that plot. And we can plot the two. So we can plot the learning rates on the x axis,
 and the losses we saw on the y axis. And often, you're going to find that your plot looks something
 like this, where in the beginning, you have very low learning rates. We basically anything,
 barely anything happened. Then we got to like a nice spot here. And then as we increased the
 learning rate enough, we basically started to be kind of unstable here. So a good learning rate
 turns out to be somewhere around here. And because we have lri here, we actually may want to
 do not lr, not the learning rate, but the exponent. So that would be the lre at i is maybe what we
 want to log. So let me reset this and redo that calculation. But now on the x axis, we have the
 exponent of the learning rate. And so we can see the exponent of the learning rate that is good to
 use. It would be sort of like roughly in the valley here, because here the learning rates are just
 way too low. And then here, we expect relatively good learning rates somewhere here. And then here
 things are starting to explode. So somewhere around negative one, as the exponent of the learning rate
 is a pretty good setting. And 10 to the negative one is 0.1. So 0.1 is actually, 0.1 was actually a
 fairly good learning rate around here. And that's what we had in the initial setting. But that's
 roughly how you would determine it. And so here now we can take out the tracking of these.
 And we can just simply set a lre to be 10 to the negative one, or basically otherwise 0.1,
 as it was before. And now we have some confidence that this is actually a fairly good learning rate.
 And so now what we can do is we can crank up the iterations. We can reset our optimization.
 And we can run for a pretty long time using this learning rate. Oops. And we don't want to print.
 It's way too much printing. So let me again reset and run 10,000 steps.
 Okay, so we're going to 2.48 roughly. Let's run another 10,000 steps.
 2.46. And now let's do one learning rate decay. What this means is we're going to take our learning
 rate and we're going to 10x lower it. And so we're at the late stages of training potentially.
 And we may want to go a bit slower. Let's do one more actually, a point one, just to see if
 we're making an indent here. Okay, we're still making dent. And by the way, the
 bagram loss that we achieved last video was 2.45. So we've already surpassed the bagram
 level. And once I get a sense that this is actually kind of starting to plateau off,
 people like to do as I mentioned, this learning rate decay. So let's try to decay the loss,
 the learning rate, I mean. And we achieve it about 2.3 now. Obviously, this is janky and not
 exactly how you would train it in production. But this is roughly what you're going through.
 You first find a decent learning rate using the approach that I showed you. Then you start with
 that learning rate and you train for a while. And then at the end, people like to do a learning
 rate decay, where you decay the learning rate by say a factor of 10, and you do a few more steps.
 And then you get a trained network roughly speaking. So we've achieved 2.3 and dramatically
 improved on the bagram language model using this simple neural net as described here,
 using these 3400 parameters. Now there's something we have to be careful with.
 I said that we have a better model because we are achieving a lower loss 2.3 much lower than 2.45
 with the bagram model previously. Now that's not exactly true. And the reason that's not true is that
 this is actually fairly small model. But these models can get larger and larger if you keep adding
 neurons and parameters. So you can imagine that we don't potentially have 1000 parameters, we could
 have 10,000 or 100,000 or millions of parameters. And as the capacity of the neural network grows,
 it becomes more and more capable of overfitting your training set. What that means is that the
 loss on the training set on the data that you're training on will become very, very low as low as
 zero. But all that the model is doing is memorizing your training set verbatim. So if you take that
 model and it looks like it's working really well, but you try to sample from it, you will
 basically only get examples exactly as they are in the training set. You won't get any new data.
 In addition to that, if you try to evaluate the loss on some withheld names or other words,
 you will actually see that the loss on those can be very high. So basically it's not a good model.
 So the standard in the field is to split up your data set into three splits as we call them.
 We have the training split, the dev split or the validation split and the test split.
 So training split, test or sorry, dev or validation split and test split. And typically,
 this would be say 80% of your data set, this could be 10% and this 10% roughly.
 So you have these three splits of the data. Now these 80% of your trainings of the data set,
 the training set is used to optimize the parameters of the model, just like we're doing here, using
 gradient descent. These 10% of the examples, the dev or validation split, they're used for development
 over all the hyper parameters of your model. So hyper parameters are, for example, the size of
 this hidden layer, the size of the embedding. So this is 100 or a two for us, but we could try
 different things of the strength of the realization, which we aren't using yet so far. So there's lots
 of different hyper parameters and settings that go into defining your neural lot. And you can try
 many different variations of them and see whichever one works best on your validation split. So this
 is used to train the parameters. This is used to train the hyper parameters. And test split is used
 to evaluate basically the performance of the model at the end. So we're only evaluating the loss on
 the test split very, very sparingly and very few times, because every single time you evaluate
 your test loss, and you learn something from it, you are basically starting to also train on the test
 split. So you are only allowed to test the loss on the test set very, very few times, otherwise you
 risk overfitting to it as well as you experiment on your model. So let's also split up our training
 data into train, dev and test. And then we are going to train on train and only evaluate on test
 very, very sparingly. Okay, so here we go. Here is where we took all the words and put them into x
 and y tensors. So instead, let me create a new cell here. And let me just copy paste some code here,
 because I don't think it's that complex, but we're going to try to save a little bit of time.
 I'm converting this to be a function now. And this function takes some list of words and builds the
 erase x and y for those words only. And then here, I am shuffling up all the words. So these are the
 input words that we get. We are randomly shuffling them all up. And then we're going to set n1 to be
 the number of examples, there's 80% of the words and n2 to be 90% of the way of the words. So basically,
 if ln of words is 30,000, n1 is, well, sorry, I should probably run this. n1 is 25,000 and n2 is
 28,000. And so here we see that I'm calling build data set to build a training set x and y by indexing
 into up to n1. So we're going to have only 25,000 training words. And then we're going to have
 roughly n2 minus n1, 3000 validation examples, or dev examples. And we're going to have
 ln of words basically minus n2 or 3,200 and four examples here for the test set. So now we have
 x's and y's for all those three splits. Oh, yeah, I'm printing their size here inside the function as well.
 But here we don't have words, but these are already the individual examples made from those words.
 So let's now scroll down here. And the data set now for training is more like this.
 And then when we reset the network, when we're training, we're only going to be training using x
 train x train and y train. So that's the only thing we're training on.
 Let's see where we are on a single batch. Let's not train maybe a few more steps.
 Training neural networks can take a while. Usually you don't do it in line. You launch a bunch of
 jobs and you wait for them to finish, can take it multiple days and so on. Luckily, this is a very
 small network. Okay, so the loss is pretty good. Oh, we accidentally used our learning rate. That
 is way too low. So let me actually come back. We use the decay learning rate of 0.01.
 So this will train much faster. And then here when we evaluate, let's use the depth set here.
 x depth and y depth to evaluate the loss. Okay. And let's not decay the learning rate and only do
 say 10,000 examples. Unless evaluate the dev loss once here. Okay, so we're getting about 2.3 on
 dev. And so the neural network running was training did not see these dev examples. It has an optimized
 on them. And yet when we evaluate the loss on these dev, we actually get a pretty decent loss.
 And so we can also look at what the loss is on all of training set.
 And so we see that the training and the dev loss are about equal. So we're not overfitting.
 This model is not powerful enough to just be purely memorizing the data. And so far,
 we are what's called underfitting, because the training loss and the dev or test losses are roughly
 equal. So what that typically means is that our network is very tiny, very small. And we expect to
 make performance improvements by scaling up the size of this neural net. So let's do that now.
 So let's come over here and let's increase the size of the neural net. The easiest way to do this
 is we can come here to the hidden layer, which currently has 100 neurons. And let's just bump
 this up. So let's do 300 neurons. And then this is also 300 biases. And here we have 300 inputs
 into the final layer. So let's initialize our neural net. We now have 10,000 parameters instead
 of 3000 parameters. And then we're not using this. And then here what I'd like to do is I'd like to
 actually keep track of step. Okay, let's just do this. Let's keep stats again. And here when we're
 keeping track of the loss, let's just also keep track of the steps. And let's just have i here.
 And let's train on 30,000 or rather say, let's try 30,000. And we are at point one.
 And we should be able to run this and optimize neural net. And then here basically, I want to
 PLT dot plot the steps and paste the loss. So these are the axis and the y's. And this is the
 loss function and how it's being optimized. Now you see that there's quite a bit of thickness to
 this. And that's because we are optimizing over these mini batches. And the mini batches create
 a little bit of noise in this. Where are we in the deficit? We are at 2.5. So we're still
 having to optimize this neural net very well. And that's probably because we make it bigger,
 it might take longer for this neural net to converge. And so let's continue training.
 Yeah, let's just continue training. One possibility is that the batch size is so low
 that we just have way too much noise in the training. And we may want to increase the batch size so
 that we have a bit more correct gradient. And we're not thrashing too much. And we can actually
 like optimize more properly. Okay, this will now become meaningless because we've re-inertilized
 these. So yeah, this looks not pleasing right now. But the problem is like a tiny improvement,
 but it's so hard to tell. Let's go again, 2.52. Let's try to decrease the learning rate by factor of 2.
 Okay, we're at 4.32. Let's continue training.
 We basically expect to see a lower loss than what we had before, because now we have a much
 much bigger model. And we were underfitting. So we'd expect that increasing the size of the model
 should help the neural net 2.32. Okay, so that's not happening too well. Now one other concern is
 that even though we've made the 10H layer here, or the hidden layer much, much bigger, it could be
 that the bottleneck of the network right now are these embeddings that are two-dimensional.
 It can be that we're just cramming way too many characters into just two dimensions,
 and the neural net is not able to really use that space effectively. And that that is sort of like
 the bottleneck to our network's performance. Okay, 2.23. So just by decreasing the learning rate,
 I was able to make quite a bit of progress. Let's run this one more time, and then evaluate the
 training and the dev loss. Now one more thing after training that I'd like to do is I'd like to
 visualize the embedding vectors for these characters before we scale up the embedding size from 2,
 because we'd like to make this bottleneck potentially go away. But once I make this
 greater than 2, we won't be able to visualize them. So here, okay, we're at 2.23 and 2.24.
 So we're not improving much more, and maybe the bottleneck now is the character embedding size,
 which is 2. So here I have a bunch of code that will create a figure, and then we're going to
 visualize the embeddings that were trained by the neural net on these characters, because right now
 the embedding size is just 2. So we can visualize all the characters with the x and the y coordinates
 as the two embedding locations for each of these characters. And so here are the x coordinates
 and the y coordinates, which are the columns of c. And then for each one, I also include the text
 of the little character. So here what we see is actually kind of interesting. The network has
 basically learned to separate out the characters and cluster them a little bit. So for example,
 you see how the vowels, A, E, I, O, U are clustered up here. So what that's telling us is that the
 neural net treats these as very similar, right? Because when they feed into the neural net,
 the embedding for all these characters is very similar. And so the neural net thinks that they're
 very similar and kind of like interchangeable, and that makes sense. Then the points that are
 like really far away are for example, Q. Q is kind of treated as an exception, and Q has a very
 special embedding vector, so to speak. Similarly, dot, which is a special character is all the way out
 here. And a lot of the other letters are sort of clustered up here. And so it's kind of interesting
 that there's a little bit of structure here, after the training. And it's not definitely not random,
 and these embeddings make sense. So we're now going to scale up the embedding size and won't
 be able to visualize it directly. But we expect that because we're underfitting, and we made this
 layer much bigger, and did not sufficiently improve the loss, we're thinking that the
 constraint to better performance right now could be these embedding vectors. So let's make them
 bigger. Okay, so let's scroll up here. And now we don't have two dimensional embeddings, we are
 going to have say 10 dimensional embeddings for each word. Then this layer will receive
 three times 10. So 30 inputs will go into the hidden layer. Let's also make the hidden layer a bit
 smaller. So instead of 300, let's just do 200 neurons in that hidden layer. So now the total number
 of elements will be slightly bigger at 11,000. And then we hear we have to be a bit careful because
 okay, the learning rate, we set to point one. Here we are hard code in six. And obviously,
 if you're working in production, you don't want to be hard coding magic numbers. But instead of
 six, this should now be 30. And let's run for 50,000 iterations. And let me split out the
 initialization here outside so that when we run this a multiple times, it's not going to wipe out
 our loss. In addition to that, here, let's instead of logging lost that item, let's actually log
 the let's do log 10, I believe that's a function of the loss. And I'll show you why in a second,
 let's optimize this. Basically, I'd like to plot the log loss instead of a loss, because when you
 plot the loss, many times it can have this hockey stick appearance. And log splashes it in. So it
 just kind of looks nicer. So the x axis is step I, and the y axis will be the loss I.
 And then here, this is 30. Ideally, we wouldn't be hard coding these.
 Because let's look at the loss. Okay, it's again very thick because the mini batch size is very
 small, but the total loss of the training set is 2.3. And the test or the deficit is 2.38 as well.
 So so far, so good. Let's try to now decrease the learning rate by a factor of 10.
 And train for another 50,000 iterations.
 We'd hope that we would be able to beat 2.32.
 But again, we're just kind of like doing this very haphazardly. So I don't actually have confidence
 that our learning rate is set very well, that our learning rate decay, which we just do at random
 is set very well. And so the optimization here is kind of suspects, to be honest. And this is not
 how you would do it typically in production. In production, you would create parameters or
 hyper parameters out of all these settings. And then you would run lots of experiments and see
 whichever ones are working well for you. Okay, so we have 2.17 now and 2.2. Okay, so you see how the
 training and the evaluation performance are starting to slightly slowly depart. So maybe we're
 getting the sense that the neural net is getting good enough or that number parameters is large
 enough that we are slowly starting to overfit. Let's maybe run one more iteration of this
 and see where we get. But yeah, basically, you would be running lots of experiments,
 and then you are slowly scrutinizing whichever ones give you the best dev performance. And then
 once you find all the hyper parameters that make your dev performance good, you take that model
 and you evaluate the test set performance a single time. And that's the number that you
 report in your paper or wherever else you want to talk about and brag about your model.
 So let's then rerun the plot and rerun the train and dev.
 And because we're getting lower loss now, it is the case that the embedding size of
 these was holding us back very likely. Okay, so 2.16, 2.19 is what we're roughly getting.
 So there's many ways to go from many ways to go from here. We can continue tuning the optimization.
 We can continue, for example, playing with the size of the neural net, or we can increase the
 number of words or characters in our case that we are taking as an input. So instead of just
 three characters, we could be taking more characters than as an input. And that could further improve
 the loss. Okay, so I changed the code slightly. So we have here 200,000 steps of the optimization.
 And in the first 100,000, we're using a learning rate of 0.1. And then in the next 100,000, we're
 using a learning rate of 0.01. This is the loss that I achieve. And these are the performance on
 the training and validation loss. And in particular, the best validation loss I've been able to obtain
 in the last 30 minutes or so is 2.17. So now I invite you to beat this number. And you have quite a
 few knobs available to you to I think surpass this number. So number one, you can of course change
 the number of neurons in the hidden layer of this model. You can change the dimensionality of the
 embedding lookup table. You can change the number of characters that are feeding in as an input,
 as the context into this model. And then of course, you can change the details of the
 optimization. How long are we running? Where's the learning rate? How does it change over time?
 How does it decay? You can change the batch size and you may be able to actually achieve a much
 better convergence speed in terms of how many seconds or minutes it takes to train the model
 and get your result in terms of really good loss. And then of course, I actually invite you to
 read this paper. It is 19 pages, but at this point, you should actually be able to read a
 good chunk of this paper and understand pretty good chunks of it. And this paper also has quite a few
 ideas for improvements that you can play with. So all of those are not available to you and you
 should be able to beat this number. I'm leaving that as an exercise to the reader. And that's it
 for now. And I'll see you next time. Before we wrap up, I also wanted to show how you would sample
 from the model. So we're going to generate 20 samples. At first, we begin with all dots. So that's
 the context. And then until we generate the 0th character again, we're going to embed the current
 context using the embedding table C. Now, usually here, the first dimension was the size of the
 training set. But here, we're only working with a single example that we're generating. So this
 is just dimension one, just for simplicity. And so this embedding then gets projected into the end
 state, you get the logits. Now we calculate the probabilities. For that, you can use f dot softmax
 of logits. And that just basically exponentiates the logits and makes them sum to one. And similar
 to cross entropy, it is careful that there's no overflows. Once we have the probabilities, we
 sample from them using torso multinomial to get our next index. And then we shift the context window
 to append the index and record it. And then we can just decode all the integers to strings and print
 them out. And so these are some example samples. And you can see that the model now works much
 better. So the words here are much more word, like or name like. So we have things like ham,
 joes, Lilla, you know, it started to sound a little bit more name like. So we're definitely
 making progress. But we can still improve on this model quite a lot. Okay, sorry, there's some bonus
 content. I wanted to mention that I want to make these notebooks more accessible. And so I don't
 want you to have to like install Jibere notebooks and torture everything else. So I will be sharing a
 link to a Google collab. And Google collab will look like a notebook in your browser. And you can
 just go to URL, and you'll be able to execute all of the code that you saw in the Google collab.
 And so this is me executing the code in this lecture. And I shortened it a little bit. But
 basically you're able to train the exact same network, and then plot and sample from the model.
 And everything is ready for you to like tinker with the numbers right there in your browser,
 no installation necessary. So I just wanted to point that out. And the link to this will be in the
 video description.
 Hi everyone. Today we are continuing our implementation of Makemore.
 Now in the last lecture we implemented the Multilayer Perceptron along the lines of
 Benjio et al 2003 for Character Level Language Modeling.
 So we followed this paper, took in a few characters in the past,
 and used an MLP to predict the next character in a sequence.
 So what we'd like to do now is we'd like to move on to more complex and larger neural
 networks like recurrent neural networks and their variations like the grew LSTM and so on.
 Now before we do that though we have to stick around the level of multilayer perception for a bit
 longer. And I'd like to do this because I would like us to have a very good intuitive
 understanding of the activations in the neural net during training and especially the gradients
 that are flowing backwards and how they behave and what they look like.
 This is going to be very important to understand the history of the development of these architectures
 because we'll see that recurrent neural networks while they are very expressive in that they are
 a universal approximator and can in principle implement all the algorithms.
 We'll see that they are not very easily optimizable with the first order gradient-based
 techniques that we have available to us and that we use all the time.
 And the key to understanding why they are not optimizable easily is to understand the
 activations and the gradients and how they behave during training.
 And we'll see that a lot of the variants since recurrent neural networks have tried to
 improve that situation. And so that's the path that we have to take and let's go start it.
 So the starting code for this lecture is largely the code from before but I've cleaned it up a
 little bit. So you'll see that we are importing all the torch and map plotlet utilities. We're
 reading into words just like before. These are eight example words. There's a total of 32,000 of them.
 Here's a vocabulary of all the lowercase letters and the special dot token.
 Here we are reading the dataset and processing it and creating three splits, the train,
 dev and the test split. Now in MLP this is the identical same MLP except you see that I removed
 a bunch of magic numbers that we had here. And instead we have the dimensionality of the embedding
 space of the characters and the number of hidden units in the hidden layer. And so I've pulled
 them outside here so that we don't have to go and change all these magic numbers all the time.
 With the same neural net with 11,000 parameters that we optimize now over 200,000 steps with batch
 size of 32. And you'll see that I refactored the code here a little bit but there are no functional
 changes. I just created a few extra variables, a few more comments and I removed all the magic
 numbers and otherwise is the exact same thing. Then when we optimize we saw that our loss looked
 something like this. We saw that the train and val loss were about 2.16 and so on. Here I refactored
 the code a little bit for the evaluation of arbitrary splits. So you pass in the string of
 which split you'd like to evaluate. And then here, depending on train, val or test, I index in and
 I get the correct split. And then this is the forward pass of the network and evaluation of the loss
 and printing it. So just making it nicer. One thing that you'll notice here is I'm using a decorator
 torch.no grad which you can also look up and read documentation of. Basically what this decorator
 does on top of a function is that whatever happens in this function is seen by torch to never require
 an gradients. So it will not do any of the bookkeeping that it does to keep track of all the gradients
 in anticipation of an eventual backward pass. It's almost as if all the tensors that get created here
 have a requires grad of false. And so it just makes everything much more efficient because you're
 telling torch that I will not call that backward on any of this computation and you don't need to
 maintain the graph under the hood. So that's what this does. And you can also use a context manager
 with torch.no grad and you can let those up. Then here we have the sampling from a model.
 Just as before. Just a poor passive in neural net. Getting the distribution sampling from it.
 Adjusting the context window and repeating until we get the special and token. And we see that we
 are starting to get much nicer looking words simple from the model. It's still not amazing and they're
 still not fully named like. But it's much better than when we had it with the bikram model.
 So that's our starting point. Now the first thing I would like to scrutinize is the initialization.
 I can tell that our network is very improperly configured at initialization. And there's multiple
 things wrong with it. But let's just start with the first one. Look here on the zero federation,
 the very first iteration. We are recording a loss of 27. And this rapidly comes down to roughly one
 or two or so. So I can tell that the initialization is all messed up because this is way too high.
 In training of neural nets, it is almost always the case that you will have a rough idea for
 what loss to expect at initialization. And that just depends on the loss function and the problem
 setup. In this case, I do not expect 27. I expect a much lower number and we can calculate it together.
 Basically, at initialization, what we'd like is that there's 27 characters that could come next
 for any one training example. At initialization, we have no reason to believe any characters to be
 much more likely than others. And so we'd expect that the probability distribution that comes out
 initially is a uniform distribution, assigning about equal probability to all the 27 characters.
 So basically, what we'd like is the probability for any character would be roughly one over 27.
 That is the probability we should record. And then the loss is the negative log probability.
 So let's wrap this in a tensor. And then we can take the log of it. And then the negative log
 probability is the loss we would expect, which is 3.29, much, much lower than 27. And so what's
 happening right now is that at initialization, the neural net is creating probability distributions
 that are all messed up. Some characters are very confident and some characters are very
 not confident. And then basically what's happening is that the network is very confidently wrong.
 And that makes that's what makes it record very high loss. So here's a smaller four-dimensional
 example of the issue. Let's say we only have four characters. And then we have logits that
 come out of the neural net, and they are very, very close to zero. Then when we take the softmax
 of all zeros, we get probabilities, there are a diffuse distribution. So sums to one, and is
 exactly uniform. And then in this case, if the label is say two, it doesn't actually matter
 if this if the label is two or three or one or zero, because it's a uniform distribution,
 we're recording the exact same loss, in this case, 1.38. So this is the loss we would expect for
 a four-dimensional example. And I can see, of course, that as we start to manipulate these
 logits, we're going to be changing the loss here. So it could be that we lock out and by chance,
 this could be a very high number, like, you know, five or something like that. Then in that case,
 we'll record a very low loss because we're signing the correct probability at initialization by chance
 to the correct label. Much more likely it is that some other dimension will have a high
 logic. And then what will happen is we start to record much higher loss. And what can come,
 what can happen is basically the logits come out like something like this, you know, and they take
 on extreme values, and we record really high loss. For example, if we have torched out random
 of four, so these are uniform, sorry, these are normally distributed numbers for them.
 And here we can also print the logits, probabilities that come out of it and loss.
 And so because these logits are near zero, for the most part, the loss that comes out is, is okay.
 But suppose this is like timestamp now, you see how because these are more extreme values,
 it's very unlikely that you're going to be guessing the correct bucket. And then you're
 confidently wrong and recording very high loss. If your logits are coming up even more extreme,
 you might get extremely seen losses like infinity even at initialization.
 So basically, this is not good. And we want the logits to be roughly zero, when the network is
 initialized. In fact, the logits can don't have to be just zero, they just have to be equal. So for
 example, if all the logits are one, then because of the normalization inside the softmax, this will
 actually come out okay. But by symmetry, we don't want it to be any arbitrary positive or negative
 number. We just want it to be all zeros and record the loss that we expect at initialization.
 So let's not concretely see where things go wrong in our example. Here we have the initialization.
 Let me reinitialize the neural net. And here, let me break after the very first iteration. So we
 only see the initial loss, which is 27. So that's way too high. And intuitively, now we can expect
 the variables involved. And we see that the logits here, if we just print some of these,
 if we just print the first row, we see that the logits take on quite extreme values.
 And that's what's creating the fake confidence and incorrect answers and makes the loss
 get very, very high. So these logits should be much, much closer to zero. So now let's think
 through how we can achieve logits coming out of this neural net to be more closer to zero.
 You see here that logits are calculated as the hidden states multiplied by w2 plus b2.
 So first of all, currently we're initializing b2 as random values of the right size. But
 because we want roughly zero, we don't actually want to be adding a bias of random numbers.
 So in fact, I'm going to add a times zero here to make sure that b2 is just basically zero
 at initialization. And second, this is H multiplied by w2. So if we want logits to be very, very small,
 then we would be multiplying w2 and making that smaller. So for example, if we scale down w2
 by 0.1, all the elements, then if I do again, just a very first iteration, you see that we are
 getting much closer to what we expect. So roughly what we want is about 3.29. This is 4.2. I can
 make this maybe even smaller, 3.32. Okay, so we're getting closer and closer. Now, you're probably
 wondering, can we just set this to zero? Then we get, of course, exactly what we're looking for
 at initialization. And the reason I don't usually do this is because I'm very nervous. And I'll
 show you in a second why you don't want to be setting w's or weights of a neural net exactly to zero.
 You usually want it to be small numbers instead of exactly zero. For this output layer in this
 specific case, I think it would be fine. But I'll show you in a second where things go wrong
 very quickly if you do that. So let's just go with 0.01. In that case, our loss is close enough,
 but has some entropy. It's not exactly zero. It's got some little entropy, and that's used for
 symmetry breaking, as we'll see in a second. The logits are now coming out much closer to zero,
 and everything is well and good. So if I just erase these, and I now take away the break statement,
 we can run the optimization with this new initialization. And let's just see what losses
 we record. Okay, so I'll let it run. And you see that we started off good. And then we came down a bit.
 The plot of the loss now doesn't have this hockey shape appearance,
 because basically what's happening in the hockey stick, the very first few iterations of the loss,
 what's happening during the optimization is the optimization is just squashing down the logits,
 and then it's rearranging the logits. So basically we took away this easy part of the loss function,
 where just the weights were just being shrunk down. And so therefore, we don't get these easy
 gains in the beginning, and we're just getting some of the hard gains of training the actual
 neural nut. And so there's no hockey stick appearance. So good things are happening in that both number
 one, loss at initialization is what we expect. And the loss doesn't look like a hockey stick.
 And this is true for any neural like you might train, and something to look at for.
 And second, the loss that came out is actually quite a bit improved. Unfortunately, I erased
 what we had here before. I believe this was 2.12. And this was with this was 2.16. So we get a slightly
 improved result. And the reason for that is because we're spending more cycles, more time
 optimizing the neural net actually, instead of just spending the first several thousand iterations,
 probably just squashing down the weights, because they are so way too high in the beginning
 and initialization. So something to look out for. And that's number one. Now let's look at the second
 problem. Let me reinitialize our neural net, and let me reintroduce to break statement.
 So we have a reasonable initial loss. So even though everything is looking good on the level of
 the loss, and we get something that we expect, there's still a deeper problem looking inside this
 neural net and its initialization. So the logits are now okay. The problem now is with the values of
 h, the activations of the hidden states. Now if we just visualize this vector, sorry, this tensor
 h, it's kind of hard to see, but the problem here, roughly speaking, is you see how many of the
 elements are one or negative one. Now recall that torch dot 10, the 10 h function is a squashing
 function. It takes arbitrary numbers and it squashes them into a range of negative one and one,
 and it does so smoothly. So let's look at the histogram of h to get a better idea of the
 distribution of the values inside this tensor. We can do this first. Well, we can see that h is
 32 examples and 200 activations in each example. We can view it as negative one to stretch it out
 into one large vector. And we can then call two list to convert this into one large python list
 of floats. And then we can pass this into plt dot hissed for histogram. And we say we want 50 bins
 and a semicolon to suppress a bunch of output we don't want. So we see this histogram and we see
 that most of the values by far take on value of negative one and one. So this 10 h is very, very
 active. And we can also look at basically why that is, we can look at the preactivations that
 feed into the 10 h. And we can see that the distribution of the preactivations are is very,
 very broad. These take numbers between negative 15 and 15. And that's why in a torture 10 h,
 everything is being squashed and capped to be in the range of negative one and one. And lots of
 numbers here take on very extreme values. Now, if you are new to neural networks, you might not
 actually see this as an issue. But if you're well versed in the dark arts of backpropagation,
 and have an intuitive sense of how these gradients flow through a neural net, you are looking at
 your distribution of 10 h activations here. And you are sweating. So let me show you why.
 We have to keep in mind that during backpropagation, just like we saw in micro grad, we are doing
 backward pass starting at the loss and flowing through the network backwards. In particular,
 we're going to back propagate through this tortot 10 h. And this layer here is made up of 200 neurons
 for each one of these examples. And it implements an element twice 10 h. So let's look at what happens
 in 10 h in the backward pass. We can actually go back to our previous micro grad code in the very
 first lecture and see how we implemented 10 h. We saw that the inputs here was x, and then we
 calculate t, which is the 10 h of x. So that's t. And t is between negative one and one. It's
 the output of the 10 h. And then in the backward pass, how do we back propagate through a 10 h?
 We take out that grad, and then we multiply it. This is the chain rule with the local gradient,
 which took the form of one minus t squared. So what happens if the outputs of your 10 h are very
 close to negative one or one? If you plug in t equals one here, you're going to get zero,
 multiplying out that grad. No matter what out that grad is, we are killing the gradient,
 and we're stopping effectively the backpropagation through this 10 h unit.
 Similarly, when t is negative one, this will again become zero, and out that grad just stops.
 And intuitively, this makes sense because this is a 10 h neuron. And what's happening is if its
 output is very close to one, then we are in the tail of this 10 h. And so changing basically the
 input is not going to impact the output of the 10 h too much, because it's so it's in a flat
 region of the 10 h. And so therefore, there's no impact on the loss. And so indeed, the
 weights and the biases along with this 10 h neuron do not impact the loss, because the output of
 this 10 h unit is in a flat region of the 10 h. And there's no influence. We can be changing
 them whenever we want, however we want, and the loss is not impacted. So that's another way to
 justify that indeed, the gradient would be basically zero, it vanishes. Indeed, when t equals zero,
 we get one times out that grad. So when the 10 h takes on exactly value of zero, then out that
 grad is just passed through. So basically what this is doing, right, is if t is equal to zero,
 then this the 10 h unit is sort of inactive, and gradient just passes through. But the more you
 are in the flat tails, the more the gradient is squashed. So in fact, you'll see that the
 the gradient flowing through 10 h can only ever decrease in the amount that it decreases is
 proportional through a square here, depending on how far you are in the flat tails of this 10 h.
 And so that's kind of what's happening here. And through this, the concern here is that if all of
 these outputs h are in the flat regions of negative one and one, then the gradients that are
 flowing through the network will just get destroyed at this layer. Now there is some redeeming quality
 here, and that we can actually get a sense of the problem here as follows. I've got some code here.
 And basically what we want to do here is we want to take a look at h, take the absolute value,
 and see how often it is in the in a flat region. So say greater than 0.99. And what you get is
 the following. And this is a Boolean tensor. So in the Boolean tensor, you get a white,
 if this is true, and a black, if this is false. And so basically what we have here is the 32
 examples and the 200 hidden neurons. And we see that a lot of this is white. And what that's
 telling us is that all these 10 h neurons were very, very active. And they're in a flat tail.
 And so in all these cases, the backward gradient would get destroyed.
 Now we would be in a lot of trouble if for any one of these 200 neurons, if it was the case that
 the entire column is white, because in that case we have what's called the dead neuron.
 And this could be a 10 h neuron where the initialization of the weights and the biases
 could be such that no single example ever activates this 10 h in the sort of active part of the 10 h.
 If all the examples land in the tail, then this neuron will never learn. It is a dead neuron.
 And so just scrutinizing this and looking for columns of completely white, we see that this is
 not the case. So I don't see a single neuron that is all of, you know, white. And so therefore it
 is the case that for every one of these 10 h neurons, we do have some examples that activate them
 in the active part of the 10 h. And so some gradients will flow through and this neuron will learn.
 And neuron will change and it will move and it will do something. But you can sometimes get
 yourself in cases where you have dead neurons. And the way this manifests is that for 10 h
 neurons, this would be when no matter what inputs you plug in from your data set, this 10 h neuron
 always fires completely one or completely negative one. And then it will just not learn
 because all the gradients will be just zero that. This is true not just for 10 h, but for a lot of
 other nonlinearities that people use in neural networks. So we certainly use 10 h a lot, but
 sigmoid will have the exact same issue because it is a squashing neuron. And so the same will be true
 for sigmoid, but, but, you know, basically the same will actually apply to sigmoid. The same will
 also apply to a relu. So relu has a completely flat region here below zero. So if you have a
 relu neuron, then it is a pass through. If it is positive, and if it's if the pre activation is
 negative, it will just shout it off. Since the region here is completely flat, then during back
 propagation, this would be exactly zeroing out the gradient. Like all of the gradient would be set
 exactly to zero, instead of just like a very, very small number, depending on how positive or
 negative t is. And so you can get, for example, a dead relu neuron, and a dead relu neuron would
 basically look like basically what it is is if a neuron with a relu nonlinearity never activates.
 So for any examples that you plug in in the data set, it never turns on. It's always in this flat
 region, then this relu neuron is a dead neuron. Its weights and bias will never learn. They will
 never get a gradient because the neuron never activated. And this can sometimes happen at
 initialization, because the weights and the biases just make it so that by chance some neurons are
 just forever dead. But it can also happen during optimization. If you have like a too high learning
 rate, for example, sometimes you have these neurons that get too much of a gradient, and they get
 knocked out of data manifold. And what happens is that from then on, no example ever activates
 this neuron. So this neuron remains dead forever. So it's kind of like a permanent brain damage in a,
 in a mind of a network. And so sometimes what can happen is if your learning rate is very high,
 for example, and you have a neural net with a relu neurons, you train the neural net and you get
 some last loss. But then actually what you do is you go through the entire training set and you
 forward your examples, and you can find neurons that never activate the are dead neurons in your
 network. And so those neurons will will never turn on. And usually what happens is that during
 training, these relu neurons are changing, moving, etc. And then because of a high gradient somewhere
 by chance, they get knocked off. And then nothing ever activates them. And from then on, they are
 just dead. So that's kind of like a permanent brain damage that can happen to some of these neurons.
 These other nonlinearities like leaky relu will not suffer from this issue as much, because you can
 see that it doesn't have flat tails, you'll almost always get gradients. And he was also fairly
 frequently used. It also might suffer from this issue because it has flat parts. So that's just
 something to be aware of and something to be concerned about. And in this case, we have way too many
 activations H that take on extreme values. And because there's no column of white, I think we
 will be okay. And indeed the network optimizes and gives us a pretty decent loss. But it's just
 not optimal. And this is not something you want, especially during initialization. And so basically
 what's happening is that this H preactivation that's flowing to 10 H, it's too extreme. It's too large.
 It's creating very, it's creating distribution that is too saturated in both sides of the 10 H.
 And it's not something you want because it means that there's less training for these neurons,
 because they update less frequently. So how do we fix this? Well, H preactivation is
 MCAT, which comes from C. So these are uniform Gaussian. But then it's multiplied by W1 plus B1.
 And H preact is too far off from zero. And that's causing the issue. So we want this
 reactivation to be closer to zero, very similar to what we had with logis. So here, we want
 actually something very, very similar. Now, it's okay to set the biases to very small number. We can
 either multiply by 001 to get like a little bit of entropy. I sometimes like to do that,
 just so that there's like a little bit of variation in diversity in the original
 initialization of these 10 H neurons. And I find in practice that that can help optimization a
 little bit. And then the weights, we can also just like squash. So let's multiply everything by
 0.1. Let's rerun the first batch. And now let's look at this. And well, first, let's look at here.
 You see now, because we multiply doubly by 0.1, we have a much better histogram. And that's
 because the preactivations are now between negative 1.5 and 1.5. And this we expect much,
 much less white. Okay, there's no white. So basically, that's because there are no neurons
 that saturated above 0.99 in either direction. So it's actually a pretty decent place to be.
 Maybe we can go up a little bit. Sorry, am I changing w1 here? So maybe we can go to 0.2.
 Okay, so maybe something like this is a nice distribution. So maybe this is what our
 initialization should be. So let me now erase these. And let me starting with initialization,
 let me run the full optimization without the break. And let's see what we get. Okay, so the
 optimization finished. And I rerun the loss. And this is the result that we get. And then just
 as a reminder, I put down all the losses that we saw previously in this lecture. So we see that
 we actually do get an improvement here. And just as a reminder, we started off with a validation
 loss of 2.17 when we started by fixing the softmax being confidently wrong, we came down to 2.13.
 And by fixing the 10 inch layer being way too saturated, we came down to 2.10. And the reason
 this is happening, of course, is because our initialization is better. And so we're spending
 more time doing productive training instead of not very productive training, because our
 gradients are set to zero. And we have to learn very simple things like the overconfidence of
 the softmax in the beginning. And we're spending cycles just like squashing down the weight matrix.
 So this is illustrating basically initialization and its impacts on performance,
 just by being aware of the internals of these neural nets and their activations and their
 gradients. Now, we're working with a very small network. This is just one layer,
 multi layer perception. So because the network is so shallow, the optimization problem is actually
 quite easy and very forgiving. So even though our initialization was terrible, the network still
 learned eventually, it just got a bit worse result. This is not the case in general, though. Once we
 actually start working with much deeper networks that have say 50 layers, things can get much
 more complicated. And these problems stack up. And so you can actually get into a place where the
 network is basically not training at all if your initialization is bad enough. And the deeper your
 network is and the more complex it is, the less forgiving it is to some of these errors. And so
 something to be definitely be aware of, and something to scrutinize, something to plot,
 and something to be careful with. And yeah. Okay, so that's great that that worked for us.
 But what we have here now is all these magic numbers like point two, like where do I come up with
 this? And how am I supposed to set these if I have a large neural network, lots and lots of layers.
 And so obviously, no one does this by hand. There's actually some relatively principled ways of
 setting these scales that I would like to introduce to you now. So let me paste some code here that
 I prepared just to motivate the discussion of this. So what I'm doing here is we have some
 random input here, X that is drawn from a Gaussian. And there's 1000 examples that are 10 dimensional.
 And then we have a weight in layer here that is also initialized using Gaussian, just like we did
 here. And we hit these neurons in the head and layer look at 10 inputs. And there are 200 neurons
 in this head layer. And then we have here, just like here, in this case, the multiplication X
 multiplied by W to get the preactivations of these neurons. And basically, the analysis here looks
 at, okay, suppose these are uniform Gaussian, and these weights are uniform Gaussian. If I do X
 times W, and we forget for now the bias and the nonlinearity, then what is the mean and the
 standard deviation of these Gaussians? So in the beginning here, the input is just a normal Gaussian
 distribution mean zero, and the standard deviation is one. And the standard deviation again is just
 the measure of a spread of the Gaussian. But then once we multiply here, and we look at the
 histogram of why we see that the mean, of course, stays the same. It's about zero,
 because this is a symmetric operation. But we see here that this standard deviation has expanded
 to three. So the input standard deviation was one, but now we've grown to three. And so what
 you're seeing in the histogram is that this Gaussian is expanding. And so we're expanding this Gaussian
 from the input. And we don't want that we want most of the neural nets to have relatively similar
 activations. So unit Gaussian roughly throughout the neural net. As the question is, how do we scale
 these W's to preserve the, to preserve this distribution, to remain a Gaussian? And so
 intuitively, if I multiply here, these elements of W by a larger number, like say, by five,
 then this Gaussian grows and grows in standard deviation. So now we're at 15. So basically these
 numbers here in the output, why take on more and more extreme values? But if we scale it down,
 like say, point two, then conversely, this Gaussian is getting smaller and smaller,
 and it's shrinking. And you can see that the standard deviation is point six. And so the
 question is, what do I multiply by here to exactly preserve the standard deviation to be one? And
 it turns out that the correct answer mathematically, when you work out through the variance of this
 multiplication here, is that you are supposed to divide by the square root of the fan in the fan
 in is the basically the number of input elements here, 10. So we are supposed to divide by 10
 square root. And this is one way to do the square root, you raise it to a power of 0.5. That's
 the same as doing a square root. So when you divide by the square root of 10, then we see that
 the output Gaussian, it has exactly standard deviation of one. Now, unsurprisingly, a number
 of papers have looked into how, but to best initialize neural networks. And in the case of
 multilay perceptions, we can have fairly deep networks that have these nonlinearities in between.
 And we want to make sure that the activations are well behaved, and they don't expand to infinity
 or shrink all the way to zero. And the question is, how do we initialize the weights so that these
 activations take on reasonable values throughout the network? Now, one paper that has studied this
 in quite a bit detail that is often referenced is this paper by Kameh et al. called Delving Deep
 Interactive Fires. Now, in this case, they actually study convolutional neural networks. And they
 study, especially the relu nonlinearity and the p relu nonlinearity, instead of a 10 h nonlinearity.
 But the analysis is very similar. And basically, what happens here is for them, the relu nonlinearity
 that they care about quite a bit here, is a squashing function, where all the negative numbers
 are simply clamped to zero. So the positive numbers are a path through, but everything
 negative is just set to zero. And because you are basically throwing away half of the distribution,
 they find in their analysis of the forward activations in the neural net, that you have to
 compensate for that with a gain. And so here, they find that basically when they initialize
 their weights, they have to do it with a zero mean Gaussian, whose standard deviation is
 square root of two over the fan in what we have here is we are initializing Gaussian
 with the square root of fan in this NL here is the fan in. So what we have is square root of one
 over the fan in, because we have the division here. Now they have to add this factor of two,
 because of the relu, which basically discards half of the distribution and clamps at a zero.
 And so that's where you get an initial factor. Now in addition to that, this paper also studies
 not just the sort of behavior of the activations in the forward pass of the neural net, but it
 also studies the back propagation. And we have to make sure that the gradients also are well-behaved.
 And so because ultimately they end up updating our parameters. And what they find here through
 a lot of the analysis that I am actually to read through, but it's not exactly approachable.
 What they find is basically, if you properly initialize the forward pass, the backward pass
 is also approximately initialized up to a constant factor that has to do with the size of the number
 of hidden neurons in an early and late layer. But basically they find empirically that this is
 not a choice that matters too much. Now this timing initialization is also implemented in PyTorch.
 So if you go to torch.nn.init documentation, you'll find climbing normal. And in my opinion,
 this is probably the most common way of initializing neural networks now. And it takes a few keyword
 arguments here. So number one, it wants to know the mode. Would you like to normalize the activations,
 or would you like to normalize the gradients to be always Gaussian with zero mean and unit
 or one standard deviation? And because they find the paper that this doesn't matter too much,
 most of the people just leave it as the default which is pen it. And then second,
 pass in the nonlinearity that you are using. Because depending on the nonlinearity, we need to
 calculate a slightly different gain. And so if your nonlinearity is just linear, so there's no
 nonlinearity, then the gain here will be one. And we have the exact same kind of formula that
 we've got up here. But if the nonlinearity is something else, we're going to get a slightly
 different gain. And so if we come up here to the top, we see that, for example, in the case of
 Ralu, this gain is a square root of two. And the reason it's a square root, because in this paper,
 you see how the two is inside of the square root. So the gain is a square root of two.
 In a case of linear or identity, we just get a gain of one. In a case of 10H, which is what
 we're using here, the advised gain is a five over three. And intuitively, why do we need a gain on
 top of the initialization? It's because 10H, just like Ralu, is a contractive transformation. So what
 that means is you're taking the output distribution from this matrix multiplication, and then you
 are squashing it in some way. Now Ralu squashes it by taking everything below zero and clamping it to
 zero. 10H also squashes it because it's a contractive operation, it will take the tails and it will
 squeeze them in. And so in order to fight the squeezing in, we need to boost the weights a
 little bit so that we renormalize everything back to standard unit standard deviation.
 So that's why there's a little bit of a gain that comes out. Now I'm skipping through this
 section a little bit quickly, and I'm doing that actually intentionally. And the reason for that is
 because about seven years ago when this paper was written, you have to actually be extremely
 careful with the activations and ingredients and their ranges and their histograms. And you have
 to be very careful with the precise setting of gains and the scrutinizing of the null linearities
 used and so on. And everything was very finicky and very fragile and very properly arranged for the
 neural network train, especially if your neural network was very deep. But there are a number of
 modern innovations that have made everything significantly more stable and more well behaved,
 and it's become less important to initialize these networks exactly right. And some of those
 modern innovations for example are residual connections, which we will cover in the future.
 The use of a number of normalization layers, like for example,
 batch normalization, layer normalization, group normalization, we're going to go into a lot of
 these as well. And number three, much better optimizers, not just a cast ingredient scent,
 the simple optimizer we're basically using here, but slightly more complex optimizers like
 RMS prop and especially Adam. And so all of these modern innovations make it less important for
 you to precisely calibrate the initialization of the neural net. All that being said in practice,
 what should we do? In practice when I initialize these neural nets, I basically just normalize my
 weights by the square root of the fan in. So basically, roughly what we did here is what I do.
 Now if we want to be exactly accurate here, we can go back in it of kind of normal, this is how
 good implemented. We want to set the standard deviation to be gained over the square root of
 fan in, right? So to set the standard deviation of our weights, we will proceed as follows.
 Basically, when we have a torch dot random, and let's say I just create a thousand numbers,
 we can look at the standard deviation of this. And of course, that's one, that's the amount of
 spread. Let's make this a bit bigger. So it's closer to one. So that's the spread of the Gaussian
 of zero mean and unit standard deviation. Now basically, when you take these and you multiply
 by say point two, that basically scales down the Gaussian and that makes its standard deviation
 point two. So basically, the number that you multiply by here ends up being the standard deviation
 of this Gaussian. So here, this is a standard deviation point two Gaussian here, when we
 sample our w one. But we want to set the standard deviation to gain over square root of fan mode,
 which is fan in. So in other words, we want to multiply by gain, which for 10 h is five over three,
 five over three is the gain. And then times, I guess our divide,
 square root of the fan in. And in this example here, the fan in was 10. And I just noticed actually
 here, the fan in for w one is actually an embed times block size, which as you all recall is
 actually 30. And that's because each character is 10 dimensional, but then we have three of them
 and we concatenate them. So actually the fan in here was 30. And I should have used 30 here probably.
 But basically, we want 30 square root. So this is the number, this is what our standard
 deviation we want to be. And this number turns out to be point three. Whereas here, just by
 fiddling with it and looking at the distribution and making sure it looks okay, we came up with
 point two. And so instead, what we want to do here is we want to make the standard deviation be
 five over three, which is our gain divide. This amount, times point two, square root. And these
 brackets here are not that necessary, but I'll just put them here for clarity. This is basically
 what we want. This is the timing in it, in our case, for a 10 h nonlinearity. And this is how we
 would initialize the neural mat. And so we're multiplying by point three, instead of multiplying
 by point two. And so we can, we can initialize this way. And then we can train the neural mat and
 see what we got. Okay, so I trained the neural mat, and we end up in roughly the same spot. So
 looking at the value should loss, we now get 2.10. And previously, we also had 2.10. There's a little
 bit of a difference, but that's just the randomness of the process, I suspect. But the big deal, of
 course, is we get to the same spot, but we did not have to introduce any magic numbers that we got
 from just looking at histograms and guess and checking, we have something that is semi-principled
 and will scale us to much bigger networks, and something that we can sort of use as a guide.
 So I mentioned that the precise setting of these initializations is not as important today due to
 some modern innovations. And I think now is a pretty good time to introduce one of those modern
 innovations. And that is batch normalization. So batch normalization came out in 2015 from a
 team at Google. And it was an extremely impactful paper because it made it possible to train very
 deep neural nets quite reliably. And basically just worked. So here's what batch normalization
 does and what's implemented. Basically, we have these hidden states, H preact, right? And we were
 talking about how we don't want these preactivation states to be way too small because then the 10H
 is not doing anything. But we don't want them to be too large because then the 10H is saturated.
 In fact, we want them to be roughly, roughly Gaussian. So zero mean and a unit or a one standard
 deviation, at least at initialization. So the insight from the batch normalization paper is,
 okay, you have these hidden states, and you'd like them to be roughly Gaussian, then why not
 take the hidden states and just normalize them to be Gaussian. And it sounds kind of crazy, but
 you can just do that because standardizing hidden states so that their unit Gaussian is a perfectly
 differential operation as well as C. And so that was kind of like the big insight in this paper.
 And when I first read it, my mind was blown because you can just normalize these hidden states. And
 if you'd like unit Gaussian states in your network, at least initialization, you can just normalize
 them to be unit Gaussian. So let's see how that works. So we're going to scroll to our preactivations
 here just before they enter into the 10H. Now the idea again is remember, we're trying to make
 these roughly Gaussian. And that's because if these are way too small numbers, then the 10H
 here is kind of inactive. But if these are very large numbers, then the 10H is way to saturate it
 and gradients in the flow. So we'd like this to be roughly Gaussian. So the insight in
 parameterization again is that we can just standardize these activations. So they are
 exactly Gaussian. So here, H preact has a shape of 32 by 200, 32 examples by 200 neurons in the
 end layer. So basically what we can do is we can take H preact and we can just calculate the mean.
 And the mean we want to calculate across the zero dimension. And we want to also keep them as true,
 so that we can easily broadcast this. So the shape of this is one by 200. In other words,
 we are doing the mean over all the elements in the batch. And similarly, we can calculate the
 standard deviation of these activations. And that will also be one by 200. Now in this paper,
 they have the sort of prescription here. And see here, we are calculating the mean, which is just
 taking the average value of any neurons activation. And then the standard deviation is basically kind
 of like the measure of the spread that we've been using, which is the distance of every one of
 these values away from the mean, and that squared and averaged. That's the variance. And then if you
 want to take the standard deviation, you will square root the variance to get the standard deviation.
 So these are the two that we're calculating. And now we're going to normalize or standardize
 these x is by subtracting the mean and dividing by the standard deviation. So basically, we're
 taking H preact, and we subtract the mean. And then we divide by the standard deviation.
 This is exactly what these two STD and mean are calculating. Oops. Sorry, this is the mean,
 and this is the variance. You see how the sigma is the standard deviation usually. So this is sigma
 square, which is the variance is the square of the standard deviation. So this is how you
 standardize these values. And what this will do is that every single neuron now, and its firing rate
 will be exactly unit caution on these 32 examples, at least of this batch. That's why it's called
 batch normalization. We are normalizing these batches. And then we could in principle, train
 this, notice that calculating the mean and their standard deviation, these are just mathematical
 formulas, they're perfectly differentiable. All this is perfectly differentiable. And we can just
 train this. The problem is you actually won't achieve a very good result with this. And the
 reason for that is we want these to be roughly Gaussian, but only at initialization. But we don't
 want these to be to be forced to be Gaussian always. We would like to allow the neural net to
 move this around to potentially make it more diffuse, to make it more sharp, to make some 10
 h neurons maybe more trigger happy or less trigger happy. So we'd like this distribution to move
 around. And we'd like the back propagation to tell us how the distribution should move around.
 And so in addition to this idea of standardizing the activations at any point in the network,
 we have to also introduce this additional component in the paper. Here, describe the scale and shift.
 And so basically what we're doing is we're taking these normalized inputs, and we are
 additionally scaling them by some gain and offsetting them by some bias to get our final output from
 this layer. And so what that amounts to is the following. We are going to allow a batch
 normalization gain to be initialized at just once. And the once will be in the shape of one by n
 hidden. And then we also will have a b and bias, which will be torched at zeros. And it will also be
 of the shape n by one by n hidden. And then here, the b and gain will multiply this.
 And the b and bias will offset it here. So because this is initialized to one and this to zero,
 at initialization, each neurons firing values in this batch will be exactly unit Gaussian,
 and will have nice numbers, no matter what the distribution of the H-preact is coming in,
 coming out, it will be unit Gaussian for each neuron. And that's roughly what we want,
 at least at initialization. And then during optimization, we'll be able to back propagate
 to b and gain and b and bias and change them. So the network is given the full ability to do with
 this whatever it wants internally. Here, we just have to make sure that we include these
 in the parameters of the neural mat, because they will be trained with back propagation.
 So let's initialize this. And then we should be able to train.
 And then we're going to also copy this line, which is the best normalization layer,
 here on a single line of code. And we're going to swing down here, and we're also going to
 do the exact same thing at test time here.
 So similar to train time, we're going to normalize, and then scale. And that's going to give us our
 train and validation loss. And we'll see in a second that we're actually going to change this
 a little bit, but for now I'm going to keep it this way. So I'm just going to wait for this to
 converge. Okay, so I allowed the neural nets to converge here. And when we scroll down, we see
 that our validation loss here is 2.10 roughly, which I wrote down here. And we see that this is
 actually kind of comparable to some of the results that we've achieved previously. Now,
 I'm not actually expecting an improvement in this case. And that's because we are dealing with a
 very simple neural net that has just a single hidden layer. So in fact, in this very simple case,
 I've just wanted a layer, we were able to actually calculate what the scale of W should be to make
 these preactivations already have a roughly Gaussian shape. So the bastardization is not
 doing much here. But you might imagine that once you have a much deeper neural net that has lots
 of different types of operations. And there's also, for example, residual connections, which we'll cover
 and so on, it will become basically very, very difficult to tune the scales of your
 weight matrices such that all the activations throughout the neural net are roughly Gaussian.
 And so that's going to become very quickly intractable. But compared to that, it's going to be much,
 much easier to sprinkle batch normalization layers throughout the neural net. So in particular,
 it's common to look at every single linear layer like this one. This is a linear layer
 multiplied by weight matrix and adding the bias. Or for example, convolutions, which we'll cover
 later and also perform basically a multiplication with weight matrix, but in a more spatially structured
 format. It's customer, it's customary to take these linear layer or convolutional layer and
 append a batch normalization layer right after it to control the scale of these activations at
 every point in the neural net. So we'd be adding these batch normal layers throughout the neural
 net, and then this controls the scale of these activations throughout the neural net. It doesn't
 require us to do perfect mathematics and care about the activation distributions for all these
 different types of neural network lego building blocks that you might want to introduce into your
 neural net. And it significantly stabilizes the training. And that's why these layers are quite
 popular. Now the stability offered by batch normalization actually comes at a terrible cost.
 And that cost is that if you think about what's happening here, something terribly strange and
 unnatural is happening. It used to be that we have a single example feeding into a neural net,
 and then we calculate this activations and its logits. And this is a deterministic sort of process,
 so you arrive at some logits for this example. And then because of efficiency of training,
 we suddenly started to use batches of examples. But those batches of examples were processed
 independently, and it was just an efficiency thing. But now suddenly in batch normalization,
 because of the normalization through the batch, we are coupling these examples mathematically,
 and in the forward pass and backward pass of a neural net. So now the hidden state activations,
 HP Act, and your logits for any one input example are not just a function of that example and its
 input, but they're also a function of all the other examples that happen to come for a ride
 in that batch. And these examples are sampled randomly. And so what's happening is for example,
 when you look at HP Act that's going to feed into H, the hidden state activations, for example,
 for any one of these input examples, is going to actually change slightly depending on what
 other examples there are in the batch. And depending on what other examples happen to come for a ride,
 H is going to change suddenly, and it's going to look jitter if you imagine sampling different
 examples, because the statistics of the mean and standard deviation are going to be impacted.
 And so you'll get a jitter for H, and you'll get a jitter for logits. And you think that this
 would be a bug or something undesirable, but in a very strange way, this actually turns out to be
 good in neural network training, and as a side effect. And the reason for that is that you can
 think of this as kind of like a regularizer, because what's happening is you have your input
 and you get your H, and then depending on the other examples, this is jittering a bit. And so
 what that does is that it's effectively padding out any one of these input examples, and it's
 introducing a little bit of entropy. And because of the padding out, it's actually kind of like a
 form of a data augmentation, which we'll cover in the future. And it's like kind of like augmenting
 the input a little bit and it's jittering it. And that makes it harder for the neural
 nuts to overfit these concrete specific examples. So by introducing all this noise,
 it actually like pats out the examples, and it regularizes the neural net. And that's one of the
 reasons why the seemingly as a second order effect, this is actually a regularizer. And that has made
 it harder for us to remove the use of batch normalization. Because basically no one likes this property
 that the examples in the batch are coupled mathematically and in the forward pass. And at least all kinds
 of like strange results, we'll go into some of that in a second as well. And at least to a lot of bugs
 and, and so on. And so no one likes this property. And so people have tried to
 deprecate the use of batch normalization and move to other normalization techniques that do
 not couple the examples of batch examples are linear normalization, instance normalization,
 group normalization, and so on. And we'll come, we'll come and sound these later.
 But basically long story short, batch normalization was the first kind of normalization layer to be
 introduced. It worked extremely well. It happened to have this regularizing effect. It stabilized
 training. And people have been trying to remove it and move to some of the other normalization
 techniques. But it's been hard because it just works quite well. And some of the reason that it
 works quite well is again because of this regularizing effect and because of the, because it is quite
 effective at controlling the activations and their distributions. So that's kind of like the
 brief story of batch normalization. And I'd like to show you one of the other weird sort of outcomes
 of this couple. So here's one of the strange outcomes that I only glossed over previously.
 When I was valuing the loss on the validation set, basically once we've trained a neural net,
 we'd like to deploy it in some kind of a setting. And we'd like to be able to feed in a single
 individual example and get a prediction out from our neural net. But how do we do that when our
 neural net now in a forward pass estimates the statistics of the mean and standard deviation of
 a batch, the neural net expects batches as an input now. So how do we feed in a single example
 and get sensible results out? And so the proposal in the batch normalization paper is the following.
 What we would like to do here is we would like to basically have a step after training
 that calculates and sets the batch room mean and standard deviation a single time over the training
 set. And so I wrote this code here in interest of time. And we're going to call what's called
 calibrate the batch room statistics. And basically what we do is torch dot torch dot no grad telling
 by torch that none of this we will call the duck backward on. And it's going to be a bit more
 efficient. We're going to take the training set, get the preactivations for every single training
 example. And then one single time estimate the mean and standard deviation or the entire training
 set. And then we're going to get B and mean and B and standard deviation. And now these are fixed
 numbers, estimating of the entire training set. And here, instead of estimating it dynamically,
 we are going to instead here use B and mean. And here, we're just going to use B and standard
 deviation. And so at test time, we are going to fix these, clamp them and use them during inference.
 And now you see that we get basically identical result. But the benefit that we've gained is that
 we can now also forward a single example, because the mean and standard deviation are now fixed
 sort of tensors. That said, nobody actually wants to estimate this mean and standard deviation as a
 second stage after neural network training, because everyone is lazy. And so this batch
 normalization paper actually introduced one more idea, which is that we can we can estimate the
 mean and standard deviation in a running matter, running matter during training of the neural
 network. And then we can simply just have a single stage of training. And on the side of that
 training, we are estimating the running mean and standard deviation. So let's see what that would
 look like. Let me basically take the mean here that we are estimating on the batch. And let me call
 this B and mean on the i-th iteration. And then here, this is B and STD.
 B and STD at i. Okay. And the mean comes here and the STD comes here. So far, I've done nothing.
 I've just moved around and I created these extra variables for the mean and standard deviation.
 And I put them here. So far, nothing has changed. But what we're going to do now is we're going to
 keep a running mean of both of these values during training. So let me swing up here and let me
 create a B and mean underscore running. And I'm going to initialize it at zeros. And then B and
 STD running, which are initialized at once. Because in the beginning, because of the way we
 initialized W1 and B1, H preact will be roughly unit Gaussian. So the mean will be roughly zero.
 And the standard deviation roughly one. So I'm going to initialize these that way. But then here,
 I'm going to update these. And in PyTorch, these mean and standard deviation that are running,
 they're not actually part of the gradient based optimization. We're never going to derive gradients
 with respect to them. They're updated on the side of training. And so what we're good to do here is
 we're going to say with torch.no grad, telling PyTorch that the update here is not supposed to be
 building out a graph because there will be no dot backward. But this running is basically going to
 be 0.99, nine times the current value, plus 0.001 times the this value, this new mean. And in the
 same way, B and STD running will be mostly what it used to be. But it will receive a small update
 in the direction of what the current standard deviation is. And as you're seeing here, this update
 is outside and on the side of the gradient based optimization. And it's simply being updated,
 not using gradient sent, it's just being updated using a janky like smooth sort of running mean
 manner. And so while the network is training, and these preactivations are sort of changing and
 shifting around during back propagation, we are keeping track of the typical mean and standard
 deviation and estimating them once. And when I run this, now I'm keeping track of this in the
 running matter. And what we're hoping for, of course, is that the mean, being mean underscore running
 and being mean underscore STD are going to be very similar to the ones that we calculated here
 before. And that way, we don't need a second stage, because we've served combined the two stages,
 and we've put them on the side of each other, if you want to look at it that way. And this is how
 this is also implemented in the batch normalization layer in PyTorch. So during training, the exact
 same thing will happen. And then later when you're using inference, it will use the estimated running
 mean of both the mean as to deviation of those hidden states. So let's wait for the optimization
 converge. And hopefully the running mean and standard deviation are roughly equal to these two.
 And then we can simply use it here. And we don't need this stage of explicit calibration at the end.
 Okay, so the optimization finished, I'll rerun the explicit estimation. And then the B and mean
 from the explicit estimation is here. And B and mean from the running estimation,
 during the, during the optimization, you can see it's very, very similar. It's not identical,
 but it's pretty close. And in the same way B and STD is this, and B and STD running is this.
 As you can see that once again, they are fairly similar values, not identical, but pretty close.
 And so then here, instead of B and mean, we can use the B and mean running. Instead of B and STD,
 we can use B and STD running. And hopefully the validation loss will not be impacted too much.
 Okay, so basically identical. And this way, we've eliminated the need for this explicit stage of
 calibration, because we are doing it in line over here. Okay, so we're almost done with
 batch normalization. There are only two more notes that I'd like to make. Number one, I've
 skipped a discussion over what is this plus epsilon doing here. This epsilon is usually like some small
 fixed number, for example, one in negative five by default. And what it's doing is that it's
 basically preventing a division by zero. In the case that the variance over your batch is exactly
 zero. In that case, here we normally have a division by zero. But because of the plus epsilon,
 this is going to become a small number in the denominator instead. And things will be more
 well behaved. So feel free to also add a plus epsilon here of a very small number. It doesn't
 actually substantially change the result. I'm going to skip it in our case, just because this is
 unlikely to happen in our very simple example here. And the second thing I want you to notice
 is that we're being wasteful here. And it's very subtle. But right here, where we are adding the bias
 into each react, these biases now are actually useless, because we're adding them to the H
 preact. But then we are calculating the mean for every one of these neurons and subtracting it.
 So whatever bias you add here is going to get subtracted right here. And so these biases are
 not doing anything. In fact, they're being subtracted out and they don't impact the rest of the
 calculation. So if you look at B1 dot grad, it's actually going to be zero, because it's being
 subtracted out and doesn't actually have any effect. And so whenever you're using rationalization
 layers, then if you have any weight layers before, like a linear or a comp or something like that,
 you're better off coming here and just like not using bias. So you don't want to use bias. And then
 here you don't want to add it because that's that spurious. Instead, we have this bachelors
 bias here. And that bachelors bias is now in charge of the biasing of this distribution,
 instead of this B1 that we had here originally. And so basically, the bachelors has its own
 bias. And there's no needs to have a bias in the layer before it, because that bias is going to be
 subtracted up anyway. So that's the other small detail to be careful with sometimes. It's not going
 to do anything catastrophic. This B1 will just be useless. It will never get any gradient. It will
 not learn it will stay constant and it's just wasteful. But it doesn't actually really impact
 anything otherwise. Okay, so I rearranged the code a little bit with comments. And I just wanted to
 give a very quick summary of the Bachelors Realization layer. We are using Bachelors Realization to control
 the statistics of activations in the neural net. It is common to sprinkle Bachelors Realization
 layer across the neural net. And usually we will place it after layers that have multiplications,
 like for example, a linear layer or a convolutional layer, which we may cover in the future.
 Now, the Bachelors Realization internally has parameters for the gain and the bias. And these
 are trained using backpropagation. It also has two buffers. The buffers are the mean and the
 standard deviation, the running mean and the running mean of the standard deviation. And these
 are not trained using backpropagation. These are trained using this janky update of kind of like
 a running mean update. So these are sort of the parameters and the buffers of Bachelors Realization.
 And then really what it's doing is it's calculating the mean and the standard deviation of the
 activations that are feeding into the Bachelors Realization layer over that batch. Then it's
 centering that batch to be unit Gaussian. And then it's offsetting and scaling it by the learned
 bias and gain. And then on top of that, it's keeping track of the mean and standard deviation
 of the inputs. And it's maintaining this running mean and standard deviation. And this will later
 be used at inference so that we don't have to re-estimate the mean and standard deviation all the time.
 And in addition, that allows us to basically forward individual examples at test time. So that's the
 Bachelors Realization layer. It's a fairly complicated layer. But this is what it's doing
 internally. Now I wanted to show you a little bit of a real example. So you can search resnet,
 which is a residual neural network. And these are contacts of neural arcs used for image classification.
 And of course, we haven't come in dress nets in detail. So I'm not going to explain all the pieces
 of it. But for now, just note that the image feeds into a resnet on the top here. And there's many,
 many layers with repeating structure, all the way to predictions of what's inside that image.
 This repeating structure is made up of these blocks. And these blocks are just sequentially
 stacked up in this deep neural network. Now the code for this, the block basically that's used
 and repeated sequentially in series is called this bottleneck block, bottleneck block.
 And there's a lot here. This is all pytorch. And of course, we haven't covered all of it,
 but I want to point out some small pieces of it. Here in the init is where we initialize the
 neural net. So this code of block here is basically the kind of stuff we're doing here. We're initializing
 all the layers. And in the forward, we are specifying how the neural net acts once you actually have
 the input. So this code here is along the lines of what we're doing here. And now these blocks
 are replicated and stacked up serially. And that's what a residual network would be.
 And so notice what's happening here. Com one, these are convolutional layers. And these convolutional
 layers, basically, they're the same thing as a linear layer, except convolutional layers don't
 apply. Convolutional layers are used for images. And so they have spatial structure. And basically,
 this linear multiplication and bias offset are done on patches instead of a map instead of the
 full input. So because these images have structure spatial structure, convolutions just basically do
 wx plus b, but they do it on overlapping patches of the input. But otherwise, it's wx plus b.
 Then we have the normal layer, which by default here is initialized to be a batch norm in 2d. So
 two dimensional batch normalization layer. And then we have a nonlinearity like relu. So instead of
 here they use relu, we are using 10h in this case. But both both are just nonlinearities and you
 can just use them relatively interchangeably for very deep networks relu typically empirically
 work a bit better. So see the motif that's being repeated here, we have convolution,
 batch normalization relu convolution, batch normalization relu, etc. And then here,
 this is residual connection that we haven't covered yet. But basically, that's the exact same pattern
 we have here, we have a weight layer like a convolution or like a linear layer, batch normalization,
 and then 10h, which is a nonlinearity. But basically, a weight layer, a normalization layer,
 and nonlinearity. And that's the motif that you would be stacking up when you create these
 deep neural networks exactly as it's done here. And one more thing I'd like you to notice is that
 here when they are initializing the conv layers, like conv one by one, the depth for that is right
 here. And so it's initializing an nn.conf2d, which is a convolutional layer in PyTorch.
 And there's much of keyword arguments here that I'm not going to explain yet. But you see how
 there's bias equals false. The bias equals false is exactly for the same reason as bias is not used
 in our case. You see how I raised the use of bias. And the use of bias is spurious because after this
 weight layer, there's a batch normalization. And the batch normalization subtracts that bias,
 and then has its own bias. So there's no need to introduce these spurious parameters.
 It wouldn't hurt performance, it's just useless. And so because they have this motif of conv,
 batch normalization, they don't need a bias here, because there's a bias inside here.
 So by the way, this example here is very easy to find. Just do a resnet PyTorch.
 And it's this example here. So this is kind of like the stock implementation of a residual neural
 network in PyTorch. And you can find that here. But of course, I haven't covered many of these
 parts yet. And I would also like to briefly descend into the definitions of these PyTorch layers and
 the parameters that they take. Now, instead of a convolutional layer, we're going to look at a
 linear layer, because that's the one that we're using here. This is a linear layer. And I haven't
 covered convolutions yet. But as I mentioned, convolutions are basically linear layers,
 except on patches. So a linear layer performs a Wx+b, except here they're calling the W-a transpose.
 So the clock is Wx+b very much like we did here. To initialize this layer, you need to know the
 fan in, the fan out. And that's so that they can initialize this W. This is the fan in and the fan
 out. So they know how big the weight matrix should be. You need to also pass in whether or not you
 want a bias. And if you set it to false, then no bias will be inside this layer. And you may want
 to do that exactly like in our case, if your layer is followed by a normalization layer, such as
 BatchNorm. So this allows you to basically disable bias. Now, in terms of the initialization,
 if we swing down here, this is reporting the variables used inside this linear layer. And
 our linear layer here has two parameters, the weight and the bias. In the same way, they have a weight
 and a bias. And they're talking about how they initialize it by default. So by default, pytorch
 will initialize your weights by taking the fan in and then doing one over fan in square root.
 And then instead of a normal distribution, they are using a uniform distribution.
 So it's very much the same thing, but they are using a 1 instead of 5 over 3. So there's no gain
 being calculated here. The gain is just 1. But otherwise, it's exactly 1 over the square root of
 fan in exactly as we have here. So 1 over the square root of k is the scale of the weights.
 But when they are drawing the numbers, they're not using a Gaussian by default. They're using a
 uniform distribution by default. And so they draw uniformly from negative square root of k to square
 root of k. But it's the exact same thing and the same motivation with respect to what we've seen
 in this lecture. And the reason they're doing this is if you have a roughly Gaussian input,
 this will ensure that out of this layer, you will have a roughly Gaussian output. And you
 basically achieve that by scaling the weights by 1 over the square root of fan in. So that's
 what this is doing. And then the second thing is the bacrimalization layer. So let's look at what
 that looks like in PyTorch. So here we have a one-dimensional bacrimalization layer exactly as we are
 using here. And there are a number of keyword arguments going into it as well. So we need to
 know the number of features. For us, that is 200. And that is needed so that we can initialize
 these parameters here. The gain, the bias, and the buffers for the running mean and standard deviation.
 Then they need to know the value of epsilon here. And by default, this is 1, negative 5. You don't
 typically change this too much. Then they need to know the momentum. And the momentum here, as they
 explain, is basically used for these running mean and running standard deviation. So by default,
 the momentum here is 0.1. The momentum we are using here in this example is 0.001. And basically,
 you may want to change this sometimes. And roughly speaking, if you have a very large batch size,
 then typically what you'll see is that when you estimate the mean and the standard deviation,
 for every single batch size, if it's large enough, you're going to get roughly the same result.
 And so therefore, you can use slightly higher momentum like 0.1. But for a batch size as small as
 32, the mean and standard deviation here might take on slightly different numbers, because there's
 only 32 examples we are using to estimate the mean and standard deviation. So the value is
 changing around a lot. And if your momentum is 0.1, that might not be good enough for this value to
 settle and converge to the actual mean and standard deviation over the entire training set. And so
 basically, if your batch size is very small, momentum of 0.1 is potentially dangerous. And it
 might make it so that the running mean and standard deviation is thrashing too much during training.
 And it's not actually converging properly.
 affine equals true determines whether this batch normalization layer has these learnable affine
 parameters, the gain and the bias. And this is almost always kept to true. I'm not actually sure
 why you would want to change this to false. Then track running stats is determining whether or
 not batch normalization layer of PyTorch will be doing this. And one reason you may want to skip
 the running stats is because you may want to, for example, estimate them at the end as a stage two
 like this. And in that case, you don't want the batch normalization layer to be doing all this
 extra compute that you're not going to use. And finally, we need to know which device we're
 going to run this batch normalization on a CPU or a GPU, and what the data type should be,
 half precision, single precision, double precision, and so on. So that's the batch normalization
 layer. Otherwise, they link to the paper is the same formula we've implemented, and everything is
 the same exactly as we've done here. Okay, so that's everything that I wanted to cover for this lecture.
 Really, what I wanted to talk about is the importance of understanding the activations and
 the gradients and their statistics in neural networks. And this becomes increasingly important,
 especially as you make your neural networks bigger, larger, and deeper. We looked at the
 distributions basically at the output layer, and we saw that if you have two confident mispredictions,
 because the activations are too messed up at the last layer, you can end up with these hockey
 stick losses. And if you fix this, you get a better loss at the end of training, because your training
 is not doing wasteful work. Then we also saw that we need to control the activations. We don't want
 them to, you know, squash to zero or explode to infinity. And because that you can run into a lot
 of trouble with all of these nonlinearities in these neural nets. And basically, you want
 everything to be fairly homogeneous throughout the neural net, you want roughly Gaussian activations
 throughout the neural net. Let me talk about, okay, if we want roughly Gaussian activations,
 how do we scale these weight matrices and biases during initialization of the neural net,
 so that we don't get, you know, so everything is as controlled as possible.
 So that gave us large boost and improvement. And then I talked about how that strategy is not
 actually possible for much, much deeper neural nets, because when you have much deeper neural
 nets with lots of different types of layers, it becomes really, really hard to precisely set
 the weights and the biases in such a way that the activations are roughly uniform throughout the
 neural net. So then I introduced the notion of the normalization layer. Now, there are many
 normalization layers that people use in practice, bash normalization, layer normalization,
 consistent normalization, group normalization. We haven't covered most of them, but I've introduced
 the first one. And also the one that I believe came out first, and that's called bash normalization.
 And we saw how bash normalization works. This is a layer that you can sprinkle throughout your
 deep neural net. And the basic idea is if you want roughly Gaussian activations,
 well, then take your activations and take the mean and the standard deviation and center your
 data. And you can do that because the centering operation is differentiable. But then on top of
 that, we actually had to add a lot of bells and whistles. And that gave you a sense of the
 complexities of the bash normalization layer, because now we're centering the data, that's great.
 But suddenly, we need the gain and the bias. And now those are trainable. And then because we are
 coupling all the training examples, now suddenly the question is, how do you do the inference?
 Or to do the inference, we need to now estimate these mean and standard deviation once
 or the entire training set, and then use those at inference. But then no one likes to do stage two.
 So instead, we fold everything into the bash normalization layer during training and try to
 estimate these in the running manner so that everything is a bit simpler. And that gives us
 the bash normalization layer. And as I mentioned, no one likes this layer. It causes a huge amount
 of bugs. And intuitively, it's because it is coupling examples in the form of bass of a neural
 nut. And I've shocked myself in the foot with this layer over and over again in my life.
 And I don't want you to suffer the same. So basically try to avoid it as much as possible.
 Some of the other alternatives to these layers are, for example, group normalization or layer
 normalization. And those have become more common in more recent deep learning. But we haven't covered
 those yet. But definitely bash normalization was very influential at the time when it came out
 in roughly 2015, because it was kind of the first time that you could train reliably
 much deeper neural nets. And fundamentally, the reason for that is because this layer was very
 effective at controlling the statistics of the activations in a neural nut. So that's the story
 so far. And that's all I wanted to cover. And in the future lecture, so hopefully we can start
 going into recurrent neural nets. And recurrent neural nets, as we'll see, are just very, very deep
 networks, because you unroll the loop. And when you actually optimize these neural nets. And that's
 where a lot of this analysis around the activation statistics and all these normalization layers
 will become very, very important for good performance. So we'll see that next time.
 Okay, so I lied. I would like us to do one more summary here as a bonus. And I think it's useful
 as to have one more summary of everything I've presented in this lecture. But also, I would
 like us to start by torturing our code a little bit. So it looks much more like what you would
 encounter in PyTorch. So you'll see that I will structure our code into these modules, like a
 linear module and a batch room module. And I'm putting the code inside these modules,
 so that we can construct neural networks very much like we would construct the
 in PyTorch. And I will go through this in detail. So we'll create our neural net,
 then we will do the optimization loop as we did before. And then the one more thing that I want
 to do here is I want to look at the activation statistics both in the forward pass and in the
 backward pass. And then here we have the evaluation and sampling just like before. So let me rewind
 all the way up here and go a little bit slower. So here I am creating a linear layer. You'll notice
 that Torch.NN has lots of different types of layers. And one of those layers is the linear layer.
 Torch.NN.linear takes a number of input features, output features, whether or not we should
 have bias, and then the device that we want to place this layer on and the data type. So I will
 omit these two, but otherwise we have the exact same thing. We have the fan in, which is the number
 of inputs, fan out the number of outputs, and whether or not we want to use a bias. And internally
 inside this layer, there's a weight and a bias if you'd like it. It is typical to initialize the
 weight using, say, random numbers drawn from Gaussian. And then here's the timing initialization
 that we discussed already in this lecture. And that's a good default and also the default that I
 believe PyTorch uses. And by default, the bias is usually initialized to zeros. Now when you call
 this module, this will basically calculate W times X plus B if you have Nb. And then when you
 also call that parameters on this module, it will return the tensors that are the parameters of this
 layer. Now next we have the pass-formalization layer. So I've written that here. And this is
 very similar to PyTorch and then that bash normal 1D layer as shown here. So I'm kind of taking
 these three parameters here, the dimensionality, the epsilon that we'll use in the division,
 and the momentum that we will use in keeping track of these running stats, the running mean
 and the running variance. Now PyTorch actually takes quite a few more things, but I'm assuming
 some of their settings. So for us, affine will be true. That means that we will be using a gamma
 beta after the normalization. The track running stats will be true. So we will be keeping track
 of the running mean and the running variance in the in the bashroom. Our device by default is the
 CPU and the data type by default is float, float 32. So those are the defaults. Otherwise,
 we are taking all the same parameters in this bashroom layer. So first I'm just saving them.
 Now here's something new. There's a dot training which by default is true. And PyTorch and
 in modules also have this attribute, that training. And that's because many modules and bashroom is
 included in that have a different behavior, whether you are training your own lot or whether
 you are running it in an evaluation mode and calculating your evaluation laws or using it for
 inference on some test examples. And bashroom is an example of this because when we are training,
 we are going to be using the mean and the variance estimated from the current batch.
 But during inference, we are using the running mean and running variance. And so also if we are
 training, we are updating mean and variance. But if we are testing, then these are not being
 updated. They're kept fixed. And so this flag is necessary and by default true, just like in PyTorch.
 Now the parameters of bashroom 1D are the gamma and the beta here. And then the running mean and
 running variance are called buffers in PyTorch nomenclature. And these buffers are trained using
 exponential moving average here explicitly. And they are not part of the back propagation
 and stochastic gradient descent. So they are not sort of like parameters of this layer. And that's
 why when we have a parameters here, we only return gamma and beta. We do not return the mean and
 the variance. This is trained sort of like internally here every forward pass using exponential moving
 average. So that's the initialization. Now in a forward pass, if we are training, then we use the
 mean and the variance estimated by the batch. We will have the paper here. We calculate the mean
 and the variance. Now up above, I was estimating the standard deviation and keeping track of the
 standard deviation here in the running standard deviation instead of running variance. But let's
 follow the paper exactly. Here they calculate the variance, which is the standard deviation squared.
 And that's what's kept track of in the running variance instead of a running standard deviation.
 But those two would be very, very similar, I believe. If we are not training, then we use
 running mean and variance. We normalize. And then here I am calculating the output of this layer.
 And I'm also assigning it to an attribute called dot out. Now that out is something that I'm using
 in our modules here. This is not what you would find in PyTorch. We are slightly deviating from it.
 I'm creating a dot out because I would like to very easily maintain all those variables so that
 we can create statistics of them and plot them. But PyTorch and modules will not have a dot out
 attribute. And finally, here we are updating the buffers using again, as I mentioned,
 exponential moving average, given the provided momentum. And importantly, you'll notice that
 I'm using the Torstav no-grat context manager. And I'm doing this because if we don't use this,
 then PyTorch will start building out an entire computational graph out of these tensors because
 it is expecting that we will eventually call that backward. But we are never going to be calling
 that backward on anything that includes running mean and running variance. So that's why we need
 to use this context manager so that we are not maintaining them using all this additional memory.
 So this will make it more efficient. And it's just telling PyTorch that while we know backward,
 we just have a bunch of tensors. We want to update them. That's it. And then we return.
 Okay, now scrolling down, we have the 10H layer. This is very, very similar to Torst.10H. And it
 doesn't do too much. It just calculates 10H, as you might expect. So that's Torst.10H. And
 there's no parameters in this layer. But because these are layers, it now becomes very easy to sort
 of like stack them up into basically just a list. And we can do all the initializations that we're
 used to. So we have the initial sort of embedding matrix. We have our layers and we can call them
 sequentially. And then again, with Torstav no-grat, there's some initializations here. So we want to
 make the output softmax a bit less confident like we saw. And in addition to that, because we are
 using a six layer multi layer perception here, so you see how I'm stacking linear, 10H, linear,
 10H, etc. I'm going to be using the game here. And I'm going to play with this in a second. So
 you'll see how when we change this, what happens to this statistics. Finally, the parameters are
 basically the embedding matrix and all the parameters in all the layers. And notice here,
 I'm using a double list comprehension. If you want to call it that, but for every layer in layers
 and for every parameter in each of those layers, we are just stacking up all those piece,
 all those parameters. Now, in total, we have 46,000 parameters. And I'm telling Patures that all of
 them require gradient. Then here, we have everything here we are actually mostly used to.
 We are sampling batch. We are doing a forward pass. The forward pass now is just the linear
 application of all the layers in order, followed by the cross entropy. And then in the backward pass,
 you'll notice that for every single layer, I now iterate over all the outputs. And I'm telling
 PyTorch to retain the gradient of them. And then here we are already used to all the all the gradients
 set to none, do the backward to fill in the gradients, do an update using stochastic gradient send,
 and then track some statistics. And then I am going to break after a single iteration.
 Now here in this cell in this diagram, I'm visualizing the histograms of the forward pass
 activations. And I'm specifically doing it at the 10 H layers. So iterating over all the layers,
 except for the very last one, which is basically just the softmax layer.
 If it is a 10 H layer, and I'm using a 10 H layer just because they have a finite output,
 negative one to one. And so it's very easy to visualize here. So you see negative one to one,
 and it's a finite range, and it is to work with. I take the out tensor from that layer into T,
 and then I'm calculating the mean, the standard deviation, and the percent saturation of T.
 And the way I define the percent saturation is that T dot absolute value is greater than 0.97.
 So that means we are here at the tails of the 10 H. And remember that when we are in the tails
 of the 10 H, that will actually stop gradients. So we don't want this to be too high. Now,
 here I'm calling Torx dot histogram, and then I am plotting this histogram.
 So basically what this is doing is that every different type of layer, and they all have a
 different color, we are looking at how many values in these testers take on any of the values below
 on this axis here. So the first layer is fairly saturated here at 20%. So you can see that it's
 got tails here, but then everything sort of stabilizes. And if we had more layers here,
 it would actually just stabilize at around the standard deviation of about 0.65,
 and the saturation would be roughly 5%. And the reason that this stabilizes and gives us a nice
 distribution here is because gain is set to 5 over 3. Now, here, this gain, you see that by
 default, we initialize with one over square root of fan in. But then here during initialization,
 I come in and I iterate over all the layers. And if it's a linear layer, I boost that by the gain.
 Now we saw that one, so basically if we just do not use a gain, then what happens? If I redraw this,
 you will see that the standard deviation is shrinking, and the saturation is coming to 0.
 And basically what's happening is the first layer is, you know, pretty decent, but then
 further layers are just kind of like shrinking down to 0. And it's happening slowly, but it's
 shrinking to 0. And the reason for that is when you just have a sandwich of linear layers alone,
 then a then initializing our weights in this manner, we saw previously would have conserved
 the standard deviation of 1. But because we have this interspersed 10H layers in there,
 these 10-linked layers are squashing functions. And so they take your distribution and they
 slightly squash it. And so some gain is necessary to keep expanding it, to fight the squashing.
 So it just turns out that 5/3 is a good value. So if we have something too small, like 1,
 we saw that things will come towards 0. But if it's something too high, let's do 2.
 Then here we see that, well, let me do something a bit more extreme because,
 so it's a bit more visible. Let's try 3. Okay, so we see here that the saturation is going to be
 way too large. Okay, so 3 would create way too saturated activations. So 5/3 is a good setting
 for a sandwich of linear layers with 10H activations, and it roughly stabilizes the standard deviation
 at a reasonable point. Now, honestly, I have no idea where 5/3 came from in PyTorch.
 When we were looking at the coming initialization, I see empirically that it stabilizes this sandwich
 of linear and 10H, and that the saturation is in a good range. But I didn't actually know
 if this came out of some math formula. I tried searching briefly for where this comes from,
 but I wasn't able to find anything. But certainly we see that empirically, these are very nice
 ranges. Our saturation is roughly 5%, which is a pretty good number. And this is a good setting
 of the gain in this context. Similarly, we can do the exact same thing with the gradients.
 So here is a very same loop if it's a 10H, but instead of taking the layer out, I'm taking the
 grad. And then I'm also showing the mean and the standard deviation, and I'm plotting the
 histogram of these values. And so you'll see that the gradient distribution is fairly reasonable,
 and in particular, what we're looking for is that all the different layers in this sandwich
 has roughly the same gradient. Things are not shrinking or exploding. So we can, for example,
 come here and we can take a look at what happens if this gain was way too small. So this was 0.5.
 Then you see the, first of all, the activations are shrinking to zero, but also the gradients
 are doing something weird. The gradients started out here, and then now they're like expanding out.
 And similarly, if we, for example, have a too high of a gain, so like 3,
 then we see that also the gradients have, there's some asymmetry going on where as you go into
 deeper and deeper layers, the activations are also changing. And so that's not what we want.
 And in this case, we saw that without the use of batch room, as we are going through right now,
 we have to very carefully set those gains to get nice activations in both the forward pass
 and the backward pass. Now, before we move on to batch normalization, I would also like to take a
 look at what happens when we have no 10H units here. So erasing all the 10H nonlinearities,
 but keeping the gain at 5/3, we now have just a giant linear sandwich. So let's see what happens
 to the activations. As we saw before, the correct gain here is 1. That is the standard deviation
 preserving gain. So 1.667 is too high. And so what's going to happen now is the following.
 I have to change this to be linear, so we are, because there's no more 10H players.
 And let me change this to linear as well. So what we're seeing is the activations started out on the
 blue and have by layer 4 become very diffuse. So what's happening to the activations is this.
 And with the gradients on the top layer, the activation, the gradient statistics are the purple,
 and then they diminish as you go down deeper in the layers. And so basically you have an asymmetry,
 like in the neural net. And you might imagine that if you have very deep neural networks,
 say like 50 layers or something like that, this just, this is not a good place to be.
 So that's why before that normalization, this was incredibly tricky to set. In particular,
 if this is too large of a gain, this happens. And if it's too little of a gain, then this happens.
 So the opposite of that basically happens. Here we have a shrinking and a diffusion,
 depending on which direction you look at it from. And so certainly this is not what you want.
 And in this case, the correct setting of the gain is exactly one, just like we're doing at
 initialization. And then we see that the statistics for the forward and the backward pass are well
 behaved. And so the reason I want to show you this is that basically like getting neuralness to train
 before these normalization layers, and before the use of advanced optimizers like Adam, which we
 still have to cover and residual connections and so on, training neural lines basically look like
 this. It's like a total balancing act. You have to make sure that everything is precisely orchestrated,
 and you have to care about the activations and the gradients and their statistics,
 and then maybe you can train something. But it was basically impossible to train very deep
 networks. And this is fundamentally the reason for that. You'd have to be very, very careful
 with your initialization. The other point here is you might be asking yourself, by the way,
 I'm not sure if I covered this. Why do we need these 10H layers at all? Why do we include them
 and then have to worry about the gain? And the reason for that, of course, is that if you just
 have a stack of linear layers, then certainly we're getting very easily nice activations and so on.
 But this is just a massive linear sandwich. And it turns out that it collapses to a single
 linear layer in terms of its representation power. So if you were to plot the output as a
 function of the input, you're just getting a linear function. No matter how many linear layers you
 stack up, you still just end up with a linear transformation. All the wx plus b's just collapse
 into a large wx plus b with slightly different w's as likely different b. But interestingly,
 even though the forward pass collapses to just a linear layer, because of back propagation and
 the dynamics of the backward pass, the optimization, actually, is not identical. You actually end up
 with all kinds of interesting dynamics in the backward pass because of the way the chain rule
 is calculating it. And so optimizing a linear layer by itself and optimizing a sandwich of 10
 linear layers. In both cases, those are just a linear transformation in the forward pass,
 but the training dynamics would be different. And there's entire papers that analyze, in fact,
 like infinitely layered linear layers and so on. And so there's a lot of things too that you can
 play with there. But basically, the attention of linearities allow us to turn this sandwich from
 just a linear function into a neural network that can, in principle, approximate any arbitrary
 function. Okay, so now I've reset the code to use the linear 10H sandwich like before. And I
 reset everything. So the gains five over three, we can run a single step of optimization. And we
 can look at the activation statistics of the forward pass and the backward pass. But I've added one
 more plot here that I think is really important to look at when you're training your own nuts
 and to consider. And ultimately, what we're doing is we're updating the parameters of the neural
 net. So we care about the parameters and their values and their gradients. So here, what I'm doing
 is I'm actually iterating over all the parameters available. And then I'm only restricting it to
 the two dimensional parameters, which are basically the weights of these linear layers. And I'm skipping
 the biases, and I'm skipping the gammas and the betas in the bathroom just for simplicity. But you
 can also take a look at those as well. But what's happening with the weights is, um,
 instructive by itself. So here we have all the different weights, their shapes. So this is the
 embedding layer, the first linear layer, all the way to the very last linear layer. And then we have
 the mean, the standard deviation of all these parameters, the histogram. And you can see that
 actually doesn't look that amazing. So there's some trouble in paradise. Even though these gradients
 looked okay, there's something weird going on here. I'll get to that in a second. And then the last
 thing here is the gradient to data ratio. So sometimes I like to visualize this as well, because
 what this gives you a sense of is what is the scale of the gradient compared to the scale of the
 actual values. And this is important because we're going to end up taking a step update. Um,
 that is the learning rate times the gradient onto the data. And so the gradient has too large of
 magnitude. If the numbers in there are too large compared to the numbers in data, then you'd be in
 trouble. But in this case, the gradient to data is our loan numbers. So the values inside grad are
 1000 times smaller than the values inside data in these weights, most of them. Now,
 notably, that is not true about the last layer. And so the last layer actually here, the output
 layer is a bit of a troublemaker in the way that this is currently arranged, because you can see that
 the last layer here in pink takes on values that are much larger than some of the values inside
 inside the neural net. So the standard deviations are roughly one in negative three throughout,
 except for the last, last layer, which actually has roughly one in negative two standard deviation
 of gradients. And so the gradients on the last layer are currently about 100 times greater,
 sorry, 10 times greater than all the other weights inside the neural net. And so that's
 problematic because in the simplest stochastic gradient in this sense, set up, you would be
 training this last layer about 10 times faster than you would be training the other layers at
 initialization. Now this actually like kind of fixes itself a little bit if you train for a bit
 longer. So for example, if I agree, then 1000, only then do a break. Let me re-initialize,
 and then let me do it 1000 steps. And after 1000 steps, we can look at the forward pass.
 Okay, so you see how the neurons are a bit are saturating a bit. And we can also look at the
 backward pass. But otherwise they look good. They're about equal. And there's no shrinking to zero
 or exploding to infinities. And you can see that here in the weights, things are also stabilizing
 a little bit. So their tails of the last pink layer are actually coming down coming in during
 the optimization. But certainly this is like a little bit troubling, especially if you are using
 a very simple update rule likes stochastic gradient descent, instead of a modern optimizer like Adam.
 Now I'd like to show you one more plot that I usually look at when I train neural networks. And
 basically the gradient to data ratio is not actually that informative, because what matters at the
 end is not the gradient to data ratio, but the update to the data ratio, because that is the amount
 by which we will actually change the data in these tensors. So coming up here, what I'd like to do
 is I'd like to introduce a new update to data ratio. It's going to be less than we're going to
 build it out every single iteration. And here I'd like to keep track of basically the ratio
 every single iteration. So without any gradients, I'm comparing the update, which is learning rate
 times the times the gradient. That is the update that we're going to apply to every parameter.
 So see I'm entering over all the parameters. And then I'm taking the basically standard deviation
 of the update we're going to apply and divided by the actual content, the data of that parameter
 and its standard deviation. So this is the ratio of basically how great are the updates to the
 values in these tensors. Then we're going to take a log of it. And actually I'd like to take a log
 10, just so it's a nicer visualization. So we're going to be basically looking at the exponents of
 this division here. And then that item to pop out the float. And we're going to be keeping track
 of this for all the parameters and adding it to this UD tensor. So now let me re-inertilize and
 run 1000 iterations. We can look at the activations, the gradients, and the parameter gradients as
 we did before. But now I have one more plot here to introduce. And what's happening here is where
 every interval will be parameters. And I'm constraining it again, like I did here, to just wait.
 So the number of dimensions in these sensors is two. And then I'm basically plotting all of these
 update ratios over time. So when I plot this, I plot those ratios and you can see that they
 evolve over time during initialization that they have got certain values. And then these updates
 are like start stabilizing usually during training. Then the other thing that I'm plotting here is
 I'm plotting here like an approximate value that is a rough guide for what it roughly should be.
 And it should be like roughly one in negative three. And so that means that basically there's
 some values in this tensor. And they take on certain values. And the updates to them at every
 single iteration are no more than roughly 1000 of the actual like magnitude in those tensors.
 If this was much larger, like for example, if this was the log of this was like say
 negative one, this is actually updating those values quite a lot. They're undergoing a lot of
 change. But the reason that the final rate, the final layer here is an outlier is because this
 layer was artificially shrunk down to keep the softmax incompetent. So here, you see how we
 multiply the weight by point one in the initialization to make the last layer prediction less confident.
 That made that artificially made the values inside that tensor way too low. And that's why we're
 getting temporarily a very high ratio. But you see that that stabilizes over time once that
 weight starts to learn, starts to learn. But basically, I like to look at the evolution of
 this update ratio for all my parameters, usually. And I like to make sure that it's not too much
 above one negative three roughly. So around negative three on this log plot. If it's below
 negative three, usually that means that parameters are not training fast enough. So if our learning
 rate was very low, let's do that experiment. Let's initialize. And then let's actually do a
 learning rate of say one in negative three here. So 0.001. If you're learning is way too low,
 this plot will typically reveal it. So you see how all of these updates are way too small. So the
 size of the update is basically 10,000 times in magnitude to the size of the numbers in that
 tensor in the first place. So this is a symptom of training way too slow. So this is another way to
 sometimes start to learn weight and to get a sense of what that learning rate should be.
 And ultimately, this is something that you would keep track of. If anything, the learning rate here
 is a little bit on the higher side, because you see that we're above the black line of
 negative three, we're somewhere around negative 2.5. It's like, okay. But everything is like
 somewhat stabilizing. And so this looks like a pretty decent setting of learning rates and so on.
 But this is something to look at. And when things are miscalibrated, you will see very quickly.
 So for example, everything looks pretty well behaved, right? But just as a comparison,
 when things are not properly calibrated, what does that look like? Let me come up here and let's
 say that for example, what do we do? Let's say that we forgot to apply this fan in normalization.
 So the weights inside the linear layers are just a sample from a Gaussian in all those stages.
 What happens to our, how do we notice that something's off? Well, the activation plot will
 tell you whoa, your neurons are way too saturated. The gradients are going to be almost up.
 The histogram for these weights are going to be all messed up as well. And there's a lot of
 asymmetry. And then if we look here, I suspect it's all going to be also pretty messed up. So
 you see there's a lot of discrepancy in how fast these layers are learning. And some of them are
 learning way too fast. So negative 1, negative 1.5, those are very large numbers in terms of this
 ratio. Again, you should be somewhere around negative 3 and not much more about that.
 So this is how miss calibration. So if your neural nets are going to manifest,
 and these kinds of plots here are a good way of bringing those miss calibrations to your attention
 and so you can address them. Okay, so far we've seen that when we have this linear 10H sandwich,
 we can actually precisely calibrate the gains and make the activations, the gradients, and the
 parameters, and the updates all look pretty decent. But it definitely feels a little bit like balancing
 of a pencil on your finger. And that's because this gain has to be very precisely calibrated.
 So now let's introduce Bachelors and the Fix into the mix. Let's see how that
 tops the fix the problem. So here, I'm going to take the Bachelors and 1D class,
 and I'm going to start placing it inside. And as I mentioned before, the standard typical
 place you would place it is between the linear layer. So right after it, but before the non-linearity,
 but people have definitely played with that. And in fact, you can get very similar results,
 even if you place it after the non-linearity. And the other thing that I wanted to mention
 is this totally fine to also place it at the end after the last linear layer and before the
 loss function. So this is potentially fine as well. And in this case, this would be output,
 would be vocab size. Now, because the last layer is a Bachelorm, we would not be changing
 to wait to make the softmax less confident. We'd be changing the gamma. Because gamma, remember,
 in the Bachelorm is the variable that multiplicativeity interacts with the output of that normalization.
 So we can initialize this sandwich now, and we can train. And we can see that the activations
 are going to, of course, look very good. And they are going to necessarily look at because now
 before every single 10H layer, there is a normalization in the Bachelorm. So this is,
 unsurprisingly, all looks pretty good. It's going to be standard deviation of roughly 0.65,
 2% and roughly equal standard deviation throughout the entire layers. So everything looks very
 homogeneous. The gradients look good, the weights look good, and their distributions. And then the
 updates also look pretty reasonable. We're going above negative three a little bit, but not by too
 much. So all the parameters are training in roughly the same rate here. But now what we've gained is
 we are going to be slightly less brittle with respect to the gain of these. So for example,
 I can make the gain be say 0.2 here, which is much slower than what we had with the 10H.
 But as we'll see, the activations will actually be exactly unaffected. And that's because of,
 again, this explicit normalization. The gradients are going to look okay. The weight gradients are
 going to look okay. But actually the updates will change. And so even though the forward and backward
 pass to a very large extent look okay, because of the backward pass of the batch norm and how the
 scale of the incoming activations interacts in the batch norm and its backward pass, this is
 actually changing the scale of the updates on these parameters. So the gradients of these weights
 are affected. So we still don't get it completely free pass to pass in arbitrary weights here.
 But everything else is significantly more robust in terms of the forward, backward,
 and the weight gradients. It's just that you may have to retune your learning rate. If you are
 changing sufficiently the scale of the activations that are coming into the batch norms. So here,
 for example, we changed the gains of these linear layers to be greater. And we're seeing that the
 updates are coming out lower as a result. And then finally, we can also, if we are using batch
 norms, we don't actually need to necessarily let me reset this to one. So there's no gain.
 We don't necessarily even have to normalize back then in sometimes. So if I take out the
 fan in, so these are just now a random Gaussian, we'll see that because of batch
 norm, this will actually be relatively well behaved. So this is of course in the forward pass look
 good. The gradients look good. The backward weight updates look okay, a little bit of fat
 tails on some of the layers. And this looks okay as well. But as you as you can see,
 we're significantly below negative three. So we'd have to bump up the learning rate of this
 batch norm so that we are training more properly. And in particular, looking at this roughly looks
 like we have to 10x the learning rate to get to about one in a degree three. So we come here and
 we would change this to be update of 1.0. And if I re-emphasize, then we'll see that everything
 still of course looks good. And now we are roughly here and we expect this to be an okay training
 run. So long story short, we are significantly more robust to the gain of these linear layers,
 whether or not we have to apply the fan in. And then we can change the gain. But we actually do
 have to worry a little bit about the update scales and making sure that the learning rate is
 properly calibrated here. But the activations of the forward backward pass and the updates
 are all are looking significantly more well behaved, except for the global scale that is
 potentially being adjusted here. Okay, so now let me summarize. There are three things I was
 hoping to achieve with this section. Number one, I wanted to introduce you to Bachelors normalization,
 which is one of the first modern innovations that we're looking into that helped stabilize
 very deep neural networks and their training. And I hope you understand how the Bachelors
 normalization works and how it would be used in your own network. Number two, I was hoping to
 pytorchify some of our code and wrap it up into these modules. So like linear,
 basharm on the 10 age, etc. These are layers or modules and they can be stacked up into neural
 nets like Lego building blocks. And these layers actually exist in pytorch. And if you import
 torch and then then you can actually do way I've constructed it, you can simply just use pytorch
 by prepending an end dot to all these different layers. And actually everything will just work
 because the API that I've developed here is identical to the API that pytorch uses.
 And the implementation also is basically as far as I'm aware, identical to the one in pytorch.
 And number three, I tried to introduce you to the diagnostic tools that you would use
 to understand whether your neural network is in a good state dynamically. So we are looking at the
 statistics and histograms and activation of the forward pass activation activations,
 the backward pass gradients. And then also we're looking at the weights that are going to be
 updated as part of stochastic gradient ascent. And we're looking at their means, standard deviations,
 and also the ratio of gradients to data, or even better, the updates to data. And we saw that
 typically we don't actually look at it as a single snapshot frozen in time at some particular
 iteration. Typically, people look at this as over time, just like I've done here. And they look at
 these update to data ratios and they make sure everything looks okay. And in particular, I said
 that one in negative three, or basically negative three on the log scale is a good,
 rough heuristic for what you want this ratio to be. And if it's way too high, then probably the
 learning rate or the updates are a little too big. And if it's way too small, that the learning
 rate is probably too small. So that's just some of the things that you may want to play with when
 you try to get your neural network to work very well. Now, there's a number of things I did not
 try to achieve. I did not try to beat our previous performance as an example by introducing the
 Bachelorette layer. Actually, I did try. And I found that I used the learning rate finding
 mechanism that I've described before. I tried to train the Bachelorette layer, a Bachelorette
 and I actually ended up with results that are very, very similar to what we've obtained before.
 And that's because our performance now is not bottlenecked by the optimization,
 which is what Bachelorette is helping with. The performance at the stage is bottlenecked by
 what I suspect is the context length of our context. So currently, we are taking three
 characters to predict the fourth one. And I think we need to go beyond that. And we need to look
 at more powerful architectures, like recurrent neural networks and transformers in order to further
 push the log probabilities that we're achieving on this dataset. And I also did not try to have a
 full explanation of all of these activations, the gradients and the backward pass and the
 statistics of all these gradients. And so you may have found some of the parts here unintuitive,
 and maybe you're slightly confused about, okay, if I change the gain here, how come that we need
 a different learning rate? And I didn't go into the full detail because you'd have to actually look
 at the backward pass of all these different layers and get an intuitive understanding of how all that
 works. And I did not go into that in this lecture. The purpose really was just to
 introduce you to the diagnostic tools and what they look like. But there's still a lot of work
 remaining on the intuitive level to understand the initialization, the backward pass and how all
 that interacts. But you shouldn't feel too bad because honestly, we are getting to the cutting edge
 of where the field is. We certainly haven't, I would say, solved initialization. And we haven't
 solved back propagation. And these are still very much an active area of research. People are still
 trying to figure out where's the best way to initialize these networks? What is the best update
 rule to use? And so on. So none of this is really solved. And we don't really have all the answers to
 all the, to, you know, all these cases. But at least, you know, we're making progress. And at least we
 have some tools to tell us whether or not things are on the right track for now. So I think we've
 We've made positive progress in this lecture,
 and I hope you enjoyed that, and I will see you next time.
 Hi everyone. So today we are once again continuing our implementation of make more.
 Now so far we've come up to here multilial perceptrons and our neural net looked like this
 and we were implementing this over the last few lectures. Now I'm sure everyone is very excited to
 go into recurrent neural networks and all of their variants and how they work and the diagrams look
 cool and it's very exciting and interesting and we're going to get a better result. But unfortunately
 I think we have to remain here for one more lecture and the reason for that is we've already
 trained this multilial perceptron right and we are getting pretty good loss and I think we have a
 pretty decent understanding of the architecture and how it works but the line of code here that I
 take an issue with is here, lost up backward. That is we are taking a pytorch autograph and using
 it to calculate all of our gradients along the way and I would like to remove the use of lost up
 backward and I would like us to write our backward pass manually on the level of tensors and I think
 that this is a very useful exercise for the following reasons. I actually have an entire blog post on
 this topic but I like to call backpropagation a leaky abstraction and what I mean by that is
 backpropagation doesn't just make your neural networks just work magically. It's not the case
 that you can just stack up arbitrary LEGO blocks of differentiable functions and just cross your
 fingers and back propagate and everything is great. Things don't just work automatically. It is a
 leaky abstraction in the sense that you can shoot yourself in a foot if you do not understanding
 its internals. It will magically not work or not work optimally and you will need to understand
 how it works under the hood if you're hoping to debug it and if you are hoping to address it in
 your neural nut. So this blog post here from a while ago goes into some of those examples.
 So for example we've already covered them some of them already. For example the flat tails of
 these functions and how you do not want to saturate them too much because your gradients will die.
 The case of dead neurons which I've already covered as well. The case of exploding or vanishing
 gradients in the case of a current neural networks which we are about to cover. And then also you
 will often come across some examples in the wild. This is a snippet that I found in a random code
 based on the internet where they actually have like a very subtle but pretty major bug in their
 implementation. And the bug points at the fact that the author of this code does not actually
 understand back propagation. So what they're trying to do here is they're trying to clip the loss
 at a certain maximum value. But actually what they're trying to do is they're trying to clip
 the gradients to have a maximum value instead of trying to clip the loss at a maximum value.
 And indirectly they're basically causing some of the outliers to be actually ignored. Because
 when you clip a loss of an outlier you are setting its gradient to zero. And so have a look through
 this and read through it. But there's basically a bunch of subtle issues that you're going to
 avoid if you actually know what you're doing. And that's why I don't think it's the case that
 because PyTorch or other frameworks offer autograd it is okay for us to ignore how it works.
 Now we've actually already covered autograd and we wrote micrograd. But micrograd was an autograd
 engine only on the level of individual scalars. So the atoms were single individual numbers.
 And you know I don't think it's enough and I'd like us to basically think about back propagation
 on level of tensors as well. And so in a summary I think it's a good exercise. I think it is very
 very valuable. You're going to become better at debugging neural networks and making sure that
 you understand what you're doing. It is going to make everything fully explicit so you're not going
 to be nervous about what is hidden away from you. And basically in general we're going to emerge
 stronger. And so let's get into it. A bit of a fun historical note here is that today
 writing your backward pass by hand and manually is not recommended and no one does it except for
 the purposes of exercise. But about 10 years ago in deep learning this was fairly standard and in
 fact pervasive. So at the time everyone used to write their backward pass by hand manually including
 myself. And it's just what you would do. So we used to write backward pass by hand and now everyone
 just called lost a backward. We've lost something. I wanted to give you a few examples of this.
 So here's a 2006 paper from Jeff Hinton and Russell Select Enough in science that was influential
 at the time. And this was training some architectures called restricted bulletin machines. And basically
 it's an auto encoder trained here. And this is from roughly 2010. I had a library for training
 restrictable machines. And this was at the time written in MATLAB. So Python was not used for
 deep learning pervasively. It was all MATLAB. And MATLAB was this scientific computing package
 that everyone would use. So we would write MATLAB, which is barely a programming language
 in a big as well. But it had a very convenient tensor class. And was this a computing environment
 and you would run here, it would all run on the CPU of course, but you would have very nice plots
 to go with it and a built in debugger. And it was pretty nice. Now the code in this package in 2010
 that I wrote for fitting researchable machines to a large extent is recognizable. But I wanted
 to show you how you would well, I'm creating the data and the XY batches. I'm initializing the
 neural nut. So it's got weights and biases just like we're used to. And then this is the training
 loop where we actually do the forward pass. And then here, at this time, they didn't even necessarily
 use back propagation to train neural networks. So this in particular implements contrastive
 divergence, which estimates a gradient. And then here we take that gradient and use it for a
 parameter update along lines that we're used to. Yeah, here. But you can see that basically people
 are meddling with these gradients directly and inline and themselves. It wasn't that common to
 use an autograd engine. Here's one more example from a paper of mine from 2014 called the Fragment
 Embeddings. And here what I was doing is I was aligning images and text. And so it's kind of like
 clip if you're familiar with it. But instead of working on the level of entire images and entire
 sentences, it was working on the level of individual objects and little pieces of sentences. And I
 was embedding them and then calculating a very much like a clip like loss. And I dug up the code
 from 2014 of how I implemented this. And it was already in numpy and Python. And here I'm implementing
 the cost function. And it was standards to implement not just the cost, but also the backward pass
 manually. So here I'm calculating the image embeddings, sentence embeddings, the last function
 I calculate this course. This is the last function. And then once I have the last function, I do the
 backward pass right here. So I backward through the loss function and through the neural net. And
 I have had regularization. So everything was done by hand manually. And you're just right
 out the backward pass. And then you would use a gradient checker to make sure that your numerical
 estimate of the gradient agrees with the one you calculated during back propagation. So this was
 very standard for a long time. But today, of course, it is standard to use an autograph engine.
 But it was definitely useful. And I think people sort of understood how these neural networks work
 on a very intuitive level. And so I think it's a good exercise again. And this is where we want to
 be. Okay, so just as a reminder from our previous lecture, this is the Jupyter Notebook that we
 implemented at the time. And we're going to keep everything the same. So we're still going to have
 a two layer multialing perception with a batch normalization layer. So the forward pass will be
 basically identical to this lecture. But here we're going to get rid of lost backward. And instead,
 we're going to write the backward pass manually. Now here's the starter code for this lecture.
 We are becoming a back prop ninja in this notebook. And the first few cells here are identical to
 what we are used to. So we are doing some imports loading the data set and processing the data set.
 None of this changed. Now here, I'm introducing a utility function that we're going to use later
 to compare the gradients. So in particular, we are going to have the gradients that we estimate
 manually ourselves. And we're going to have gradients that PyTorch calculates. And we're going to be
 checking for correctness, assuming of course that PyTorch is correct. Then here, we have the
 initialization that we are quite used to. So we have our embedding table for the characters,
 the first layer, second layer, and a batch normalization in between. And here's where we create all the
 parameters. Now you will note that I changed the initialization a little bit to be small numbers.
 So normally you would set the biases to be all zero. Here I am setting them to be small random
 numbers. And I'm doing this because if your variables are initialized to exactly zero,
 sometimes what can happen is that can mask an incorrect implementation of a gradient.
 Because when everything is zero, it sort of simplifies and gives you a much simpler expression
 of the gradient than you would otherwise get. And so by making it small numbers, I'm trying to
 unmask those potential errors in these calculations. You also notice that I'm using B1 in the first
 layer. I'm using a bias despite batch normalization right afterwards. So this would typically not be
 what you do because we talked about the fact that you don't need a bias. But I'm doing this here
 just for fun, because we're going to have a gradient with respect to it. And we can check that we are
 still calculating it correctly, even though this bias is asparious. So here I'm calculating a single
 batch. And then here I am doing a forward pass. Now you'll notice that the forward pass is significantly
 expanded from what we are used to. Here the forward pass was just here. Now the reason that
 the forward pass is longer is for two reasons. Number one here, we just had an F dot cross entropy.
 But here I am bringing back a explicit implementation, the loss function. And number two, I broke
 up the implementation into manageable trunks. So we have a lot, a lot more intermediate tensors
 along the way in the forward pass. And that's because we are about to go backwards and calculate
 the gradients in this back propagation from the bottom to the top. So we're going to go upwards.
 And just like we have, for example, the lock props tensor in a forward pass, in a backward pass,
 we're going to have a D lock props, which is going to store the derivative of the loss with
 respect to the lock props tensor. And so we're going to be pretending D to every one of these
 tensors and calculating it along the way of this back propagation. So as an example, we have a B
 and raw here, we're going to be calculating a DB and raw. So here I'm telling PyTorch that we
 want to retain the grad of all these intermediate values. Because here in exercise one, we're going
 to calculate the backward pass. So we're going to calculate all these D variables and use the CMP
 function I've introduced above to check our correctness with respect to what PyTorch is telling
 us. This is going to be exercise one, where we sort of back propagate through this entire graph.
 Now, just to give you a very quick preview of what's going to happen in exercise two and below,
 here we have fully broken up the loss and back propagated through it manually in all the little
 atomic pieces that make it up. But here we're going to collapse the loss into a single cross
 entropy call. And instead we're going to analytically derive using math and paper and pencil, the gradient
 of the loss with respect to the logits. And instead of back propagating through all of its little chunks
 one at a time, we're just going to analytically drive what that gradient is. And we're going to
 implement that, which is much more efficient, as we'll see in a bit. Then we're going to do the
 exact same thing for batch normalization. So instead of breaking up batch room into all the little
 tiny components, we're going to use a pen and paper and mathematics and calculus to derive the
 gradient through the batch, batch room layer. So we're going to calculate the backward pass
 through a batch room layer in a much more efficient expression, instead of backward propagating
 through all of its little pieces independently. So it's going to be exercise three. And then
 exercise four, we're going to put it all together. And this is the full code of training this two
 layer MLP. And we're going to basically insert our manual backdrop. We're going to take out
 loss the backward. And you will basically see that you can get all the same results using fully
 your own code. And the only thing we're using from PyTorch is the torch.tensor to make the
 calculations efficient. But otherwise, you will understand fully what it means to forward and
 backward in your land and train it. And I think that'll be awesome. So let's get to it.
 Okay, so I ran all the cells of this notebook all the way up to here. And I'm going to erase this,
 and I'm going to start implementing backward pass, starting with D log problems. So we want to
 understand what should go here to calculate the gradient of the loss with respect to all the
 elements of the log props tensor. Now I'm going to give away the answer here, but I wanted to put
 a quick note here that I think would be most pedagogically useful for you is to actually go into the
 description of this video and find the link to this stupid notebook. You can find it both on
 GitHub, but you can also find Google collab with it. So you don't have to install anything. You'll
 just go to a website on Google collab. And you can try to implement these derivatives or gradients
 yourself. And then if you are not able to come to my video and see me do it. And so work in tandem
 and try to first yourself and then see me give away the answer. And I think that would be most
 valuable to you. And that's how I recommend you go through this lecture. So we are starting here
 with D log props. Now D log props will hold the derivative of the loss with respect to all the
 elements of log props. What is inside log props? The shape of this is 32 by 27. So it's not going to
 surprise you that D log props should also be an array of size 32 by 27. Because we want the derivative
 of the loss with respect to all of its elements. So the sizes of those are always going to be equal.
 Now, how does log props influence the loss? Okay, loss is negative log props indexed with range of N
 and YB and then the mean of that. Now, just as a reminder, YB is just basically an array of all the
 correct indices. So what we're doing here is we're taking the log props array of size 32 by 27.
 Right. And then we are going every single row. And in each row, we are plugging,
 plugging out the index eight and then 14 and 15 and so on. So we're going down the rows. That's
 the iterator range of N. And then we are always plugging out the index at the column specified
 by this tensor YB. So in the zero throw, we are taking the eighth column. In the first row,
 we're taking the 14th column, etc. And so log props at this plucks out all those
 log probabilities of the correct next character in a sequence. So that's what that does. And the
 shape of this or the size of it is of course 32, because our batch size is 32. So these elements
 get plucked out and then their mean and the negative of that becomes loss. So I always like
 to work with simpler examples to understand the numerical form of derivative. What's going on here
 is once we've plucked out these examples, we're taking the mean and then the negative. So the loss
 basically, I can write it this way is the negative of say a plus b plus c. And the mean of those
 three numbers would be say, negative with divide three, that would be how we achieve the mean of
 three numbers a, b, c, although we actually have 32 numbers here. And so what is basically the loss
 by say like dA, right? Well, if we simplify this expression mathematically, this is negative 1
 over 3 of a and negative plus negative 1 over 3 of b plus negative 1 over 3 of c. And so what
 is the loss by dA? It's just negative 1 over 3. And so you can see that if we don't just have a,
 b and c, but we have 32 numbers, then d loss by d, you know, every one of those numbers is going to
 be 1 over n more generally, because n is the size of the batch 32 in this case. So d loss by d
 lockprobs is negative 1 over n in all these places. Now what about the other elements inside
 lockprobs? Because lockprobs is a large array. You see that lockprobs are checked is 32 by 27,
 but only 32 of them participate in the loss calculation. So what's the derivative of all the
 other most of the elements that do not get plucked out here? Well, their loss intuitively is zero,
 sorry, their gradient intuitively is zero. And that's because they did not participate in the loss.
 So most of these numbers inside this tensor does not feed into the loss. And so if we were to
 change these numbers, then the loss doesn't change, which is the equivalent of way of saying that
 the rate of the loss with respect to them is zero, they don't impact it. So here's a way to
 implement this derivative then we start out with torched at zeros of shape 32 by 27. Or let's just
 say, instead of doing this, because we don't want to hard code numbers, let's do torched at zeros
 like lockprobs. So basically, this is going to create an array of zeros exactly in the shape of
 lockprobs. And then we need to set the derivative negative one over n inside exactly these locations.
 So here's what we can do. The lockprobs indexed in the identical way will be just set to negative
 one over zero, divide n, right, just like we derived here. So now let me erase all these reasoning.
 And then this is the candidate derivative for delockprobs. Let's uncomment the first line and
 check that this is correct. Okay, so CMP ran and let's go back to CMP. And you see that what
 is doing is it's calculating if the calculated value by us, which is dt, is exactly equal to t
 dot grad as calculated by pytorch. And then this is making sure that all the elements are exactly
 equal, and then converting this to a single Boolean value, because we don't want to Boolean tensor,
 we just want to Boolean value. And then here, we are making sure that, okay, if they're not exactly
 equal, maybe they are approximately equal because of some floating point issues, but they're very,
 very close. So here we are using torched at all close, which has a little bit of a wiggle available,
 because sometimes you can get very, very close. But if you use a slightly different calculation,
 because of floating point arithmetic, you can get a slightly different result. So this is
 checking if you get an approximately close result. And then here we are checking the maximum,
 basically the value that has the highest difference, and what is the difference,
 and the absolute value difference between those two. And so we are printing whether we have an
 exact equality, an approximate equality, and what is the largest difference. And so here,
 we see that we actually have exact equality. And so therefore, of course, we also have an
 approximate equality. And the maximum difference is exactly zero. So basically, our delog props is
 exactly equal to what pytorch calculated to be log props dot grad in its back propagation.
 So so far, we're doing pretty well. Okay, so let's now continue our back propagation.
 We have that log props depends on props through a log. So all the elements of props are being
 element wise applied log two. Now, if we want deep props, then then remember your micro graph training,
 we have like a log node, it takes in props and creates log props. And deep props will be the
 local derivative of that individual operation log times the derivative loss with respect to its
 output, which in this case is D log props. So what is the local derivative of this operation?
 Well, we are taking log element wise, and we can come here and we can see, well, from all
 files, your friend, that D by DX of log of X is just simply one of our X. So therefore,
 in this case, X is problems. So we have D by DX is one over X, which is one of our props.
 And then this is the local derivative. And then times we want to train it. So this is chain rule
 times the log props. Then let me uncomment this and let me run the cell in place. And we see that
 the derivative of props as we calculated here is exactly correct. And so notice here how this works.
 Props that are props is going to be inverted and then element wise multiplied here. So if your
 props is very, very close to one, that means your network is currently predicting the character
 correctly, then this will become one over one and the log props is just passed through.
 But if your probabilities are incorrectly assigned, so if the correct character here
 is getting a very low probability, then 1.0 dividing by it will boost this and then multiply by the
 log props. So basically, what this line is doing intuitively is it's taking the examples that
 have a very low probability currently assigned and it's boosting their gradient. You can again,
 look at it that way. Next up is count some imp. So we want the river of this. Now, let me just pause
 here and kind of introduce what's happening here in general, because I know it's a little bit confusing.
 We have the logis that come out of the neural net. Here, what I'm doing is I'm finding the
 maximum in each row, and I'm subtracting it for the purpose of numerical stability. And we talked
 about how if you do not do this, you run numerical issues of some of the logits take on two large
 values, because we end up exponentiating them. So this is done just for safety numerically.
 Then here's the exponentiation of all the sort of like logits to create our counts.
 And then we want to take the sum of these counts and normalize so that all of the props sum to one.
 Now here, instead of using one over counts sum, I use raise to the power of negative one.
 Mathematically, they are identical. I just found that there's something wrong with the
 pytorch implementation of the backward pass of division. And it gives like a real result,
 but that doesn't happen for star star negative one. So I'm using this formula instead. But basically,
 all that's happening here is we got the logits, we want to exponentiate all of them, and we want
 to normalize the counts to create our probabilities. It's just that it's happening across multiple
 lines. So now here, we want to first take the derivative, we want to back propagate into counts
 and then into counts as well. So what should be the count sum? Now we actually have to be careful
 here, because we have to scrutinize and be careful with the shapes. So counts that shape,
 and then counts some in that shape are different. So in particular, counts is 32 by 27, but this
 counts some is 32 by one. And so in this multiplication here, we also have an implicit broadcasting
 that pytorch will do, because it needs to take this column tensor of 32 numbers and replicate it
 horizontally 27 times to align these two tensors. So we can do an element twice multiply. So really,
 what this looks like is the following using a toy example again. What we really have here is just
 props is counts times consummive. So it's a equals a times B. But a is three by three, and b is just
 three by one, a column tensor. And so pytorch internally replicated this elements of B, and it did that
 across all the columns. So for example, B one, which is the first element of B, would be replicated
 here across all the columns in this multiplication. And now we're trying to back propagate through
 this operation to count some in. So when we are calculating this derivative, it's important to
 realize that these two, this looks like a single operation, but actually is two operations applied
 sequentially. The first operation that I touched it is it took this column tensor and replicated it
 across all the, across all the columns basically 27 times. So that's the first operation, it's a
 replication. And then the second operation is the multiplication. So let's first background
 through the multiplication. If these two arrays were of the same size, and we just have a and b
 both of them three by three, then how do we, how do we back propagate through a multiplication?
 So if you just have scalars and not tensors, then if you have sequels a times b, then what is the
 root of the of c with respect to b? Well, it's just a. And so that's the local derivative.
 So here in our case, undoing the multiplication and back propagate through just multiplication
 itself, which is element wise, is going to be the local derivative, which in this case is simply
 counts, because counts is the a. So it's the local derivative, and then times, because the
 chain rule deprops. So this here is the derivative or the gradient, but with respect to replicated
 b. But we don't have a replicated b, we just have a single b column. So how do we now back propagate
 through the replication? And intuitively, this b one is the same variable, and it's just reused
 multiple times. And so you can look at it as being equivalent to a case wave encountered in
 micro grad. And so here, I'm just pulling out a random graph we used in micro grad. We had an
 example where a single node has its output feeding into two branches of basically the graph until
 the loss function. And we're talking about how the correct thing to do in the backward pass is,
 we need to sum all the gradients that arrive at any one node. So across these different branches,
 the gradients would sum. So if a note is used multiple times, the gradients for all of its uses,
 sum during back propagation. So here, b one is used multiple times in all these columns. And
 therefore, the right thing to do here is to sum horizontally across all the rows. So we'll just
 sum in dimension one, but we want to retain this dimension so that the, so that counts them in,
 and its gradient are going to be exactly the same shape. So we want to make sure that we keep them
 as true. So we don't lose this dimension. And this will make the count some m be exactly shape 32 by
 one. So revealing this comparison as well and running this, we see that we get an exact match.
 So this derivative is exactly correct. And let me erase this. Now let's also back propagate into
 counts, which is the other variable here to create props. So from props to count some
 in, we just did that, let's go into counts as well. So the counts will be
 the counts are a. So dc by da is just b. So therefore it's count some in. And then times
 chain rule, the props. Now, councilman is three to my one. D props is 32 by 27. So
 those will broadcast fine and will give us decounts. There's no additional summation required here.
 There will be a broadcasting that happens in this multiply here, because councilman needs to be
 replicated again to correctly multiply d props. But that's going to give the correct result.
 So as far as the single operation is concerned, so we back propagate from props to counts,
 but we can't actually check the derivative of counts. I have it much later on. And the reason for that
 is because count some in depends on counts. And so there's a second branch here that we have to finish
 because councilman back propagates into count some and councilman will back propagate into counts.
 And so counts is a node that is being used twice. It's used right here into props and it goes through
 this other branch through councilman. So even though we've calculated the first contribution of it,
 we still have to calculate the second contribution of it later. Okay, so we're continuing with this
 branch. We have the derivative for councilman. Now we want the derivative counts some. So decount
 sum equals what is the local derivative of this operation? So this is basically an element
 wise one over counts some. So count sum raised to the power of negative one is the same as
 one over councilman. If we go to all from alpha, we see that x is negative one d by d by d x of it
 is basically negative x to the negative two. Right one negative one over square is the same as
 negative x to the negative two. So decount sum here will be local derivative is going to be negative
 counts sum to the negative two. That's the local derivative times chain rule, which is
 decount sum in. So that's decount sum. Let's uncomment this and check that I am correct.
 Okay, so we have perfect equality. And there's no sketching us going on here with any shapes
 because these are of the same shape. Okay, next up we want to back propagate through this line.
 We have that count sum is counts that sum along the rows. So I wrote out some help here.
 We have to keep in mind that counts of course is 32 by 27 and count sum is 32 by one. So in
 this back propagation, we need to take this column of the rudis and transform it into a
 array of the roots to the machine array. So what is this operation doing? We're taking
 in some kind of an input like say a three by three matrix A and we are summing up the rows into
 column tensor B, B1 B2 B3. That is basically this. So now we have the derivatives of the loss with
 respect to B, all the elements of B. And now we want to derivative loss with respect to all these
 little a's. So how do the b's depend on the a's is basically what we're after. What is the local
 derivative of this operation? Well, we can see here that B1 only depends on these elements here.
 The derivative of B1 with respect to all of these elements down here is zero. But for these
 elements here like a 1 1 a 1 2 etc, the local derivative is one, right? So dB1 by dA1 1,
 for example is one. So it's one one and one. So when we have the derivative loss with respect to B1,
 the local derivative of B1 with respect to these inputs is zeros here, but it's one on these guys.
 So in the chain rule, we have the local derivative times sort of the derivative of B1. And so because
 the local derivative is one on these three elements, the local derivative of multiplying the derivative
 of B1 will just be the derivative of B1. And so you can look at it as a router. Basically an
 addition is a router of gradient. Whatever gradient comes from above, it just gets routed equally
 to all the elements that participate in that addition. So in this case, the derivative of B1
 will just flow equally to the derivative of a 1 1, a 1 2 and a 1 3. So if we have a derivative
 of all the elements of B in this column tensor, which is d counts some that we've calculated just now,
 we basically see that what that amounts to is all of these are now flowing to all these elements of A,
 and they're doing that horizontally. So basically what we want is we want to take the
 d counts sum of size 32 by 1, and we just want to replicate it 27 times horizontally to create 32
 by 27 array. So there's many ways to implement this operation. You could of course just replicate
 the tensor. But I think maybe one clean one is that d counts is simply torch dot once like
 so just a two dimensional arrays of ones in the shape of counts. So 32 by 27 times d counts sum.
 So this way we're letting the broadcasting here basically implement the replication. You can look
 at it that way. But then we have to also be careful because d counts was all already calculated.
 We calculated earlier here, and that was just the first branch and we're now finishing the second
 branch. So we need to make sure that these gradients add so plus equals. And then here,
 let's comment out the comparison. And let's make sure crossing fingers that we have the correct
 result. So pytorch agrees with us on this gradient as well. Okay, hopefully we're getting a hang of
 this now counts as an element wise exp of norm logits. So now we want d norm logits.
 And because it's an element has operation, everything is very simple. What is the local
 derivative of e to the x? It's famously just e to the x. So this is the local derivative.
 That is the local derivative. Now we already calculated it and it's inside counts. So we
 made as well potentially just reuse counts. That is the local derivative times d counts.
 Funny as that looks, counts times d counts is the derivative on the norm logits. And now let's
 erase this and let's verify and it looks good. So that's norm logits. Okay, so we are here on this
 line now, d norm logits. We have that and we're trying to calculate the logits and the logit
 maxes. So back propagating through this line. Now we have to be careful here because the shapes
 again are not the same. And so there's an implicit broadcasting happening here. So normal
 just has this shape 32 by 27, logits does as well, but logit maxes is only 32 by one. So there's a
 broadcasting here in the minus. Now here I tried to sort of write out a toy example again. We
 basically have that this is our C equals a minus b. And we see that because of the shape, these
 are three by three, but this one is just a column. And so for example, every element of C, we have
 to look at how it came to be. And every element of C is just the corresponding element of a minus
 basically that associated B. So it's very clear now that the derivatives of every one of these
 C's with respect to their inputs are one for the corresponding a and it's a negative one for the
 corresponding B. And so therefore, the derivatives on the C will flow equally to the corresponding
 A's. And then also to the corresponding B's. But then in addition to that, the B's are broadcast.
 So we'll have to do the additional sum just like we did before. And of course, derivatives for B's
 will undergo a minus because the local derivative here is negative one. So the C 32 by D B3 is negative
 one. So let's just implement that. Basically, D logits will be exactly copying the derivative
 on normal digits. So D logits equals D normal logits, and I'll do a dot clone for safety. So
 we're just making a copy. And then we have that D logit maxis will be the negative of D normal
 logits, because of the negative sign. And then we have to be careful because logit maxis is a column.
 And so just like we saw before, because we keep replicating the same elements across all the
 columns, then in the backward pass, because we keep reusing this, these are all just like separate
 branches of use of that one variable. And so therefore, we have to do a sum along one would
 keep them equals true, so that we don't destroy this dimension. And then the logit maxis will be the
 same shape. Now we have to be careful because this D logits is not the final D logits. And that's
 because not only do we get gradient signal into logits through here, but the logit maxis is a
 function of logits. And that's a second branch into logits. So this is not yet our final derivative
 for logits, we will come back later for the second branch. For now, the logit maxis is the final
 derivative. So let me uncomment this CMP here, and let's just run this. And logit maxis hit by torch
 agrees with us. So that was the derivative into through this line. Now, before we move on, I want
 to pause here briefly, and I want to look at these logit maxis and especially their gradients.
 We've talked previously in the previous route lecture that the only reason we're doing this
 is for the numerical stability of the softmax that we are implementing here. And we talked about
 how if you take these logits for any one of these examples, so one row of this logits tensor,
 if you add or subtract any value equally to all the elements, then the value of the probes will be
 unchanged. You're not changing the softmax. The only thing that this is doing is it's making sure
 that X doesn't overflow. And the reason we're using a max is because then we are guaranteed
 that each row of logits, the highest number is zero. And so this will be safe. And so basically what
 that has repercussions. If it is the case that changing logit maxis does not change the props,
 and therefore there's not change the loss, then the gradient on logit maxis should be zero,
 right? Because saying those two things is the same. So indeed, we hope that this is very,
 very small numbers. And you'd be hope this is zero. Now, because of floating point sort of
 wonkiness, there's doesn't come out exactly zero, only in some of the rows it does. But we get
 extremely small values like one in negative nine or 10. And so this is telling us that the values
 of logit maxis are not impacting the loss as they shouldn't. It feels kind of weird to back
 propagate through this branch, honestly, because if you have any implementation of like f dot cross
 entropy and pytorch and you you block together all these elements and you're not doing the back
 propagation piece by piece, then you would probably assume that the derivative through here is exactly
 zero. So you would be sort of skipping this branch because it's only done for numerical stability.
 But it's interesting to see that even if you break up everything into the full atoms and you still
 do the computation as you'd like with respect to numerical stability, the correct thing happens.
 And you still get a very, very small gradients here, basically reflecting the fact that the
 values of these do not matter with respect to the final loss. Okay, so let's now continue
 back propagation through this line here. We've just calculated the logit maxis and now we want
 to back prop into logits through this second branch. Now here, of course, we took logits and we took
 the max along all the rows. And then we looked at its values here. Now, the way this works is that
 in pytorch, this thing here, the max returns both the values and it returns the indices at which
 those values to call the maximum value. Now, in the forward pass, we only used values because
 that's all we needed. But in the backward pass, it's extremely useful to know about where those
 maximum values occurred. And we have the indices at which they occurred. And this will of course
 helps us to help us do the back propagation, because what should the backward pass be here in this case?
 We have the logis tensor, which is 32 by 27. And in each row, we find the maximum value.
 And then that value gets plucked out into logit maxis. And so intuitively, basically,
 the derivative flowing through here then should be one times the local derivative is one for the
 appropriate entry that was plucked out. And then times the global derivative of the logit maxis.
 So really what we're doing here, if you think through it, is we need to take the delogit maxis,
 and we need to scatter it to the correct positions in these logits from where the maximum values came.
 And so I came up with one line of code, sort of that does that. Let me just erase a bunch of stuff
 here. So the line of, you could do it kind of very similar to what we've done here, where we create
 a zeros, and then we populate the correct elements. So we use the indices here, and we would set them
 to be one. But you can also use one hat. So at that one hat, and then I'm taking the logis that max
 over the first dimension that indices. And I'm telling PyTorch that the
 dimension of every one of these tensors should be 27. And so what this is going to do is, okay,
 I apologize, this is crazy. Be guilty that I am sure of this. It's really just an array of
 where the max is came from in each row, and that element is one, and the other elements are zero.
 So it's a one-hat vector in each row, and these indices are now populating a single one in the
 proper place. And then what I'm doing here is I'm multiplying by the logit maxis. And keep in mind
 that this is a column of 32 by one. And so when I'm doing this times the logit maxis, the logit maxis
 will broadcast, and that column will get replicated, and then the element-wise multiply will ensure
 that each of these just gets routed to whichever one of these bits is turned on. And so that's
 another way to implement this kind of an operation. And both of these can be used. I just thought I
 would show an equivalent way to do it. And I'm using plus equals because we already calculated
 the logits here, and this is now the second branch. So let's look at logits and make sure that this
 is correct. And we see that we have exactly the correct answer. Next up, we want to continue with
 logits here. That is an outcome of a matrix multiplication and a bias offset in this linear
 layer. So I've printed out the shapes of all these intermediate tensors. We see that logits is of
 course 32 by 27, as we've just seen. Then the age here is 32 by 64. So these are 64-dimensional
 hidden states. And then this w matrix projects those 64-dimensional vectors into 27 dimensions.
 And then there's a 27-dimensional offset, which is a one-dimensional vector. Now we should note
 that this plus here actually broadcasts because h multiplied by w2 will give us a 32 by 27.
 And so then this plus b2 is a 27-dimensional vector here. Now in the rules of broadcasting,
 what's going to happen with this bias vector is that this one-dimensional vector of 27 will get
 aligned with an padded dimension of 1 on the left. And it will basically become a row vector,
 and then it will get replicated vertically 32 times to make it 32 by 27. And then there's
 an element bias multiply. Now the question is how do we back propagate from logits to the hidden
 states, the weight matrix of w2 and the bias b2. And you might think that we need to go to
 some matrix calculus. And then we have to look up the derivative for a matrix multiplication.
 But actually you don't have to do any of that. And you can go back to first principles and derive
 this yourself on a piece of paper. And specifically what I like to do and what I find works well for
 me is you find a specific small example that you then fully write out. And then in the process of
 analyzing how that individual small example works, you will understand a broader pattern.
 And you'll be able to generalize and write out the full general formula for how these
 derivatives flow in an expression like this. So let's try that out. So part in the low budget
 production here, but what I've done here is I'm writing it out on a piece of paper. Really what
 we are interested in is we have a multiply b plus c. And that creates a d. And we have the
 derivative of the loss with respect to d. And we'd like to know what the derivative of the loss is
 with respect to a b and c. Now these here are a little two dimensional examples of matrix
 multiplication. Two by two times a two by two, plus a two, a vector of just two elements,
 c one and c two, gives me a two by two. Now notice here that I have a bias vector here called c.
 And the bias vector c one and c two. But as I described over here, that bias vector will become
 a row vector in the broadcasting and will replicate vertically. So that's what's happening here as
 well. C one c two is replicated vertically. And we see how we have two rows of C one C two as a result.
 So now when I say write it out, I just mean like this, basically break up this matrix multiplication
 into the actual thing that that's going on under the hood. So as a result of matrix multiplication
 and how it works, d one one is the result of a dot product between the first row of a and the first
 column of B. So a one one b one one plus a one two b two one plus c one. And so on so forth for all
 the other elements of D. And once you actually write it out, it becomes obvious this is just a
 bunch of multiplies and ads. And we know from micro grad how to differentiate multiplies and ads.
 And so this is not scary anymore. It's not just matrix multiplication. It's just tedious
 unfortunately. But this is completely tractable. We have d l by D for all these. And we want d l by
 all these little other variables. So how do we achieve that? And how do we actually get the
 gradients? Okay, so the low budget production continues here. So let's for example derive the
 derivative of the loss with respect to a one one. We see here that a one one occurs twice in our
 simple expression right here, right here. And influence is D one one and D one two. So this is
 so what is D L by D a one one? Well, it's D L by D one one times the local derivative of D one one,
 which in this case is just B one one, because that's what's multiplying a one one here. So,
 and likewise here the local derivative of D one two with respect to a one one is just B one two.
 And so B one two will in the chain rule therefore multiply D L by D one two. And then because a one
 one is used both to produce D one one and D one two, we need to add up the contributions of both
 of those sort of chains that are running in parallel. And that's why we get a plus just adding up those
 two, those two contributions. And that gives us D L by D a one one. We can do the exact same
 analysis for the other one for all the other elements of a. And when you simply write it out,
 it's just super simple taking ingredients on, you know, expressions like this. You find that
 this matrix D L by D a that we're after, right, if we just arrange all of them in the same shape as
 A takes. So A is just two or two matrix. So D L by D a here will be also just the same shape
 tester with the derivatives now. So D L by D a one one, etc. And we see that actually we can
 express what we've written out here as a matrix multiply. And so it just so happens that D L by,
 that all of these formulas that we've derived here by taking gradients can actually be expressed
 as a matrix multiplication. And in particular, we see that it is the matrix multiplication of these
 two matrices. So it is the D L by D. And then matrix multiplying B, but B transpose actually.
 So you see that B two one and B one two have changed place. Whereas before we had, of course,
 B one one B one two, B two one B two two. So you see that this other matrix B is transposed.
 And so basically what we have on story short, just by doing very simple reasoning here,
 by breaking up the expression in the case of a very simple example, is that D L by D a is,
 which is this, is simply equal to D L by D D matrix multiplied with B transpose.
 So that is what we have so far. Now we also want the derivative with respect to B and C.
 Now, for B, I'm not actually doing the full derivation because honestly it's, it's not deep,
 it's just annoying, it's exhausting. You can actually do this analysis yourself.
 You'll also find that if you take this, these expressions and you differentiate with respect
 to B instead of A, you will find that D L by D B is also a matrix multiplication. In this case,
 you have to take the matrix A and transpose it. And matrix multiply that with D L by D D.
 And that's what gives you a deal by D B. And then here for the offsets C one and C two,
 if you again just differentiate with respect to C one, you will find an expression like this.
 And C two and expression like this. And basically you'll find that D L by D C is simply because
 they're just offsetting these expressions. You just have to take the deal by D D matrix
 of the derivatives of D and you just have to sum across the columns. And that gives you the
 derivatives for C. So long story short, the backward pass of a matrix multiply is a matrix multiply.
 And instead of just like we had D equals A times B plus C, in a scalar case, we sort of like
 arrive at something very, very similar, but now with a matrix multiplication instead of a scalar
 multiplication. So the derivative of D with respect to A is D L by D D matrix multiply B
 Trespos. And here it's a transpose multiply D L by D D. But in both cases, matrix multiplication
 with the derivative and the other term in the multiplication. And for C it is a sum.
 Now I'll tell you a secret. I can never remember the formulas that we just arrived for back
 propagate information multiplication. And I can back propagate through these expressions just fine.
 And the reason this works is because the dimensions have to work out. So let me give you an example.
 Say I want to create D H. Then what should D H be number one? I have to know that the shape of D
 H must be the same as the shape of H. And the shape of H is 30 to by 64. And then the other piece of
 information I know is that D H must be some kind of matrix multiplication of D logits with W two.
 And D logits is 32 by 27. And W two is 64 by 27. There is only a single way to make the shape or
 count in this case. And it is indeed the correct result. In particular here, H needs to be 32 by 64.
 The only way to achieve that is to take a D logits and matrix multiply it with you see how I have
 to take W two, but I have to transpose it to make the dimensions work out. So W to transpose.
 And it's the only way to make these two matrix multiply those two pieces to make the shapes
 work out. And that turns out to be the correct formula. So if we come here, we want D H, which is
 D A. And we see that D A is DL by D D matrix multiply B transpose. So that's D logits multiply
 and B is W two. So W two transpose, which is exactly what we have here. So there's no need to
 remember these formulas. Similarly, now if I want D W two, well, I know that it must be a matrix
 multiplication of D logits and H. And maybe there's a few transpose, or there's one transpose in
 there as well. And I don't know which way it is. So I have to come to W two. And I see that it's
 shape is 64 by 27. And that has to come from some matrix multiplication of these two. And so to get
 a 64 by 27, I need to take H, I need to transpose it. And then I need to matrix multiply it. So that
 will become 64 by 32. And then I need to make your small bind with the 32 by 27. And that's going to
 give me a 64 by 27. So I need to make your small, apply this with the logis that shape, just like
 that. That's the only way to make the dimensions work out. And just use matrix multiplication. And
 if we come here, we see that that's exactly what's here. So a transpose, a for us is H,
 multiply with the logis. So that's W two. And then DB two is just the vertical sum. And actually,
 in the same way, there's only one way to make the shapes work out. I don't have to remember that
 it's a vertical sum along the zero of axis, because that's the only way that this makes sense,
 because B two shape is 27. So in order to get a D logits here, it's 32 by 27. So knowing that
 it's just some over D logits in some direction, that direction must be zero, because I need to
 eliminate this dimension. So it's this. So this is, so this kind of like the hacky way. Let me
 copy paste and delete that and let me swing over here. And this is our backward pass for the linear
 layer, hopefully. So now let's uncomment these three. And we're checking that we got all the three
 derivatives correct, and run. And we see that H, W two and B two are all exactly correct. So we
 back propagated through a linear layer. Now next up, we have derivative for the H already,
 and we need to back propagate through 10 H into H preact. So we want to derive D H preact.
 And here we have to back propagate through a 10 H, and we've already done this in micrograds.
 And we remember that 10 H is a very simple backward formula. Now, unfortunately, if I just
 put in D by DX of 10 H of X into Wolfram alpha, it lets us down. It tells us that it's a hyperbolic
 secant function squared of X. It's not exactly helpful. But luckily, Google image search does not
 let us down. And it gives us the simpler formula. And in particular, if you have that A is equal to
 10 H of Z, then D A by D Z, back propagating through 10 H, is just one minus a square. And take note
 that one minus a square A here is the output of the 10 H, not the input to the 10 H Z. So the D A
 by D Z is here formulated in terms of the output of that 10 H. And here also in Google image search,
 we have the full derivation. If you want to actually take the actual definition of 10 H and
 work through the math to figure out one minus 10 square of Z. So one minus a square is the local
 derivative. In our case, that is one minus the output of 10 H square, which here is H. So it's
 H square. And that is the local derivative. And then times the chain rule, D H. So that is going to be
 our candidate implementation. So if we come here, and then uncomment this, let's hope for the best.
 And we have the right answer. Okay, next up, we have D H P react. And we want to back propagate
 into the gain, the B and raw and the B and bias. So here, this is the bashroom parameters, B and
 gain and bias inside the bashroom that take the B and raw, that is exact unit Gaussian,
 and they scale it and shift it. And these are the parameters of the bashroom. Now, here, we have a
 multiplication, but it's worth noting that this multiply is very, very different from this matrix
 multiply here. Matrix multiply our dot products between rows and columns of these matrices involved.
 This is an element twice multiply. So things are quite a bit simpler. Now, we do have to be careful
 with some of the broadcasting happening in this line of code, though. So you see how B and gain
 and B and bias are one by 64. But H preact and B and raw are 32 by 64. So we have to be careful
 with that and make sure that all the shapes work out fine. And that the broadcasting is correctly
 back propagated. So in particular, let's start with D B and gain. So D B and gain should be.
 And here, this is again, element twice multiply. And whenever we have a times B equals C,
 we saw that the local derivative here is just if this is a local derivative is just the B, the
 other. So the local derivative is just B and raw. And then times chain rule. So D H preact.
 So this is the candidate gradient. Now again, we have to be careful because B and gain is of
 size one by 64. But this here would be 32 by 64. And so the correcting to do in this case, of course,
 is that B and gain here is a rule vector of 64 numbers. It gets replicated vertically in this
 operation. And so therefore the correcting to do is to sum because it's being replicated.
 And therefore, all the gradients in each of the rows that are now flowing backwards need to sum
 up to that same tensor D B and gain. So if to sum across all the zero, all the examples, basically,
 which is the direction which just gets replicated. And now we have to be also careful because we
 be in game is of shape one by 64. So in fact, I need to keep them as true. Otherwise, I will
 just get 64. Now I don't actually really remember why the B and gain and the B and bias. I made them
 be one by 64. But the biases be one and B two, I just made them be one dimensional vectors.
 They're not two dimensional tensors. So I can't recall exactly why I left the gain and the bias
 as two dimensional. But it doesn't really matter as long as you are consistent and you're keeping
 it the same. So in this case, we want to keep the dimension so that the tensor shapes work.
 Next up, we have B and raw. So D B and raw will be B and gain multiplying D H preact. That's our
 chain rule. Now what about the dimensions of this? We have to be careful, right? So D H preact is
 32 by 64. B and gain is one by 64. So we'll just get replicated and to create this multiplication,
 which is the correct thing, because in a forward pass, it also gets replicated in just the same way.
 So in fact, we don't need the brackets here, we're done. And the shapes are already correct.
 And finally, for the bias, very similar, this bias here is very, very similar to the bias we saw in
 the linear layer. And we see that the gradients from H preact will simply flow into the biases
 and add up, because these are just these are just offsets. And so basically, we want this to be D
 H preact, but it needs to sum along the right dimension. And in this case, similar to the gain,
 we need to sum across the zero dimension, the examples, because of the way that the bias gets
 replicated very quickly. And we also want to have keep them as true. And so this will basically take
 this and sum it up and give us a one by 64. So this is the candidate implementation, it makes all
 the shapes work. Let me bring it up down here. And then let me uncomment these three lines
 to check that we are getting the correct result for all the three tensors. And indeed, we see that
 all of that got back propagated correctly. So now we get to the batch norm layer. We see how here
 being gained and being biased are the parameters. So the back propagation ends. But being raw now
 is the output of the standardization. So here, what I'm doing, of course, is I'm breaking up the
 batch norm into manageable pieces. So we can back propagate through each line individually. But
 basically what's happening is B and mean I is the sum. So this is the B and mean I, I apologize for
 the variable naming. B and diff is X minus mu. B and diff two is X minus mu squared here inside
 the variance. B and var is the variance. So sigma squared, this is B and var. And it's basically the
 sum of squares. So this is the X minus mu squared, and then the sum. Now you'll notice one departure
 here. Here it is normalized as one over M, which is the number of examples. Here I'm normalizing as
 one over N minus one instead of M. And this is deliberate. I'll come back to that in a bit when
 we are at this line. It is something called the bestial correction. But this is how I want it in our case.
 B and var in then becomes basically B and var plus epsilon. Epsilon is one negative five. And then
 it's one over square root is the same as raising to the power of negative point five, right? Because
 point five is squared. And then negative makes it one over square root. So B and var in is one over
 this denominator here. And then we can see that B and raw, which is the X hat here, is equal to
 the B and diff, the numerator, multiplied by the B and var in. And this line here that creates
 pre H pre act was the last piece we've already back propagated through it. So now what we want to
 do is we are here and we have B and raw, and we have to first back propagate into B and diff and B
 and var in. So now we're here and we have DB and raw. And we need to back propagate through this line.
 Now I've written out the shapes here and indeed B and var in is a shape one by 64. So there is a
 broadcasting happening here that we have to be careful with. But it is just an element-wise simple
 multiplication. By now we should be pretty comfortable with that to get DB and diff. We know that this
 is just B and var in multiplied with DB and raw. And conversely, to get DB and var in, we need to
 take B and diff and multiply that by DB and raw. So this is the candidate. But of course, we need
 to make sure that broadcasting is obeyed. So in particular, B and var in multiplying with DB and
 raw will be okay and give us 32 by 64 as we expect. But DB and var in would be taking a 32 by 64
 multiplying it by 32 by 64. So this is a 32 by 64. But of course, DB, this B and var in is only
 one by 64. So the second line here needs a sum across the examples. And because there's this
 dimension here, we need to make sure that keep them is true. So this is the candidate.
 Let's erase this and let's swing down here and implement it. And then let's comment out
 DB and var in and DB and diff. Now, we'll actually notice that DB and diff, by the way, is going to
 be incorrect. So when I run this, B and var in this correct, B and diff is not correct.
 And this is actually expected because we're not done with B and diff. So in particular,
 when we slide here, we see here that B and raw is a function of B and diff. But actually,
 B and var is a function of B and var, which is a function of B and diff to, which is a function
 of B and diff. So it comes here. So B, D and diff, these variable names are crazy. I'm sorry.
 It branches out into two branches, and we've only done one branch of it. We have to continue
 our back propagation and eventually come back to be in diff. And then we'll be able to do a plus
 equals and get the actual current gradient. For now, it is good to verify that CBMP also works.
 It doesn't just lie to us and tell us that everything is always correct. It can in fact
 detect when your gradient is not correct. So it's that's good to see as well. Okay. So now we have
 the derivative here, and we're trying to back propagate through this line. And because we're
 raising to a power of negative point five, I brought up the power rule. And we see that basically,
 we have that the B and var will now be we bring down the exponent. So negative point five times
 X, which is this. And now race to the power of negative point five minus one, which is a negative
 one point five. Now, we would have to also apply a small chain rule here in our head, because we
 need to take further derivative of B and var with respect to this expression here inside the
 bracket. But because it's an element wise operation, and everything is fairly simple,
 that's just one. And so there's nothing to do there. So this is the local derivative. And then
 times the global derivative to create the chain rule. This is just times the B and var.
 So this is our candidate. Let me bring this down. And uncommon to the check.
 And we see that we have the correct result. Now, before we got propagate through the next line,
 I want to briefly talk about the note here, where I'm using the bestness correction,
 dividing by n minus one, instead of dividing by n, when I normalize here, the sum of squares.
 Now, you'll notice that this is the departure from the paper, which uses one over n instead,
 not one over n minus one. There m is rn. And so it turns out that there are two ways of estimating
 variance of an array. One is the biased estimate, which is one over n. And the other one is the
 unbiased estimate, which is one over n minus one. Now, confusingly, in the paper, this is
 not very clearly described. And also, it's a detail that kind of matters, I think.
 They are using the biased version of train time. But later, when they are talking about the inference,
 they are mentioning that when they do the inference, they are using the unbiased estimate,
 which is the n minus one version, in basically for inference, and to calibrate the running mean
 and running variance, basically. And so they actually introduce a train test mismatch,
 where in training, they use the biased version. And in the test time, they use the unbiased version.
 I find this extremely confusing. You can read more about the Bessel's correction, and why
 dividing by n minus one gives you a better estimate of the variance. In a case where you have population
 size, or samples for a population, they are very small. And that is indeed the case for us,
 because we are dealing with mini batches. And these mini matches are a small sample of a larger
 population, which is the entire training set. And so it just turns out that if you just estimate
 it using one over n, that actually almost always underestimates the variance. And it is a biased
 estimator, and it is advised that you use the unbiased version and divide by n minus one. And
 you can go through this article here that I liked, that actually describes the full reasoning,
 and I'll link it in the video description. Now, when you calculate the torso variance,
 you'll notice that they take the unbiased flag, whether or not you want to divide by n,
 or n minus one. Confusingly, they do not mention what the default is for unbiased. But I believe
 unbiased by default is true. I'm not sure why the docs here don't cite that. Now, in the Bessel
 Room, 1D, the documentation again is kind of wrong and confusing. It says that the standard
 deviation is calculated via the biased estimator. But this is actually not exactly right. And people
 have pointed out that it is not right in a number of issues since then. Because actually,
 the rabbit hole is deeper, and they follow the paper exactly. And they use the biased version
 for training. But when they're estimating the running standard deviation, we are using the unbiased
 version. So again, there's the train test mismatch. So long story short, I'm not a fan of train
 test discrepancies. I basically kind of consider the fact that we use the biased version, the
 training time, and the unbiased test time, I basically consider this to be a bug. And I don't
 think that there's a good reason for that. It's not really, they don't really go into the detail
 of the reasoning behind it in this paper. So that's why I basically prefer to use the Bessel's
 correction in my own work. Unfortunately, Bessel Room does not take a keyword argument that tells
 you whether or not you want to use the unbiased version or the biased version in both training
 tests. And so therefore anyone using Bessel Roomization, basically in my view has a bit of a bug in the
 code. And this turns out to be much less of a problem if your batch, many batch sizes are a bit
 larger. But still, I just might have kind of a unpodable. So maybe someone can explain why this is okay.
 But for now, I prefer to use the unbiased version consistently both during training and at test time.
 And that's why I'm using one over n minus one here. Okay, so let's now actually back propagate
 through this line. So the first thing that I always like to do is I like to scrutinize the
 shapes first. So in particular here, looking at the shapes of what's involved, I see that B and
 var shape is one by 64. So it's a row vector and B and if two dot shape is 32 by 64. So clearly
 here we're doing a sum over the zero axis to squash the first dimension of of the shapes here,
 using a sum. So that right away actually hints to me that there will be some kind of a replication
 or broadcasting in the backward pass. And maybe you're noticing the pattern here, but basically,
 anytime you have a sum in the forward pass, that turns into a replication or broadcasting in the
 backward pass along the same dimension. And conversely, when we have a replication or a broadcasting in
 the forward pass, that indicates a variable reuse. And so in the backward pass, that turns into a
 sum over the exact same dimension. And so hopefully you're noticing that duality that those two are
 kind of like the opposite of each other in the forward and backward pass. Now, once we understand
 the shapes, the next thing I like to do always is I like to look at a two example in my head to
 sort of just like understand roughly how the variable dependencies go in the mathematical formula.
 So here we have a two dimensional array at the end of two, which we are scaling by a constant.
 And then we are summing vertically over the columns. So if we have a two by two matrix A,
 and then we sum over the columns and scale, we would get a row vector b1 b2. And b1 depends on a
 in this way, whereas just some, they're scaled of a and b2 in this way, where it's the second column,
 sum and scale. And so looking at this basically, what we want to do now is we have the derivatives
 on b1 and b2, and we want to back propagate them into a's. And so it's clear that just
 differentiating in your head, the local derivative here is 1 over n minus 1 times 1
 for each one of these a's. And basically the derivative of b1 has to flow through the columns of a
 scaled by 1 over n minus 1. And that's roughly what's happening here. So intuitively, the derivative
 flow tells us that d bn df2 will be the local derivative of this operation. And there are many
 ways to do this by the way, but I like to do something like this, torch dot one's like of bn
 df2. So I'll create a large array to the measure of ones. And then I will scale it. So 1.0 divided by
 n minus 1. So this is a array of 1 over n minus 1. And that's sort of like the local derivative.
 And now for the chain rule, I will simply just multiply it by d bn bar.
 And notice here what's going to happen. This is 32 by 64. And this is just 1 by 64. So I'm letting
 the broadcasting do the replication, because internally in pytorch, basically d bn bar,
 which is 1 by 64 row vector, well, in this multiplication, get copied vertically until
 the two are of the same shape. And then there will be an element wise multiply. And so that
 so that the broadcasting is basically doing the replication. And I will end up with the derivatives
 of d bn df2 here. So this is the kentexolution. Let's bring it down here. Let's uncomment this line
 where we check it. And let's hope for the best. And indeed, we see that this is the correct formula.
 Next up, let's differentiate here into b and df. So here we have that b and df is element wise
 squared to create b and df2. So this is a relatively simple derivative, because it's a simple
 element wise operation. So it's kind of like the scalar case. And we have that d b and df
 should be, if this is x squared, then derivative of it is 2x. Right? So it's simply two times
 b and df. That's the local derivative. And then times chain rule. And the shape of these is the
 same, they are of the same shape. So times this. So that's the backward pass for this variable.
 Let me bring that down here. And now we have to be careful, because we already calculated db and
 df, right? So this is just the end of the other, you know, other branch coming back to b and df.
 Because b and df were already back propagated to way over here from b and raw. So we now completed
 the second branch. And so that's why I have to do plus equals. And if you recall, we had an
 incorrect derivative for b and df before. And I'm hoping that once we append this last missing
 piece, we have the exact correctness. So let's run and b and df2, b and df now actually shows
 the exact correct derivative. So that's comforting. Okay, so let's now back propagate through
 this line here. The first thing we do, of course, is we check the shapes. And I wrote them out here.
 And basically the shape of this is 32 by 64. HPBN is the same shape. But b and me and I is a
 row vector one by 64. So this minus here will actually do broadcasting. And so we have to be
 careful with that. And as a hint to us, again, because of the duality, a broadcasting in a forward
 pass means variable reuse. And therefore, there will be a sum in the backward pass.
 So let's write out the backward pass here now. Back propagate into the HPBN. Because these are
 the same shape, then the local derivative for each one of the elements here is just one for the
 corresponding element in here. So basically, what this means is that the gradient just simply copies
 is just a variable assignment, its quality. So I'm just going to clone this tensor, just for safety
 to create an exact copy of DB and diff. And then here to back propagate into this one,
 what I'm inclined to do here is DB and me and I will basically be what is the local derivative?
 Well, it's negative torch dot one, like of the shape of B and diff, right? And then times the
 derivative here, DB and diff. And this here is the back propagation for the replicated B and
 mean I. So I still have to back propagate through the replication in the broadcasting,
 and I do that by doing a sum. So I'm going to take this whole thing, and I'm going to do a sum
 over the zero dimension, which was the replication. So if you scrutinize this, by the way, you'll
 notice that this is the same shape as that. And so what I'm doing, what I'm doing here,
 doesn't actually make that much sense because it's just a array of one, it's multiplying the
 B and diff. So in fact, I can just do this. And there's a equivalent. So this is the candidate
 backward pass, let me copy it here. And then let me comment out this one and this one.
 Enter. And it's wrong. Damn. Actually, sorry, this is supposed to be wrong. And it's supposed to be
 wrong because we are back propagating from a B and diff into H preb and, but we're not done,
 because B and mean I depends on H preb and there will be a second portion of that derivative
 coming from this second branch. So we're not done yet, and we expect it to be incorrect. So
 there you go. So let's not back propagate from B and mean I into H preb and. And so here again,
 we have to be careful because there's a broadcasting along, or there's a sum along the zero dimension.
 So this will turn into broadcasting in the backward pass now. And I'm going to go a little bit faster
 on this line because it is very similar to the line that we had before, and multiplies in the
 past. In fact, so the H preb and will be the gradient will be scaled by one over N. And then
 basically this gradient here, DBM, mean, I is going to be scaled by one over N. And then it's going to
 flow across all the columns and deposit itself into D H preb and. So what we want is this thing
 scaled by one over N. Let me put the constant up front here. So scaled on the gradient, and now
 we need to replicate it across all the across all the rows here. So we I like to do that by
 torch dot ones like off basically H preb and. And I will let broadcasting do the work of
 replication. So like that. So this is the H preb and hopefully we can plus equals that.
 So this here is broadcasting. And then this is the scaling. So this should be correct.
 Okay, so that completes the back propagation of the bathroom layer. And we are now here.
 Let's back propagate through the linear layer one here. Now, because everything is getting a
 little vertically crazy, I copy pasted the line here. And let's just back propagate through this
 one line. So first, of course, we inspect the shapes and we see that this is 3,2, by 64.
 Emcat is 32 by 30. W one is 30 30 by 64. And B one is just 64. So as I mentioned,
 back propagating through linear layers is fairly easy just by matching the shapes. So let's do that.
 We have that D empat. Should be some matrix multiplication of D H preb and with W one and one
 transpose thrown in there. So to make empat be 32 by 30, I need to take D H preb and
 32 by 64 and multiply it by W one that transpose.
 To get D only one, I need to end up with 30 by 64. So to get that, I need to take
 empat transpose and multiply that by D H preb and finally to get D B one.
 This is a addition and we saw that basically I need to just sum the elements in D H preb
 and along some dimension. And to make the dimensions work out, I need to sum along the zero access here
 to eliminate this dimension. And we do not keep them. So that we want to just get a single one
 dimensional lecture of 64. So these are the claimed derivatives. Let me put that here and
 let me uncommon three lines and cross our fingers. Everything is great. Okay, so we now continue
 almost there. We have the derivative of emcat and we want to derivative, we want to back propagate
 into em. So I again copied this line over here. So this is the forward pass and then this is the
 shapes. So remember that the shape here was 32 by 30 and the original shape of em was 32 by 3 by 10.
 So this layer in the forward pass, as you recall, did the concatenation of these three 10 dimensional
 character vectors. And so now we just want to undo that. So this is actually relatively straight
 forward operation, because the backward pass of the, what is the view view is just a repress
 representation of the array. It's just a logical form of how you interpret the array. So let's
 just reinterpret it to be what it was before. So in other words, the em is not 32 by 30.
 It is basically the emcat. But if you view it as the original shape, so just m dot shape.
 You can pass intubples into view. And so this should just be, okay,
 we just rerepresent that view. And then we uncomment this line here and hopefully,
 yeah, so the derivative of em is correct. So in this case, we just have to rerepresent
 the shape of those derivatives into the original view. So now we are at the final line. And the
 only thing that's left to back propagate through is this indexing operation here, m is c at xb.
 So as I did before, I copy pasted this line here. And let's look at the shapes of everything
 that's involved and remind ourselves how this worked. So m dot shape was 32 by three by 10.
 So it's 32 examples. And then we have three characters. Each one of them has a 10 dimensional
 embedding. And this was achieved by taking the lookup table C, which have 27 possible characters,
 each of them 10 dimensional. And we looked up at the rows that were specified inside this
 tensor xb. So xb is 32 by three. And it's basically giving us for each example, the identity or the
 index of which character is part of that example. And so here I'm showing the first five rows of
 of this tensor xb. And so we can see that, for example, here it was the first example in this batch,
 is that the first character in the first character and the fourth character comes into the neural
 net. And then we want to predict the next character in a sequence after the character is 114.
 So basically what's happening here is there are integers inside xb. And each one of these
 integers is specifying which row of C we want to pluck out. Right. And then we arrange those
 rows that we've plucked out into three, two by three by 10 tensor, and we just package them
 in, we just package them into this tensor. And now what's happening is that we have d amp.
 So for every one of these basically plucked out rows, we have their gradients now. But they're
 arranged inside this 32 by three by 10 tensor. So all we have to do now is we just need to route
 this gradient backwards through this assignment. So we need to find which row of C that every one
 of these 10 dimensional embeddings come from. And then we need to deposit them into DC.
 So we just need to undo the indexing. And of course, if any of these rows of C was used multiple
 times, which almost certainly is the case, like the row one and one was used multiple times,
 then we have to remember that the gradients that arrive there have to add. So for each occurrence,
 we have to have an addition. So let's now write this out. And I don't actually know of like a
 much better way to do this than a for loop, unfortunately, in Python. So maybe someone can come up with
 a vectorized efficient operation. But for now, let's just use for loops. So let me create a torch
 dot zeros like C to initialize just 27 by 10 tensor of all zeros. And then honestly,
 4k in range, XB dot shape at zero. Maybe someone has a better way to do this. But for J in range,
 XB dot shape at one. This is going to iterate over all the, all the elements of XB, all these
 integers. And then let's get the index at this position. So the index is basically XB at kj.
 So that an example of that is 11 or 14 and so on. And now in a forward pass, we took,
 we basically took the row of C at index, and we deposited it into M at k a j. That's what happened.
 That's where they are packaged. So now we need to go backwards, and we just need to route
 DM at the position kj. We now have these derivatives for each position and it's 10
 dimensional. And you just need to go into the correct row of C. So DC rather at IX is this,
 but plus equals, because there could be multiple occurrences, like the same row could have been
 used many, many times. And so all of those derivatives will just go backwards through the indexing,
 and they will add. So this is my candidate solution. Let's copy it here.
 Let's uncomment this and cross our fingers. Hey, so that's it. We've back propagated through
 this entire beast. So there we go. Totally makes sense. So now we come to exercise two.
 It basically turns out that in this first exercise, we were doing way too much work.
 We were back propagating way too much. And it was all good practice and so on,
 but it's not what you would do in practice. And the reason for that is, for example,
 here I separated out this loss calculation over multiple lines, and I broke it up all,
 all two, like its smallest atomic pieces, and we back propagated through all of those individually.
 But it turns out that if you just look at the mathematical expression for the loss,
 then actually you can do the differentiation on pen and paper, and a lot of terms cancel and
 simplify. And the mathematical expression you end up with can be significantly shorter and
 easier to implement than back propagating through all the pieces of everything you've done.
 So before we had this complicated forward pass, going from logits to the loss,
 but in PyTorch, everything can just be glued together into a single call f dot cross entropy.
 You just pass in logits and the labels, and you get the exact same loss, as I verify here.
 So our previous loss and the fast loss coming from the chunk of operations as a single mathematical
 expression is the same, but it's much, much faster and forward pass. It's also much, much faster
 and backward pass. And the reason for that is if you just look at the mathematical form of this
 and differentiate again, you will end up with a very small and short expression. So that's what
 we want to do here. We want to in a single operation or in a single go, or like very quickly, go directly
 to delogits. And we need to implement delogits as a function of logits and YBs. But it will be
 significantly shorter than whatever we did here, where to get to delogits, we have to go all the way
 here. So all of this work can be skipped in a much, much simpler mathematical expression
 that you can implement here. So you can give it a shot yourself, basically look at what exactly
 is the mathematical expression of loss and differentiate with respect to the logits.
 So let me show you a hint. You can of course try it fully yourself. But if not, I can give you some
 hint of how to get started mathematically. So basically what's happening here is we have
 logits, then there's the softmax that takes the logits and gives you probabilities. Then we are
 using the identity of the correct next character to pluck out a row of probabilities, take the
 negative log of it to get our negative log probability. And then we average up all the
 log probabilities or negative log probabilities to get our loss. So basically what we have is for
 a single individual example, rather, we have that loss is equal to negative log probability,
 where P here is kind of like thought of as a vector of all the probabilities. So at the y
 position, where y is the label. And we have that P here, of course, is the softmax. So the
 i component of P of this probability vector is just the softmax function. So raising all the
 logits, basically to the power of E and normalizing. So everything comes to one. Now if you write out
 P of y here, you can just write out the softmax. And then basically what we're interested in is
 we're interested in the derivative of the loss with respect to the i-th logit.
 And so basically it's a d by dli of this expression here, where we have l indexed with the specific
 label y. And on the bottom we have a sum over j of e to the lj and the negative log of all that.
 So potentially give it a shot pen and paper and see if you can actually derive the expression
 for the loss by dli. And then we're going to implement it here. Okay, so I am going to give away
 the result here. So this is some of the math I did to derive the gradients analytically. And so we
 see here that I'm just applying the rules of calculus from your first or second year of bachelor's
 degree if you took it. And we see that the expression is actually simplified quite a bit. You have to
 separate out the analysis in the case where the i-th index that you're interested in inside
 logits is either equal to the label or it's not equal to the label. And then the expression
 simplifies and cancels in a slightly different way. And what we end up with is something very,
 very simple. We either end up with basically p at i where p is again this vector of probabilities
 after a softmax or p at i minus one, where we just simply subtract to one. But in any case we just
 need to calculate the softmax p and then in the correct dimension we need to subtract to one.
 And that's the gradient, the form that it takes analytically. So let's implement this basically.
 And we have to keep in mind that this is only done for a single example. But here we are working
 with batches of examples. So we have to be careful of that. And then the loss for a batch is the
 average loss over all the examples. So in other words, it's the example for all the individual
 examples is the loss for each individual example summed up and then divided by n. And we have to
 back propagate through that as well and be careful with it. So d logits is going to be f dot softmax.
 PyTorch has a softmax function that you can call. And we want to apply the softmax on the logits
 and we want to go in the dimension that is one. So basically we want to do the softmax along the
 rows of these logits. Then at the correct positions we need to subtract a one. So d logits at
 iterating over all the rows and indexing into the columns provided by the correct labels inside yb.
 We need to subtract one. And then finally, it's the average loss that is the loss. And in the
 average there's a one over n of all the losses added up. And so we need to also back propagate
 through that division. So the gradient has to be scaled down by n as well, because of the mean.
 But this otherwise should be the result. So now if we verify this, we see that we don't get an
 exact match. But at the same time, the maximum difference from logits from PyTorch and our d
 logits here is on the order of 5e negative nine. So it's a tiny, tiny number. So because of loading
 point of onkiness, we don't get the exact bitwise result. But we basically get the correct answer
 approximately. Now I'd like to pause here briefly before we move on to the next exercise,
 because I'd like us to get an intuitive sense of what d logits is, because it has a beautiful and
 very simple explanation, honestly. So here I'm taking the logits and I'm visualizing it. And we
 can see that we have a batch of 32 examples of 27 characters. And what is the logits intuitively,
 right? D logits is the probabilities that the probabilities matrix in a forward pass. But then
 here, these black squares are the positions of the correct indices where we subtracted a one.
 And so what is this doing, right? These are the derivatives on d logits. And so let's look at
 just the first row here. So that's what I'm doing here. I'm calculating the probabilities of these
 logits, and then I'm taking just the first row. And this is the probability row. And then the
 logits of the first row and multiplying by n just for us so that we don't have the scaling
 by n in here, and everything is more interpretable. We see that it's exactly equal to the probability,
 of course, but then the position of the correct index has a minus equals one. So minus one on that
 position. And so notice that if you take the logits at zero, and you sum it, it actually
 sums to zero. And so you should think of these gradients here at each cell as like a force.
 We are going to be basically pulling down on the probabilities of the incorrect characters.
 And we're going to be pulling up on the probability at the correct index. And that's what's basically
 happening in each row. And the amount of push and pull is exactly equalized because the sum is zero.
 So the amount to which we pulled down on the probabilities and the demand that we push up on
 the probability of the correct character is equal. So it's sort of the repulsion and attraction are
 equal. And think of the neural mat now as a, as a like a massive pulley system or something like
 that, we're up here on top of the logits, and we're pulling up, we're pulling down the probabilities
 and correct and pulling up the property of the correct. And in this complicated pulley system,
 because everything is mathematically just determined, just think of it as sort of like
 this tension translating to this complicating pulley mechanism. And then eventually we get a tug
 on the weights and the biases. And basically in each update, we just kind of like tug in the
 direction that we like for each of these elements. And the parameters are slowly given in to the tug.
 And that's what training in neural mat kind of like looks like on a high level. And so I think the
 the forces of push and pull in these gradients are actually very intuitive here. We're pushing
 and pulling on the correct answer and the incorrect answers. And the amount of force that we're
 applying is actually proportional to the probabilities that came out in the forward pass. And so for
 example, if our probabilities came out exactly correct, so they would have had zero everywhere,
 except for one at the correct position, then the the logits would be all row of zeros for that
 example, there would be no push and pull. So the amount to which your prediction is incorrect
 is exactly the amount by which you're going to get a pull or push in that dimension.
 So if you have, for example, a very confidently mispredicted element here,
 then what's going to happen is that element is going to be pulled down very heavily. And the
 correct answer is going to be pulled up to the same amount. And the other characters are not going
 to be influenced too much. So the amount to which you mis-predict is then proportional to the
 strength of the pull. And that's happening independently in all the dimensions of this of this tensor.
 And it's sort of very intuitive and very used to think through. And that's basically the magic
 of the cross entropy loss and what is doing dynamically in the backward pass of the neural mat.
 So now we get to exercise number three, which is a very fun exercise, depending on your definition
 of fun. And we are going to do for batch normalization exactly what we did for cross entropy loss in
 exercise number two. That is we are going to consider it as a glued single mathematical expression
 and back propagate through it in a very efficient manner, because we are going to derive a much
 simpler formula for the backward pass of batch normalization. And we're going to do that using
 pen and paper. So previously we've broken up batch normalization into all of the intermediate
 pieces and all the atomic operations inside it. And then we back propagate it through it one by one.
 Now we just have a single sort of forward pass of a batch room. And it's all glued together.
 And we see that we get the exact same result as before. Now for the batch backward pass,
 we'd like to also implement a single formula basically for back propagating through this entire
 operation that is the batch normalization. So in the forward pass previously, we took
 H prebn, the hidden states of the pre-bacterialization and created H preact, which is the hidden states
 just before the activation. In the batch normalization paper, H prebn is X and H preact is Y.
 So in the backward pass, what we'd like to do now is we have D H preact and we'd like to produce
 D H prebn. And we'd like to do that in a very efficient manner. So that's the name of the game.
 Calculate the H prebn given the H preact. And for the purposes of this exercise, we're going to
 ignore gamma and beta and their derivatives because they take on a very simple form in a very similar
 way to what we did up above. So let's calculate this, given that right here. So to help you a
 little bit like I did before, I started off the implementation here on pen and paper. And I took
 two sheets of paper to derive the mathematical formulas for the backward pass. And basically,
 to set up the problem, just write out the mu sigma square variance, Xi hat and Yi exactly as in the
 paper except for the Bessel correction. And then in the backward pass, we have the derivative of
 the loss with respect to all the elements of Y. And remember that Y is a vector. There's there's
 multiple numbers here. So we have all the derivatives with respect to all the Ys. And then there's a
 dima in the beta. And this is kind of like the compute graph. The gamma and the beta, there's the
 X hat. And then the mu and the sigma squared and the X. So we have DL by D YI. And we won't DL by D Xi
 for all the I's in these vectors. So this is the compute graph. And you have to be careful because
 I'm trying to note here that these are vectors. There's many nodes here inside X, X hat and Y.
 But mu and sigma, sorry, sigma square are just individual scalars, single numbers. So you have
 to be careful with that. You have to imagine there's multiple nodes here, or you're going to get your
 math roll. So as an example, I would suggest that you go in the following order, one, two, three,
 four, in terms of the back propagation. So back propagating to X hat, then to sigma square,
 then into mu, and then into X. Just like an anthropological sort in micro grad, we would go
 from right to left. You're doing the exact same thing, except you're doing it with symbols and on
 piece of paper. So for number one, I'm not giving away too much. If you want DL of the XI hat,
 then we just take DL by D YI and multiply by gamma, because of this expression here,
 where any individual YI is just gamma times XI hat plus beta. So it doesn't help you too much
 there. But this gives you basically the derivatives for all the X hats. And so now,
 try to go through this computational graph and derive what is DL by D sigma square.
 And then what is DL by D mu, and then what is DL by D X eventually. So give it a go,
 and I'm going to be revealing the answer one piece at a time. Okay, so to get DL by D sigma square,
 we have to remember again, like I mentioned, that there are many Xs, X hats here. And remember
 that sigma square is just a single individual number here. So when we look at the expression
 for DL by D sigma square, we have that we have to actually consider all the possible paths that
 we basically have that there's many X hats, and they all feed off from the all depend on sigma
 square. So sigma square has a large fan out. There's lots of arrows coming out from sigma square
 into all the X hats. And then there's a back propagating signal from each X hat into sigma square. And
 that's why we actually need to sum over all those eyes from i equal to one to m of the DL by D Xi hat,
 which is the global gradient times the X i hat by D sigma square, which is the local gradient
 of this operation here. And then mathematically, I'm just working it out here, and I'm simplifying
 and you get a certain expression for DL by D sigma square. We're going to be using this
 expression when we back propagate into mu, and then eventually into X. So now let's continue our
 back propagation into mu. So what is DL by D mu? Now again, be careful that mu influences X hat,
 and X hat is actually lots of values. So for example, if our mini batch size is 32, as it is in our
 example that we were working on, then this is 32 numbers and 32 arrows going back to mu. And then
 mu going to sigma square is just a single arrow because sigma square is a scalar. So in total,
 there are 33 arrows emanating from you. And then all of them have gradients coming into mu,
 and they all need to be summed up. And so that's why when we look at the expression for DL by D mu,
 I am summing up over all the gradients of DL by D Xi hat times the Xi hat by D mu.
 So that's this arrow and the 32 arrows here. And then plus the one arrow from here, which is DL by D
 sigma square times the sigma square by D mu. So now we have to work out that expression,
 and let me just reveal the rest of it. Simplifying here is not complicated, the first term, and you
 just get an expression here. For the second term though, there's something really interesting that
 happens. When we look at the sigma square by D mu, and we simplify, at one point, if we assume
 that in a special case where mu is actually the average of Xi's, as it is in this case,
 then if we plug that in, then actually the gradient vanishes and becomes exactly zero.
 And that makes the entire second term cancel. And so these, if you just have a mathematical
 expression like this, and you look at D sigma square by D mu, you would get some mathematical
 formula for how mu impacts sigma square. But if it is the special case that mu is actually equal
 to the average, as it is in the case of rationalization, that gradient will actually vanish and become
 zero. So the whole term cancels, and we just get a fairly straightforward expression here for DL by
 D mu. Okay, and now we get to the craziest part, which is deriving DL by D Xi, which is ultimately
 what we're after. Now, let's count. First of all, how many numbers are there inside X? As I mentioned,
 there are 32 numbers. There are 32 little Xi's. And let's count the number of arrows emanating
 from each Xi. There's an arrow going to mu, an arrow going to sigma square. And then there's an
 arrow going to X hat. But this arrow here, let's group now that a little bit. Each Xi hat is just a
 function of Xi and all the other scalars. So Xi hat only depends on Xi and none of the other
 Xs. And so therefore, there are actually in this single arrow, there are 32 arrows. But those 32
 arrows are going exactly parallel. They don't interfere. They're just going parallel between
 X and X hat. You can look at it that way. And so how many arrows are emanating from each Xi? There
 are three arrows, mu sigma square, and the associated X hat. And so in back propagation, we now need
 to apply the chain rule. And we need to add up those three contributions. So here's what that
 looks like. If I just write that out, we have, we're going through, we're chaining through mu
 sigma square and through X hat. And those three terms are just here. Now we already have three of
 these. We have DL by DXI hat. We have DL by D mu, which we derived here. And we have DL by D
 sigma square, which we derived here. But we need three other terms here. This one, this one, and
 this one. So I invite you to try to derive them. It's not that complicated. You're just looking at
 these expressions here and differentiating with respect to Xi. So give it a shot, but here's the
 result. Or at least what I got. Yeah, I'm just, I'm just differentiating with respect to Xi for
 all of these expressions. And honestly, I don't think there's anything too tricky here. It's
 basic calculus. Now it gets a little bit more tricky is we are now going to plug everything
 together. So all of these terms multiplied with all of these terms and added up according to this
 formula. And that gets a little bit hairy. So what ends up happening is, you get a large expression.
 And the thing to be very careful with here, of course, is we are working with a DL by DXI for
 specific by here. But when we are plugging in some of these terms, like say, this term here,
 DL by DXI squared, you see how DL by DXI squared, I end up with an expression. And I'm iterating
 over little eyes here. But I can't use I as the variable when I plug in here, because this is a
 different eye from this eye. This eye here is just a place or a local variable for for a for loop
 in here. So here, when I plug that in, you notice that I rename the I to a J. Because I need to
 make sure that this J is not that this J is not this I, this J is like a little local iterator over 32
 terms. And so you should be careful with that when you're plugging in the expressions from here
 to here, you may have to rename eyes into J's. You have to be very careful what is actually an I
 with respect to the L by DXI. So some of these are J's, some of these are I's. And then we simplify
 this expression. And I guess like the big thing to notice here is a bunch of terms just gonna come
 out to the front and you can refactor them. There's a sigma squared plus epsilon raised to the power
 of negative three over two. This sigma squared plus epsilon can be actually separated out into
 three terms. Each of them are sigma squared plus epsilon to the negative one over two. So the three
 of them multiplied is equal to this. And then those three terms can go different places,
 because of the multiplication. So one of them actually comes out to the front and will end up
 here outside. One of them joins up with this term and one of them joins up with this other term.
 And then when you simplify the expression, you'll notice that some of these terms that are coming
 out are just the XI hats. So you can simplify just by rewriting that. And what we end up with at the
 end is a fairly simple mathematical expression over here that I cannot simplify further. But
 basically, you'll notice that it only uses the stuff we have and it derives the thing we need.
 So we have dl by dy for all the eyes. And those are used plenty of times here.
 And also in the show what we're using is these XI hats and XJ hats. And they just come from the
 forward pass. And otherwise, this is a simple expression and it gives us dl by d xi for all
 the eyes. And that's ultimately what we're interested in. So that's the end of a Batchnorm
 backward pass analytically. Let's now implement this final result. Okay, so I implemented the
 expression into a single line of code here. And you can see that the max diff is tiny. So this
 is the correct implementation of this formula. Now, I'll just basically tell you that getting
 this formula here from this mathematical expression was not trivial. And there's a lot going on
 packed into this one formula. And this is all exercised by itself. Because you have to consider
 the fact that this formula here is just for a single neuron and a batch of 32 examples. But
 what I'm doing here is I'm actually we actually have 64 neurons. And so this expression has to
 imperil evaluate the Batchnorm backward pass for all those 64 neurons in parallel independently.
 So this has to happen basically in every single column of the inputs here. And in addition to that,
 you see how there are a bunch of sums here. And we need to make sure that when I do those sums
 that they broadcast correctly onto everything else that's here. And so getting this expression is
 just like highly non trivial and I invite you to basically look through it and step through it.
 And it's a whole exercise to make sure that this checks out. But once all the shapes agree,
 and once you convince yourself that it's correct, you can also verify that patchers gets the exact
 same answer as well. And so that gives you a lot of peace of mind that this mathematical formula
 is correctly implemented here and broadcast it correctly and replicated in parallel for all the
 64 neurons inside this Batchnorm layer. Okay, and finally exercise number four asks you to put it
 all together. And here we have a redefinition of the entire problem. So you see that we reinstallize
 the neural net from scratch and everything. And then here, instead of calling a loss that backward,
 we want to have the manual back propagation here as we derived it up above. So go up, copy paste all
 the chunks of code that we've already derived, put them here and derive your own gradients,
 and then optimize this neural net, basically using your own gradients all the way to the
 calibration of the Batchnorm and the evaluation of the loss. And I was able to achieve quite a good
 loss, basically the same loss you would achieve before. And that shouldn't be surprising because
 all we've done is we've really gotten into loss that backward and we've pulled out all the code
 and inserted it here. But those gradients are identical and everything is identical and the
 result are identical. It's just that we have full visibility on exactly what goes on under the hood
 of a lot of backward in this specific case. Okay, and this is all of our code. This is the full
 backward pass using basically the simplified backward pass for the cross entropy loss and the
 Batchnormization. So back propagating through cross entropy, the second layer, the 10-H null
 linearity, the Batchnormization through the first layer and through the embedding. And so you see
 that this is only maybe what is this 20 lines of code or something like that. And that's what
 gives us gradients. And now we can potentially erase loss as backward. So the way I have the code
 set up is you should be able to run this entire cell once you fill this in. And this will run for
 only 100 iterations and then break. And it breaks because it gives you an opportunity to check your
 gradients against PyTorch. So here our gradients we see are not exactly equal. They are approximately
 equal and the differences are tiny, one in negative nine or so. And I don't exactly know where they're
 coming from, to be honest. So once we have some confidence that the gradients are basically correct,
 we can take out the gradient checking. We can disable this breaking statement. And then we can
 basically disable loss the backward. We don't need it anymore. It feels amazing to say that.
 And then here, when we are doing the update, we're not going to use p.grad. This is the old way
 of PyTorch. We don't have that anymore because we're not doing backward. We are going to use
 this update where we you see that I'm iterating over, I've arranged the grads to be in the same
 order as the parameters and I'm zipping them up the gradients and the parameters into p and grad.
 And then here I'm going to step with just the grad that we derived manually. So the last piece
 is that none of this now requires gradients from PyTorch. And so one thing you can do here
 is you can do with Torched.NoGrad and offset this whole code block. And really what you're saying
 is you're telling PyTorch that, "Hey, I'm not going to call backward on any of this." And this
 last PyTorch to be a bit more efficient with all of it. And then we should be able to just run this
 and it's running. And you see that loss of backward is commented out and we're optimizing.
 So we're going to leave this run and hopefully we get a good result. Okay, so I allowed the
 neural out to finish optimization. Then here I calibrated the batch on parameters because I did
 not keep track of the running mean and variance in their training loop. Then here I ran the loss
 and you see that we actually obtained a pretty good loss very similar to what we've achieved
 before. And then here I'm sampling from the model and we see some of the name like gibberish
 that we're sort of used to. So basically the model worked and samples for decent results
 compared to what we were used to. So everything is the same but of course the big deal is that we
 did not use lots of backward. We did not use PyTorch autograd and we estimated our gradients
 ourselves by hand. And so hopefully you're looking at this, the backward pass of this neural net
 and you're thinking to yourself, actually that's not too complicated. Each one of these layers is
 like three lines of code or something like that. And most of it is fairly straightforward,
 potentially with the notable exception of the batch normalization backward pass. Otherwise,
 it's pretty good. Okay, and that's everything I wanted to cover for this lecture. So hopefully
 you found this interesting. And what I liked about it honestly is that it gave us a very nice
 diversity of layers to back propagate through. And I think it gives a pretty nice and comprehensive
 sense of how these backward passes are implemented and how they work. And you'd be able to derive
 them yourself. But of course in practice, you probably don't want to and you want to use the
 PyTorch autograd. But hopefully you have some intuition about how gradients flow backwards
 through the neural net, starting at the loss and how they flow through all the variables and
 all the intermediate results. And if you understood a good chunk of it and if you have a sense of that,
 then you can count yourself as one of these buff doggies on the left, instead of the
 doggies on the right here. Now in the next lecture, we're actually going to go to
 recurrent neural nets, LSTMs, and all the other variants of ARNIS. And we're going to start to
 complexify the architecture and start to achieve better log likelihoods. And so I'm really looking
 forward to that. And I'll see you then.
 Hi everyone. Today we are continuing our implementation of Makemore, our favorite
 character level language model. Now you'll notice that the background behind me is different,
 that's because I am in Kyoto and it is awesome. So I'm in a hotel room here.
 Now over the last few lectures we've built up to this architecture that is a multi-layer
 perceptron character level language model. So we see that it receives three previous characters
 and tries to predict the fourth character in a sequence using a very simple multi-layer perceptron
 using one hidden layer of neurons with tenational nearities. So what we'd like to do now in this
 lecture is I'd like to complexify this architecture. In particular we would like to take more characters
 in a sequence as an input, not just three. And in addition to that we don't just want to feed them
 all into a single hidden layer because that squashes too much information too quickly. Instead we would
 like to make a deeper model that progressively fuses this information to make its guess about
 the next character in a sequence. And so we'll see that as we make this architecture more complex
 we're actually going to arrive at something that looks very much like a weight net.
 So weight net is this paper published by Define in 2016 and it is also a language model basically
 but a try to predict audio sequences instead of character level sequences or word level sequences.
 But fundamentally the modeling setup is identical. It is an autoregressive model
 and it tries to predict the next character in a sequence and the architecture actually takes
 this interesting hierarchical sort of approach to predicting the next character in a sequence
 with this tree-like structure. And this is the architecture and we're going to implement it
 in the course of this video. So let's get started. So the story code for part five is very similar
 to where we ended up in part three. Recall that part four was the manual dot replication exercise
 that is kind of an aside. So we are coming back to part three, copy-pasting chunks out of it
 and that is our starter code for part five. I've changed very few things otherwise.
 So a lot of this should look familiar to if you've gone through part three. So in particular,
 very briefly we are doing imports, we are reading our data set of words and we are processing the
 data set of words into individual examples and none of this data generation code has changed.
 And basically we have lots and lots of examples. In particular, we have 182,000 examples of three
 characters trying to predict the fourth one. And we've broken up every one of these words into
 little problems of giving three characters predict the fourth one. So this is our data set and this
 is where we're trying to get the neural lab to do. Now in part three, we started to develop our code
 around these layer modules that are, for example, a class linear. And we're doing this because we
 want to think of these modules as building blocks and like a Lego building block bricks that we can
 sort of like stack up into neural networks. And we can feed data between these layers and stack
 them up into sort of graphs. Now we also developed these layers to have APIs and signatures very
 similar to those that are found in PyTorch. So we have Torched.nn and it's got all these layer
 building blocks that you would use in practice. And we were developing all of these to mimic
 APIs of these. So for example, we have linear. So there will also be a Torched.nn.linear
 and its signature will be very similar to our signature. And the functionality will be also
 quite identical as far as I'm aware. So we had the linear layer with the Batchroom 1D layer
 and the 10H layer that we developed previously. And linear just does a matrix multiply in the
 forward pass of this module. Batchroom, of course, is this crazy layer that we developed in the
 previous lecture. And what's crazy about it is, well, there's many things. Number one, it has these
 running mean and variances that are trained outside of back propagation. We are trained using
 exponential moving average inside this layer when we call the forward pass. In addition to that,
 there's this training plug because the behavior of Batchroom is different during train time
 and evaluation time. And so suddenly we have to be very careful that Batchroom is in its correct
 state, that it's in the evaluation state or training state. So that's something to now keep track of
 something that sometimes introduces bugs because you forget to put it into the right mode.
 And finally, we saw that Batchroom couples the statistics or the activations across the examples
 in the batch. So normally we thought of the batch as just an efficiency thing. But now we are
 coupling the computation across batch elements. And it's done for the purposes of controlling
 the activation statistics as we saw in the previous video. So it's a very weird layer,
 at least a lot of bugs. Partly, for example, because you have to modulate the training in
 eval phase and so on. In addition, for example, you have to wait for the mean and variance to
 settle and to actually reach a steady state. And so you have to make sure that you basically
 there's state in this layer and state is harmful, usually. Now, I brought out the generator object.
 Previously, we had a generator equals g and so on inside these layers. I've discarded that in favor
 of just initializing the torch RNG outside here just once globally, just for simplicity.
 And then here we are starting to build out some of the neural elements. This should look very
 familiar. We are we have our embedding table C, and then we have a list of players. And it's a
 linear feeds to batch or feeds to 10H. And then a linear output layer and its weights are scaled
 down. So we are not confidently wrong at initialization. We see that this is about 12,000 parameters.
 We're telling pators that the parameters require gradients. The optimization is as far as I'm
 aware, identical and should look very, very familiar. Nothing changed here. Last function looks very
 crazy. We should probably fix this. And that's because 32 batch elements are too few. And so you
 can get very lucky, lucky or unlucky in any one of these batches, and it creates a very thick
 loss function. So we're going to fix that soon. Now, once we want to evaluate the trained neural
 network, we need to remember because of the batch from layers to set all the layers to be training
 equals false. So this only matters for the batch from layer so far. And then we evaluate.
 We see that currently we have validation loss of 2.10, which is fairly good, but there's still
 ways to go. But even at 2.10, we see that when we sample from the model, we actually get relatively
 name like results that do not exist in a training set. So for example, a von, kilo, a pros, a liar,
 etc. So certainly not reasonable, not unreasonable, I would say, but not amazing. And we can still
 push this validation loss even lower and get much better samples that are even more name like.
 So let's improve this model. Okay, first, let's fix this graph because it is daggers in my eyes,
 and I just can't take it anymore. So last I, if you recall, is a Python list of floats. So for
 example, the first 10 elements look like this. Now what we'd like to do basically is we need to
 average up some of these values to get a more sort of representative value along the way. So one
 way to do this is to follow him in pytorch. If I create, for example, a tensor of the first 10
 numbers, then this is currently a one dimensional array. But recall that I can view this array
 as two dimensional. So for example, I can view it as a two by five array. And this is a 2D tensor
 now, two by five. And you see what pytorch has done is that the first row of this tensor is the
 first five elements. And the second row is the second five elements. I can also view it as a
 five by two as an example. And then recall that I can also use negative one in place of one of
 these numbers in pytorch will calculate what that number must be in order to make the number
 of elements work out. So this can be this or like that, both will work. Of course, this would not work.
 Okay, so this allows it to spread out some of the consecutive values into rows. So that's very
 helpful because what we can do now is first of all, we're going to create a torch.tensor
 out of the list of floats. And then we're going to view it as whatever it is, but we're going to
 stretch it out into rows of 1000 consecutive elements. So the shape of this now becomes 200 by 1000.
 And each row is 1000 consecutive elements in this list. So that's very helpful because now we can do
 a mean along the rows. And the shape of this will just be 200. And so we've taken basically the mean
 on every row. So PLT dot plot of that should be something nicer. Much better. So we see that we've
 basically made a live progress. And then here, this is the learning rate decay. So here we see that
 the learning rate decay subtracted a ton of energy out of the system and allowed us to settle into
 serve the local minimum in this optimization. So this is a much nicer plot. Let me come up and
 delete the monster. And we're going to be using this going forward. Now next up, what I'm bothered
 by is that you see our forward pass is a little bit gnarly and takes away too many lines of code.
 So in particular, we see that we've organized some of the layers inside the layers list,
 but not all of them for no reason. So in particular, we see that we still have the embedding table,
 special case outside of the layers. And in addition to that, the viewing operation here is also
 outside of our layers. So let's create layers for these. And then we can add those layers to just
 our list. So in particular, the two things that we need is here, we have this embedding table. And
 we are indexing at the integers inside the batch exp inside the tensor exp. So that's an embedding
 table lookup just done with indexing. And then here we see that we have this view operation,
 which if you recall from the previous video, simply rearranges the character embeddings
 and stretches them out into row. And effectively what for that does is the concatenation operation
 basically, except it's free because viewing is very cheap in PyTorch. And no memory is being
 copied. We're just re-representing how we view that tensor. So let's create modules for both
 of these operations, the embedding operation and the flattening operation. So I actually wrote the
 code in just to save some time. So we have a module embedding and a module flatten. And both of them
 simply do the indexing operation in a forward pass and the flattening operation here. And this
 C now will just become a soft that weight inside an embedding module. And I'm calling these layers
 specifically embedding and flattening because it turns out that both of them actually exist in PyTorch.
 So in PyTorch, we have an end out embedding. And it also takes the number of embeddings and the
 dimensionality of the embedding, just like we have here. But in addition, PyTorch takes in a lot of
 other keyword arguments that we are not using for our purposes yet. And for flatten, that also
 exists in PyTorch. And it also takes additional keyword arguments that we are not using. So we
 have a very simple flatten. But both of them exist in PyTorch, they're just a bit more simpler.
 And now that we have these, we can simply take out some of these special-case things. So instead
 of C, we're just going to have an embedding and a vocab size and an embed. And then after the embedding,
 we are going to flatten. So let's construct those modules. And now I can take out this C.
 And here, I don't have to special case anymore, because now C is the embedding's weight and it's
 inside layers. So this should just work. And then here, our forward pass simplifies substantially,
 because we don't need to do these now outside of these layers, outside and explicitly.
 They're now inside layers. So we can delete those. But now to kick things off, we want this little
 X, which in the beginning is just Xb, the tensor of integers specifying the identities of these
 characters at the input. And so these characters can now directly feed into the first layer,
 and this should just work. So let me come here and insert a break, because I just want to make
 sure that the first iteration of this runs, and that there's no mistake. So that ran properly.
 And basically, we substantially simplified the forward pass here. Okay, I'm sorry, I changed my
 microphone. So hopefully the audio is a little bit better. Now, one more thing that I would like to
 do in order to pytorchify our code even further, is that right now we are maintaining all of our
 modules in a naked list of layers. And we can also simplify this, because we can introduce the
 concept of pytorch containers. So in torch.nn, which we are basically rebuilding from scratch here,
 there's a concept of containers. And these containers are basically a way of organizing
 layers into lists or dicks and so on. So in particular, there's a sequential,
 which maintains a list of layers, and is a module class in pytorch. And it basically just passes a
 given input through all the layers sequentially exactly as we are doing here. So let's write our
 own sequential, I've written a code here. And basically the code for sequential is quite straightforward.
 We pass in a list of layers, which we keep here, and then given any input in a forward pass,
 we just call the layers sequentially, and return the result. In terms of the parameters, it's just
 all the parameters of the child modules. So we can run this, and we can again simplify this
 substantially, because we don't maintain this naked list of layers, we now have a notion of a
 model, which is a module. And in particular, is a sequential of all the layers.
 And now parameters are simply just a model, but parameters. And so that list comprehension now
 lives here. And then here we are, press here we are doing all the things we used to do.
 Now here, the code again simplifies substantially, because we don't have to do this forwarding here,
 instead of just call the model on the input data, and the input data here are the integers inside
 XB. So we can simply do logits, which are the outputs of our model, are simply the model called
 on XB. And then the cross entropy here takes the logits and the targets. So this simplifies
 substantially. And then this looks good. So let's just make sure this runs, that looks good.
 Now here, we actually have some work to do still here, but I'm going to come back later. For now,
 there's no more layers, there's a model that layers, but it's not a to access attributes of
 these classes directly. So we'll come back and fix this later. And then here, of course, this
 simplifies substantially as well, because logits are the model called on X. And then these logits
 come here. So we can evaluate the train evaluation loss, which currently is terrible, because we
 just initialized in your alert. And then we can also sample from the model. And this simplifies
 dramatically as well, because we just want to call the model on to the context and outcome
 logits. And then these logits go into softmax and get the probabilities, etc. So we can sample
 from this model. What did I screw up? Okay, so I fixed the issue, and we now get the result that
 we expect, which is gibberish, because the model is not trained, because we reinitialize it from
 scratch. The problem was that when I fixed this cell to be modeled out layers, instead of just
 layers, I did not actually run the cell. And so our neural net was in a training mode. And what
 caused the issue here is the batch term layer, as a batch term layer often likes to do,
 because batch term was in the training mode. And here we are passing in an input, which is a batch
 of just a single example made up of the context. And so if you are trying to pass in a single
 example into a batch norm that is in the training mode, you're going to end up estimating the variance
 using the input. And the variance of a single number is not a number, because it is a measure
 of a spread. So for example, the variance of just a single number five, you can see is not a number.
 And so that's what happened. And the batch term basically caused an issue, and then that polluted
 all of the further processing. So all that we had to do was make sure that this runs. And we
 basically made the issue of, again, we didn't actually see the issue with the loss. We could
 have evaluated the loss, but we got the wrong result, because Bashur was in the training mode.
 And and so we still get a result is just the wrong result, because it's using the sample statistics
 of the batch, whereas we want to use the running mean and running variance inside the Bashur.
 And so, again, an example of introducing a bug in line, because we did not properly maintain
 the state of what is training or not. Okay, so I rerun everything. And here's what we are.
 As a reminder, we have the training loss of 2.05 and validation 2.10. Now, because these losses
 are very similar to each other, we have a sense that we are not overfitting too much on this task,
 and we can make additional progress in our performance by scaling up the size of the neural
 network and making everything bigger and deeper. Now, currently, we are using this architecture here,
 where we are taking in some number of characters, going into a single hidden layer, and then going
 to the prediction of the next character. The problem here is we don't have an naive way of making this
 bigger in a productive way. We could, of course, use our layers, sort of building blocks and materials
 to introduce additional layers here and make the network deeper. But it is still the case that we
 are crushing all of the characters into a single layer all the way at the beginning.
 And even if we make this a bigger layer and add neurons, it's so kind of like silly to
 squash all that information so fast in a single step. So what we'd like to do instead is we'd like
 our network to look a lot more like this in the WaveNet case. So you see in the WaveNet, when we
 are trying to make the prediction for the next character in the sequence, it is a function of
 the previous characters that are feeding that feed in, but not all of these different characters
 are not just crushed to a single layer. And then you have a sandwich. They are crushed slowly.
 So in particular, we take two characters and we fuse them into sort of like a
 diagram representation. And we do that for all these characters consecutively. And then we take
 the bi-grams and we fuse those into four characters while with chunks. And then we fuse that again.
 And so we do that in this like tree-like, hierarchical manner. So we fuse the information from the
 previous context slowly into the network as it gets deeper. And so this is the kind of architecture
 that we want to implement. Now in the WaveNet's case, this is a visualization of a stack of dilated
 causal convolution layers. And this makes it sound very scary, but actually the idea is very simple.
 And the fact that it's a dilated causal convolution layer is really just an implementation detail to
 make everything fast. We're going to see that later. But for now, let's just keep the basic idea
 of it, which is this progressive fusion. So we want to make the network deeper. And at each level,
 we want to fuse only two consecutive elements, two characters, then two bi-grams, then two
 four grams, and so on. So let's implement this. Okay, so first up, let me scroll to where we built
 the data set and let's change the block size from three to eight. So we're going to be taking
 eight characters of context to predict the ninth character. So the data set now looks like this.
 We have a lot more context feeding in to predict any next character in a sequence. And these eight
 characters are going to be processed in this tree-like structure. Now if we scroll here,
 everything here should just be able to work. So we should be able to redefine the network.
 You see that the number of parameters has increased by 10,000. And that's because the block size has
 grown. So this first linear layer is much, much bigger. Our linear layer now takes eight characters
 into this middle layer. So there's a lot more parameters there. But this should just run.
 Let me just break right after the very first iteration. So you see that this runs just fine.
 It's just that this network doesn't make too much sense. We're crushing way too much information,
 way too fast. So let's now come in and see how we could try to implement the hierarchical scheme.
 Now before we dive into the detail of the re-implementation here, I was just curious to
 actually run it and see where we are in terms of the baseline performance of just lazily scaling up
 the context length. So I'll let it run. We get a nice loss curve. And then evaluating the loss,
 we actually see quite a bit of improvement just from increasing the context log length.
 So I started a little bit with performance log here. And previously where we were is we were
 getting performance of 2.10 on the validation loss. And now simply scaling up the context length from
 328 gives us a performance of 2.02. So quite a bit of an improvement here. And also when you
 sample from the model, you see that the names are definitely improving qualitatively as well.
 So we could of course spend a lot of time here tuning things and making it even bigger and scaling
 up a network further, even with this simple sort of setup here. But let's continue and let's
 implement here a model and treat this as just a rough baseline performance. But there's a lot of
 optimization like left on the table in terms of some of the hyper parameters that you're hopefully
 getting a sense of now. Okay, so let's scroll up now and come back up. And what I've done here is
 I've created a bit of a scratch space for us to just like look at the forward pass of the neural
 net and inspect the shape of the tensor along the way as the neural net forwards. So here,
 I'm just temporarily for debugging, creating a batch of just say, for examples. So for random
 integers, then I'm plucking out those rows from our training set. And then I'm passing into the
 model, the input XB. Now the shape of XB here, because we have only four examples is four by eight.
 And this eight is now the current block size. So inspecting XB, we just see that we have four
 examples. Each one of them is a row of XB. And we have eight characters here. And this integer
 tensor just contains the identities of those characters. So the first layer of our neural
 net is the embedding layer. So passing XB, this integer tensor through the embedding layer,
 creates an output that is four by eight by 10. So our embedding table has, for each character,
 a 10 dimensional vector that we are trying to learn. And so what the embedding layer does here,
 is it blocks out the embedding vector for each one of these integers, and organizes it all in a
 four by eight by 10 tensor now. So all of these integers are translated into 10 dimensional
 vectors inside this three dimensional tensor now. Now passing that through the flatten layer,
 as you recall, what this does is it views this tensor as just a four by 80 tensor. And what that
 affects the way does is that all these 10 dimensional embeddings for all these eight characters just
 end up being stretched out into a long row. And that looks kind of like a concatenation operation,
 basically. So by viewing the tensor differently, we now have a four by 80. And inside this 80,
 it's all the 10 dimensional vectors just concatenating next to each other. And the linear layer,
 of course, takes 80 and creates 200 channels, just via matrix multiplication. So so far so good.
 Now I'd like to show you something surprising. Let's look at the insides of the linear layer and
 remind ourselves how it works. The linear layer here in the forward pass takes the input x,
 multiplies it with a weight, and then optionally adds bias. And the weight here is two dimensional,
 as defined here, and the bias is one dimensional here. So effectively, in terms of the shapes involved,
 what's happening inside this linear layer looks like this right now. And I'm using random numbers
 here, but I'm just illustrating the shapes and what happens. Basically, a four by 80 input comes
 into the linear layer. It's multiplied by this 80 by 200 weight matrix inside. And there's a plus
 200 bias. And the shape of the whole thing that comes out of the linear layer is four by 200,
 as we see here. Now notice here, by the way, that this here will create a four by 200 tensor,
 and then plus 200, there's a broadcasting happening here, about four by 200 broadcasts with 200.
 So everything works here. So now the surprising thing that I'd like to show you that you may
 not expect is that this input here that is being multiplied, doesn't actually have to be two-dimensional.
 This matrix multiply operator and PyTorch is quite powerful. And in fact, you can actually
 pass in higher dimensional arrays, tensors, and everything works fine. So for example,
 this could be four by five by 80. And the result in that case will become four by five by 200.
 You can add as many dimensions as you like on the left here. And so effectively, what's happening
 is that the matrix multiplication only works on the last dimension. And the dimensions before it
 in the input tensor are left unchanged. So that is basically these, these dimensions on the left
 are all treated as just a batch dimension. So we can have multiple batch dimensions,
 and then in parallel over all those dimensions, we are doing the matrix multiplication on the last
 dimension. So this is quite convenient because we can use that in our network now, because remember
 that we have these eight characters coming in. And we don't want to now flatten all of it out into
 a large eight dimensional vector, because we don't want to a matrix multiply 80 into a weight matrix
 multiply immediately. Instead, we want to group these like this. So every consecutive two elements,
 one, two, and three, and four, and five, and six, and seven, and eight, all of these should be now
 basically flattened out and multiplied by a weight matrix. But all of these four groups
 here, we'd like to process in parallel. So it's kind of like a batch dimension that we can introduce.
 And then we can in parallel, basically process all of these, uh, bygram groups in the four batch
 dimensions of an individual example, and also over the actual batch dimension of the, you know,
 four examples in our example here. So let's see how that works. Effectively, what we want is right now,
 we take a four by 80 and multiply by eight by 200. Two in the linear layer, this is what happens.
 But instead of what we want is we don't want 80 characters or 80 numbers to come in. We only
 want two characters to come in on the very first layer. And those two characters should be fused.
 So in other words, we just want 20 to come in, right? 20 numbers would come in. And here,
 we don't want a four by 80 to feed into the linear layer. We actually want these groups of two to
 feed in. So instead of four by 80, we want this to be a four by four by 20. So these are the four
 groups of two, and each one of them is 10 dimensional vector. So what we want is now is we need to
 change the flatten layer. So it doesn't output a four by 80, but it outputs a four by four by 20,
 where basically these, um, every two consecutive characters are packed in on the very last dimension.
 And then these four is the first batch dimension. And this four is the second batch dimension,
 referring to the four groups inside every one of these examples. And then this will just multiply
 like this. So this is what we want to get to. So we're going to have to change the linear layer
 in terms of how many inputs it expects. It shouldn't expect expect 80. It should just expect 20 numbers.
 And we have to change our flatten layer. So it doesn't just fully flatten out this entire example.
 It needs to create a four by four by 20, instead of a four by 80. So let's see how this could be
 implemented. Basically, right now we have an input that is a four by eight by 10, that feeds into the
 flatten layer. And currently, the flatten layer just stretches it out. So if you remember the
 implementation of flatten, it takes our X and it just views it as whatever the batch dimension is
 and then negative one. So effectively, what it does right now is it does ew of four negative one,
 and the shape of this, of course, is four by eight. So that's what currently happens. And we instead
 want this to be a four by four by 20, where these consecutive 10 dimensional vectors get concatenated.
 So you know how when Python, you can take a list of range of 10. So we have numbers from zero to nine.
 And we can index like this to get all the even parts. And we can also index like starting at one
 and going in steps up to get all the odd parts. So one way to implement this, it would be as follows.
 We can take E and we can index into it for all the batch elements, and then just even elements in
 this dimension. So at index zero to four and eight. And then all the parts here from this last
 dimension. And this gives us the even characters. And then here, this gives us all the odd characters.
 And basically what we want to do is we make sure we want to make sure that these get concatenated
 impact torch. And then we want to concatenate these two tensors along the second dimension.
 So this and the shape of it would be four by four by 20. This is definitely the result we want.
 We are explicitly grabbing the even parts and the odd parts. And we're arranging those four by four
 by 10 right next to each other and concatenate. So this works. But it turns out that what also works
 is you can simply use a view again and just request the right shape. And it just so happens that in
 this case, those vectors will again end up being arranged in exactly the way we want. So in particular,
 if we take E and we just view it as a four by four by 20, which is what we want, we can check that
 this is exactly equal to what let me call this, this is the explicit concatenation, I suppose.
 So explicit dot shape is for by four by 20. If you just view it as for by four by 20,
 you can check that when you compare to explicit, you get a bit, this is element wise operation.
 So making sure that all of them are true, a value is the true. So basically long story short,
 we don't need to make an explicit call to concatenate, etc. We can simply take this
 input tensor to flatten and we can just view it in whatever way we want. And in particular,
 we don't want to stretch things out with negative one. We want to actually create a three dimensional
 array. And depending on how many vectors that are consecutive, we want to fuse, like for example,
 two, then we can just simply ask for this dimension to be 20. And use a negative one here,
 and by to figure out how many groups it needs to pack into this additional batch dimension.
 So let's now go into flatten and implement this. Okay, so I scroll up here to flatten.
 And what we'd like to do is we'd like to change it now. So let me create a constructor
 and take the number of elements that are consecutive that we would like to concatenate now in the last
 dimension of the output. So here, we're just going to remember, solve that and equals n. And then I
 want to be careful here because pytorch actually has a torched up flatten. And its keyword arguments
 are different, and they kind of like function differently. So our flatten is going to start to
 depart from pytorch flatten. So let me call it flatten consecutive or something like that,
 just to make sure that our APIs are about equal. So this basically flattens only some
 and consecutive elements and puts them into the last dimension. Now here, the shape of X is B by
 T by C. So let me pop those out into variables, and recall that in our example down below,
 B was four, T was eight, and C was 10. Now instead of doing X dot view of B by negative one,
 right, this is what we had before. We want this to be B by negative one by,
 and basically here, we want C times n. That's how many consecutive elements we want.
 And here instead of negative one, I don't super love to use up negative one because
 I like to be very explicit so that you get error messages when things don't go according to your
 expectation. So what do we expect here? We expect this to become T divide and using integer division
 here. So that's what I expect to happen. And then one more thing I want to do here is remember
 previously, all the way in the beginning, n was three, and basically we're concatenating
 all the three characters that existed there. So we basically concatenated everything.
 And so sometimes that can create a spurious dimension of one here. So if it is the case that
 X dot shape at one is one, then it's kind of like a spurious dimension. So we don't want to return
 a three dimensional tensor with a one here, we just want to return a two dimensional tensor
 exactly as we did before. So in this case, basically, we will just say X equals X dot squeeze.
 That is a pytorch function. And squeeze takes a dimension that it either squeezes out all the
 dimensions of a tensor that are one, or you can specify the exact dimension that you want to be
 squeezed. And again, I like to be as explicit as possible always. So I expect to squeeze out the
 first dimension only of this tensor, this three dimensional tensor. And if this dimension here is
 one, then I just want to return B by C times n. And so self dot out will be X. And then we return
 self dot out. So that's the candidate implementation. And of course, this should be self dot in
 instead of just n. So let's run. And let's come here now. And take it for a spin. So flatten consecutive.
 And in the beginning, let's just use eight. So this should recover the previous behavior. So
 flatten consecutive of eight, which is the current block size. You can do this. That should recover
 the previous behavior. So we should be able to run the model. And here we can inspect, I have a
 old code snippet here, where I iterate over all the layers, I print the name of this class,
 and the shape. And so we see the shapes as we expect them after every single layer and it's
 output. So now let's try to restructure it using our flatten consecutive and do it heroically. So in
 particular, we want to flatten consecutive, not just not block size, but just to. And then we want
 to process this with linear. Now, then the number of inputs to this linear will not be an embed
 times block size. It will now only be an embed times two, 20. This goes through the first layer.
 And now we can in principle, just copy paste this. Now the next linear layer should expect
 and hidden times two. And the last piece of it should expect and it intends to again.
 So this is sort of like the Ne version of it. So running this, we now have a much, much bigger
 model. And we should be able to basically just forward the model. And now we can inspect the
 numbers in between. So four by 20 was flatten consecutive, and then four by 20. This was projected
 into four by four by 200. And then bash arm just worked out of the box. We have to verify that
 bash arm does the correct thing, even though it takes a three dimensional embed instead of two
 dimensional input. Then we have 10 H, which is element wise, then we crushed it again. So
 flatten consecutively and ended up with the four by two by 400. Now, then linear brought it back
 down to 200 bash arm 10 H. And lastly, we get a four by 400. And we see that the flatten consecutive
 for the last flatten here, it squeezed out that dimension of one. So we only ended up with four
 by 400. And then linear bash arm 10 H. And the last linear layer to get our logits. And so the
 logis end up in the same shape as they were before. But now we actually have a nice three layer
 neural net. And it basically corresponds to whoops, sorry, it basically corresponds exactly to this
 network now, except on this piece here, because we only have three layers. Whereas here in this
 example, there's four layers, with a total receptive field size of 16 characters, instead of just
 eight characters, the block size here is 16. So this piece of it is basically implemented here.
 Now we just have to kind of figure out some good channel numbers to use here. Now in particular,
 I changed the number of hidden units to be 68 in this architecture, because when I use 68,
 the number of parameters comes out to be 22,000. So that's exactly the same that we had before.
 And we have the same amount of capacity at this neural net in terms of the number of parameters.
 But the question is whether we are utilizing those parameters in a more efficient architecture.
 So what I did then is I got rid of a lot of the debugging cells here. And I rerun the optimization
 and scrolling down to the result, we see that we get the identical performance roughly.
 So our validation loss now is 2.029, and previously it was 2.027. So controlling for
 number of parameters, changing from the flat to here, Google is not giving us anything yet.
 That said, there are two things to point out. Number one, we didn't really torture the architecture
 here very much. This is just my first guess. And there's a bunch of hyperparameters searched
 that we could do in order in terms of how we allocate our budget of parameters to what layers.
 Number two, we still may have a bug inside the Bashroom 1D layer. So let's take a look at
 that because it runs, how does it do the right thing? So I pulled up the layer inspector sort of
 that we have here and printed out the shape along the way. And currently it looks like the
 Bashroom is receiving an input that is 32 by 4 by 68. And here on the right, I have the current
 implementation of Bashroom that we have right now. Now this Bashroom assumed in the way we wrote it,
 and at the time that X is two-dimensional. So it was n by d, where n was the backed size. So that's
 why we only reduced the mean and the variance over the zero-th dimension. But now X will basically
 become three-dimensional. So what's happening inside the Bashroom layer right now and how come
 it's working at all and not giving any errors? The reason for that is basically because everything
 broadcasts properly, but the Bashroom is not doing what we wanted to do. So in particular, let's
 basically think through what's happening inside the Bashroom, looking at what's happening here.
 I have the code here. So we're receiving an input of 32 by 4 by 68. And then we are doing
 here, X dot mean, here I have E instead of X. But we're doing the mean over zero. And that's
 actually giving us one by four by 68. So we're doing the mean only over the very first dimension.
 And it's giving us a mean and a variance that still maintain this dimension here. So these means
 are only taken over 32 numbers in the first dimension. And then when we perform this, everything
 broadcasts correctly still. But basically what ends up happening is when we also look at the running mean,
 the shape of it. So I'm looking at the model that layers that three, which is the first Bashroom
 layer. And then looking at whatever the running mean became, and its shape, the shape of this
 running mean now is one by four by 68. Right? Instead of it being, you know, just size of dimension,
 because we have 68 channels, we expect to have 68 means and variances that we're maintaining.
 But actually, we have an array of four by 68. And so basically, what this is telling us is
 this Bashroom is only this Bashroom is currently working in parallel over
 four times 68, instead of just 68 channels. So basically, we are maintaining statistics for
 every one of these four positions individually and independently. And instead, what we want to do
 is we want to treat this for as a batch dimension, just like the zero dimension. So as far as the
 Bashroom is concerned, it doesn't want to average, we don't want to average over 32 numbers. We want
 to now average over 32 times four numbers for every single one of these 68 channels. And so let me
 now remove this. It turns out that when you look at the documentation of torch dot mean,
 so let's go to torch dot mean.
 In one of its signatures, when we specify the dimension, we see that the dimension here is not
 just it can be in or it can also be a tuple of ints. So we can reduce over multiple integers at
 the same time over multiple dimensions at the same time. So instead of just reducing over zero,
 we can pass in a tuple 01. And here 01 as well. And then what's going to happen is the output,
 of course, is going to be the same. But now what's going to happen is because we reduce over 0 and 1,
 if we look at in that shape, we see that now we've reduced, we took the mean over both the
 zero and the first dimension. So we're just getting 68 numbers and a bunch of spurious
 dimensions here. So now this becomes one by one by 68. And the running mean and the running
 variance analogously will become one by one by 68. So even though there are these spurious
 dimensions, the current the current the correct thing will happen in that we are only maintaining
 means and variances for 64, sorry, for 68 channels. And we're not calculating the mean
 variance across 32 times four dimensions. So that's exactly what we want. And let's change the
 implementation of batch number one D that we have so that it can take in two dimensional or
 three dimensional inputs and perform accordingly. So at the end of the day, the fix is relatively
 straightforward. Basically, the dimension we want to reduce over is either zero or the tuple 0 and 1,
 depending on the dimensionality of X. So if X dot and dim is two, so it's a two dimensional tensor,
 then dimension we want to reduce over is just the integer zero. L of X dot and dim is three,
 so it's a three dimensional tensor, then the dims, we're going to assume are zero and one that
 we want to reduce over. And then here we just pass in dim. And if the dimensionality of X is
 anything else, we'll now get an error, which is good. So that should be the fix. Now I want
 to point out one more thing. We're actually departing from the API of PyTorch here a little bit,
 because when you come to batch room on D and PyTorch, you can scroll down and you can see that the
 input to this layer can either be N by C, where N is the batch size and C is the number of features
 or channels, or it actually does accept three dimensional inputs, but it expects it to be N by
 C by L, where L is say like the sequence length or something like that. So this is a problem because
 you see how C is nested here in the middle. And so when it gets three dimensional inputs,
 this batch room layer will reduce over zero and two instead of zero and one. So basically,
 PyTorch batch norm one D layer assumes that C will always be the first dimension, whereas we
 assume here that C is the last dimension, and there are some number of batch dimensions beforehand.
 And so it expects N by C or N by C by L, we expect N by C or N by L by C.
 And so it's a deviation. I think it's okay. I prefer it this way, honestly. So this is the way
 that we will keep it for our purposes. So I redefined the layers, re-initialized the neural
 nut, and did a single forward pass with a break just for one step. Looking at the shapes along the
 way, they're of course identical. All the shapes are the same. By the way, we see that things are
 actually working as we want them to now, is that when we look at the batch room layer,
 the running mean shape is now one by one by 68. So we're only maintaining 68 means for every one of
 our channels, and we're treating both the 0th and the first dimension as a batch dimension,
 which is exactly what we want. So let me retrain the neural nut now. Okay, so I retrained the
 neural nut with the bug fix, we get a nice curve, and when we look at the validation performance,
 we do actually see a slight improvement. So it went from 2.0 to 9 to 2.0 to 2. So basically,
 the bug inside the batch room was holding us back like a little bit it looks like,
 and we are getting a tiny improvement now, but it's not clear if this is statistically significant.
 And the reason we slightly expect an improvement is because we're not maintaining so many different
 means and variances that are only estimated using 32 numbers effectively. Now we are estimating
 them using 32 times four numbers. So you just have a lot more numbers that go into any one
 estimate of the mean and variance. And it allows things to be a bit more stable and less wiggly
 inside those estimates of those statistics. So pretty nice. With this more general architecture
 in place, we are now set up to push the performance further by increasing the size of the network.
 So for example, I bumped up the number of embeddings to 24 instead of 10, and also increased number
 of hidden units. But using the exact same architecture, we now have 76,000 parameters,
 and the training takes a lot longer. But we do get a nice curve. And then when you actually
 evaluate the performance, we are now getting validation performance of 1.993. So we've crossed
 over the 2.0 sort of territory. And we're at about 1.99, but we are starting to have to
 wait quite a bit longer. And we're a little bit in the dark with respect to the correct setting
 of the hyper parameters here and the learning rates and so on, because the experiments are
 starting to take longer to train. And so we are missing sort of like an experimental harness
 on which we could run a number of experiments and really tune this architecture very well.
 So I'd like to conclude now with a few notes. We basically improved our performance from a
 starting of 2.1 down to 1.9. But I don't want that to be the focus because honestly, we're kind of
 in the dark. We have no experimental harness. We're just guessing and checking. And this whole
 thing is terrible. We're just looking at the training loss. Normally you want to look at both
 the training and the validation loss together. The whole thing looks different if you're actually
 trying to squeeze out numbers. That said, we did implement this architecture from the WaitNet paper,
 but we did not implement this specific forward pass-off it where you have a more complicated
 linear layer sort of that is this gated linear layer kind of. And there's residual connections
 and skip connections and so on. So we did not implement that. We just implemented this structure.
 I would like to briefly hint or preview how what we've done here relates to convolutional neural
 networks as used in the WaitNet paper. And basically the use of convolutions is strictly for efficiency.
 It doesn't actually change the model we've implemented. So here, for example, let me look at a specific
 name to work with an example. So there's a name in our training set and it's DeAndre. And it has
 seven letters. So that is eight independent examples in our model. So all these rows here are independent
 examples of DeAndre. Now you can forward of course any one of these rows independently. So I can
 take my model and call it on any individual index. Notice by the way here, I'm being a little bit
 tricky. The reason for this is that extra at seven dot shape is just one dimensional array of eight.
 So you can't actually call the model on it. You're going to get an error because there's no batch
 dimension. So when you do extra at list of seven, then the shape of this becomes one by eight. So I get
 an extra batch dimension of one, and then we can forward the model. So that forwards a single example.
 And you might imagine that you actually may well know forward all of these eight at the same time.
 So preallocating some memory and then doing a for loop H times and forwarding all of those eight
 here will give us all the logits in all these different cases. Now for us with the model as
 we've implemented it right now, this is eight independent calls to our model. But what convolutions
 allow you to do is it allow you to basically slide this model efficiently over the input sequence.
 And so this for loop can be done not outside in Python, but inside of kernels in CUDA. And so
 this for loop gets hidden into the convolution. So the convolution basically you can think of it as
 it's a for loop applying a little linear filter over space of some input sequence. And in our case,
 the space we're interested in is one dimensional, and we're interested in sliding these filters
 over the input data. So this diagram actually is fairly good as well. Basically what we've done
 is here they are highlighting in black one individual one single sort of like tree of this
 calculation. So just calculating the single output example here. And so this is basically what we've
 implemented here. We've implemented a single this black structure. We've implemented that and
 calculated a single output like a single example. But what convolutions allow you to do is it
 allows you to take this black structure and kind of like slide it over the input sequence
 here and calculate all of these orange outputs at the same time. Or here that corresponds to
 calculating all of these outputs of at all the positions of DeAndre at the same time.
 And the reason that this is much more efficient is because number one as I mentioned the for loop
 is inside the CUDA kernels in the sliding. So that makes it efficient. But number two,
 notice the variable reuse here. For example, if we look at this circle, this node here, this node
 here is the right child of this node, but it's also the left child of the node here. And so basically
 this node and its value is used twice. And so right now in this naive way, we'd have to recalculate it.
 But here we are allowed to reuse it. So in the convolutional neural network, you think of these
 linear layers that we have above as filters. And we take these filters and their linear filters,
 and you slide them over input sequence. And we calculate the first layer, and then the second layer,
 and then the third layer, and then the output layer of the sandwich. And it's all done very
 efficiently using these convolutions. So we're going to cover that in a future video.
 The second thing I hope you took away from this video is you've seen me basically implement all of
 these layer Lego building blocks or module building blocks. And I'm implementing them over here.
 And we've implemented a number of layers together. And we've also implementing these
 these containers. And we've overall pytorchified our code quite a bit more. Now basically what we're
 doing here is we're re-implementing Torched.nn, which is the neural networks library on top of
 Torched.tensor. And it looks very much like this, except it is much better, because it's in pytorch.
 Instead of jinkling my jinkling notebook. So I think going forward, I will probably have
 considered us having unlocked Torched.nn. We understand roughly what's in there, how these
 modules work, how they're nested, and what they're doing on top of Torched.tensor. So hopefully we'll
 just switch over and continue and start using Torched.nn directly. The next thing I hope you got a bit
 of a sense of is what the development process of building deep neural networks looks like,
 which I think was relatively representative to some extent. So number one, we are spending a lot
 of time in the documentation page of pytorch. And we're reading through all the layers looking at
 documentation, where are the shapes of the inputs, what can they be, what does the layer do, and so
 on. Unfortunately, I have to say the pytorch documentation is not very good. They spend a ton of time on
 hardcore engineering of all kinds of distributed primitives, etc. But as far as I can tell, no one
 is maintaining documentation. It will lie to you, it will be wrong, it will be incomplete, it will
 be unclear. So unfortunately, it is what it is, and you just kind of do your best with what they've
 given us. Number two, the other thing that I hope you got a sense of is there's a ton of trying to
 make the shapes work. And there's a lot of gymnastics around these multi-dimensional arrays. And are
 they two dimensional, three dimensional, four dimensional, what layers take what shapes is it
 NCL or NLC? And you're permuting and viewing, and it just can get pretty messy. And so that brings
 me to number three, I very often prototype these layers and implementations in Jupyter Notebooks,
 and make sure that all the shapes work out. And I'm spending a lot of time basically babysitting
 the shapes and making sure everything is correct. And then once I'm satisfied with the functionality
 in a Jupyter Notebook, I will take that code and copy paste it into my repository of actual code
 that I'm training with. And so then I'm working with VS code on the side. So I usually have Jupyter
 Notebook and VS code, I develop a Jupyter Notebook, I paste into VS code, and then I kick off experiments
 from from the repo, of course, from the code repository. So that's roughly some notes on the
 development process of working with neural ones. Lastly, I think this lecture unlocks a lot of
 potential further lectures, because number one, we have to convert our neural network to actually
 use these dilated causal convolutional layers. So implementing the comment. Number two, I potentially
 starting to get into what this means, where our residual connections and skip connections,
 and why are they useful. Number three, we, as I mentioned, we don't have any experimental
 harness. So right now I'm just guessing checking everything. This is not representative of typical
 deep learning workflows. You have to set up your evaluation harness, you can kick off experiments,
 you have lots of arguments that your script can take, you're kicking off a lot of experimentation,
 you're looking at a lot of plots of training and validation losses, and you're looking at
 what is working and what is not working. And you're working on this population level,
 and you're doing all these hyperparameter searches. And so we've done none of that so far.
 So how to set that up and how to make it good, I think is a whole another topic.
 And number three, we should probably cover recurrent neural networks,
 our NENS, LSTMs, groups, and of course, transformers. So many places to go, and we'll cover that in
 the future. For now, sorry, I forgot to say that if you are interested, I think it is kind of
 interesting to try to beat this number 1.993. Because I really haven't tried a lot of experimentation
 here, and there's quite a bit of long fruit potentially to still purchase further. So I
 haven't tried any other ways of allocating these channels in this neural net, maybe the number of
 dimensions for the embedding is all wrong. Maybe it's possible to actually take the original
 network with just one hidden layer and make it big enough and actually beat my fancy hierarchical
 network. It's not obvious. It'll be kind of embarrassing if this did not do better, even once
 you torture it a little bit. Maybe you can read the WaitNet paper and try to figure out how some of
 these layers work and implement them yourselves using what we have. And of course, you can always
 tune some of the initialization or some of the optimization and see if you can improve it that
 way. So I'd be curious if people can come up with some ways to beat this. And yeah, that's it for now.
 Bye.
 Hi everyone. So by now you have probably heard of chat GPT. It has taken the world and AI community
 by storm and it is a system that allows you to interact with an AI and give it text-based tasks.
 So for example, we can ask chat GPT to write us a small haiku about how important it is that people
 understand AI and then they can use it to improve the world and make it more prosperous. So when we
 run this, AI knowledge brings prosperity for all to see embraces power. Okay, not bad. And so you
 could see that chat GPT went from left to right and generated all these words sequentially.
 Now I asked it already the exact same prompt a little bit earlier and it generated a slightly
 different outcome. AI's power to grow, ignore insults us back, learn, prosperity, weights.
 So pretty good in both cases and slightly different. So you can see that chat GPT is a probabilistic
 system and for any one prompt it can give us multiple answers sort of replying to it.
 Now this is just one example of a prompt. People have come up with many, many examples and there
 are entire websites that index interactions with chat GPT. And so many of them are quite humorous,
 explain HTML to me like I'm a dog, write release notes for chess too, write a note about Elon Musk
 buying a Twitter and so on. So as an example, please write a breaking news article about a leaf
 falling from a tree and a shocking turn of events. A leaf has fallen from a tree in the local
 park. Witnesses report that the leaf which was previously attached to a branch of a tree
 detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty
 remarkable system and it is what we call a language model because it models the sequence of words
 or characters or tokens more generally and it knows how sort of words follow each other in
 English language. And so from its perspective what it is doing is it is completing the sequence.
 So I give it the start of a sequence and it completes the sequence with the outcome.
 And so it's a language model in that sense. Now I would like to focus on the under the hood of
 under the hood components of what makes chat GPT work. So what is the neural network under the
 hood that models the sequence of these words. And that comes from this paper called Attention is
 All You Need. In 2017 a landmark paper a landmark paper in AI that proposed the transformer architecture.
 So GPT is short for generatively pre-trained transformer. So transformer is the neural
 nut that actually does all the heavy lifting under the hood. It comes from this paper in 2017.
 Now if you read this paper this reads like a pretty random machine translation paper and that's
 because I think the authors didn't fully anticipate the impact that the transformer would have on the
 field. And this architecture that they produced in the context of machine translation in their case
 actually ended up taking over the rest of AI in the next five years after. And so this architecture
 with minor changes was copy-pasted into a huge amount of applications in AI in more recent years.
 And that includes at the core of chat GPT. Now we are not going to what I'd like to do now is
 I'd like to build out something like chat GPT. But we're not going to be able to of course
 reproduce chat GPT. This is a very serious production grade system. It is trained on
 a good chunk of internet. And then there's a lot of pre-training and fine-tuning stages to it.
 And so it's very complicated. What I'd like to focus on is just to train a transformer based
 language model. And in our case it's going to be a character level language model. I still think
 that is a very educational with respect to how these systems work. So I don't want to train on
 the chunk of internet. We need a smaller data set. In this case I propose that we work with my
 favorite toy data set. It's called tiny Shakespeare. And what it is is basically it's a concatenation
 of all of the works of Shakespeare in my understanding. And so this is all of Shakespeare in a single
 file. This file is about one megabyte. And it's just all of Shakespeare. And what we are going to do
 now is we're going to basically model how these characters follow each other. So for example,
 given a chunk of these characters like this, given some context of characters in the past,
 the transformer neural network will look at the characters that I've highlighted and it's going
 to predict that G is likely to come next in the sequence. And it's going to do that because we're
 going to train that transformer on Shakespeare. And it's just going to try to produce character
 sequences that look like this. And in that process is going to model all the patterns inside this
 data. So once we've trained the system, I just like to give you a preview, we can generate infinite
 Shakespeare. And of course it's a fake thing that looks kind of like Shakespeare. Apologies for
 there's some jank that I'm not able to resolve in here. But you can see how this is going character
 by character. And it's kind of like predicting Shakespeare-like language. So verily my lord,
 the sights have left the again, the king coming with my curses with precious pale. And then
 Tranios says something else, etc. And this is just coming out of the transformer in a very similar
 manner, as it would come out in chat GPT. In our case, character by character in chat GPT,
 it's coming out on the token by token level. And tokens are these sort of like little subword
 pieces. So they're not word level, they're kind of like work chunk level. And now I've already
 written this entire code to train these transformers. And it is in a GitHub repository that you can
 find. And it's called nano GPT. So nano GPT is a repository that you can find on my GitHub.
 And it's a repository for training transformers on any given text. And what I think is interesting
 about it, because there's many ways to train transformers, but this is a very simple implementation.
 So it's just two files of 300 lines of code, each one file defiles the GPT model, the transformer,
 and one file trains it on some given text data set. And here I'm showing that if you train it on
 the open WebText data set, which is a fairly large data set of web pages, then I reproduce the
 the performance of GPT two. So GPT two is an early version of open AI's GPT from 2017,
 if I recall correctly. And I've only so far reproduced the smallest one 24 million parameter model.
 But basically, this is just proving that the code base is correctly arranged. And I'm able to load
 the neural network weights that open AI has released later. So you can take a look at the
 finished code here in nano GPT. But what I would like to do in this lecture is I would like to
 basically write this repository from scratch. So we're going to begin with an empty file,
 and we're going to define a transformer piece by piece. We're going to train it on the tiny
 Shakespeare data set. And we'll see how we can then generate infinite Shakespeare. And of course,
 this can copy paste to any arbitrary text data set that you like. But my goal really here is to
 just make you understand and appreciate how under the chat GPT works. And really, all that's required
 is a proficiency in Python, and some basic understanding of copula statistics. And it would help if you
 also see my previous videos on the same YouTube channel, in particular, my make more series,
 where I define smaller and simpler neural network language models. So multi-layer perceptions and so
 on, it really introduces the language modeling framework. And then here in this video, we're
 going to focus on the transformer neural network itself. Okay, so I created a new Google collab
 Jupyter notebook here. And this will allow me to later easily share this code that we're going to
 develop together with you so you can follow along. So this will be in a video description later.
 Now here, I've just done some preliminaries. I downloaded the data set, the tiny Shakespeare data
 set at this URL. And you can see that it's about a one megabyte file. Then here, I open the input.txt
 file and just read in all the text of the string. And we see that we are working with one million
 characters roughly. And the first 1000 characters, if we just print them out, are basically what you
 would expect. This is the first 1000 characters of the tiny Shakespeare data set roughly up to here.
 So so far, so good. Next, we're going to take this text. And the text is a sequence of characters
 in Python. So when I call the set constructor on it, I'm just going to get the set of all the
 characters that occur in this text. And then I call list on that to create a list of those
 characters instead of just a set so that I have an ordering an arbitrary ordering. And then I sort
 that. So basically, we get just all the characters that occur in the entire data set and they're sorted.
 Now the number of them is going to be our vocabulary size. These are the possible elements of our
 sequences. And we see that when I print here, the characters, there's 65 of them in total. There's
 a space character, and then all kinds of special characters, and then capitals and lowercase letters.
 So that's our vocabulary. And that's the sort of like possible characters that the model can see
 or emit. Okay, so next, we would like to develop some strategy to tokenize the input text. Now,
 when people say tokenize, they mean convert the raw text as a string to some sequence of integers
 according to some note, according to some vocabulary of possible elements. So as an example, here,
 we are going to be building a character level language model. So we're simply going to be
 translating individual characters into integers. So let me show you a chunk of code that sort of
 does that for us. So we're building both the encoder and the decoder. And let me just talk
 through what's happening here. When we encode an arbitrary text, like hi there, we're going to
 receive a list of integers that represents that string. So for example, 46 47, etc. And then we
 also have the reverse mapping. So we can take this list and decode it to get back the exact same
 string. So it's really just like a translation to integers and back for arbitrary string. And for us,
 it is done on a character level. Now the way this was achieved is we just iterate over all the
 characters here and create a lookup table from the character to the integer and vice versa. And then
 to encode some string, we simply translate all the characters individually and to decode it back,
 we use the reverse mapping concatenate all it. Now this is only one of many possible encodings
 or many possible sort of tokenizers. And it's a very simple one. But there's many other
 schemas that people have come up with in practice. So for example, Google uses a sentence piece.
 So sentence piece will also encode text into integers, but in a different schema, and using a
 different vocabulary. And sentence piece is a sub word sort of tokenizer. And what that means
 is that you're not encoding entire words, but you're not also encoding individual characters.
 It's it's a sub word unit level. And that's usually what's adopted in practice. For example,
 also OpenAI has this library called tick token that uses a byte pair encoding tokenizer.
 And that's for GPT uses. And you can also just encode words into like hell world into lists of
 integers. So as an example, I'm using the tick token library here. I'm getting the encoding from
 GPT two, or that was used for GPT two, instead of just having 65 possible characters or tokens,
 they have 50,000 tokens. And so when they encode the exact same string high there,
 we only get a list of three integers. But those integers are not between zero and 64,
 they are between zero and 5000 50,000 256. So basically, you can trade off the codebook size
 and the sequence lengths. So you can have very long sequences of integers with very small
 vocabularies, or we can have short sequences of integers with very large vocabularies. And so
 typically people use in practice these sub word encodings. But I'd like to keep our tokenizer
 very simple. So we're using character level tokenizer. And that means that we have very
 small codebooks. We have very simple encode and decode functions. But we do get very long
 sequences as a result. But that's the level at which we're going to stick with this lecture,
 because it's the simplest thing. Okay, so now that we have an encoder and a decoder,
 effectively tokenizer, we can tokenize the entire training set of Shakespeare. So here's a chunk of
 code that does that. And I'm going to start to use the pytorch library, and specifically the
 torque dot tensor from the pytorch library. So we're going to take all of the text in tiny
 Shakespeare, encode it, and then wrap it into a torch dot tensor to get the data tensor. So here's
 what the data tensor looks like when I look at just the first 1000 characters or the 1000 elements
 of it. So we see that we have a massive sequence of integers. And this sequence of integers here
 is basically an identical translation of the first 1000 characters here. So I believe, for
 example, that zero is a new line character. And maybe one is a space, not 100% sure. But from now
 on, the entire data set of text is re represented as just it just stretched out as a single very
 large sequence of integers. Let me do one more thing before we move on here. I'd like to separate
 out our data set into a train and a validation split. So in particular, we're going to take the
 first 90% of the data set and consider that to be the training data for the transformer.
 And we're going to withhold the last 10% at the end of it to be the validation data. And this will
 help us understand to what extent our model is overfitting. So we're going to basically hide
 and keep the validation data on the side, because we don't want just a perfect memorization of this
 exact Shakespeare, we want a neural network that sort of creates Shakespeare like text. And so it
 should be fairly likely for it to produce the actual like stowed away true Shakespeare text.
 And so we're going to use this to get a sense of the overfitting. Okay, so now we would like to
 start plugging these text sequences or integer sequences into the transformer so that it can train
 and learn those patterns. Now, the important thing to realize is we're never going to actually feed
 entire text into your transformer all at once. That would be computationally very expensive and
 prohibitive. So when we actually train a transformer on a lot of these data sets, we only work with
 chunks of the data set. And when we train the transformer, we basically sample random little
 chunks out of the training set and train on just chunks at a time. And these chunks have basically
 some kind of a length and some maximum length. Now, the maximum length typically, at least in the
 code, I usually write is called block size. You can you can find it on the different names,
 like context length or something like that. Let's start with the block size of just eight. And
 let me look at the first train data characters, the first block size plus one characters. I'll
 explain why plus one in a second. So this is the first nine characters in the sequence, in the
 training set. Now, what I'd like to point out is that when you sample a chunk of data like this,
 so say, if these nine characters out of the training set, this actually has multiple examples
 packed into it. And that's because all of these characters follow each other. And so what this
 thing is going to say when we plug it into a transformer is we're going to actually simultaneously
 train it to make prediction at every one of these positions. Now, in the in a chunk of nine
 characters, there's actually eight individual examples packed in there. So there's the example
 that when 18, when in the context of 18, 47 luckily comes next, in a context of 18 and 47,
 56 comes next. In a context of 18, 47, 56, 57 can come next and so on. So that's the eight
 individual examples. Let me actually spell it out with code. So here's a chunk of code to illustrate.
 X are the inputs to the transformer. It will just be the first block size characters.
 Y will be the next block size characters. So it's offset by one. And that's because Y
 are the targets for each position in the input. And then here I'm iterating over all the block
 size of eight. And the context is always all the characters in X up to T and including T.
 And the target is always the teeth character, but in the targets array Y. So let me just run this.
 And basically it spells out what I said in words. These are the eight examples hidden in a chunk
 of nine characters that we sampled from the training set. I want to mention one more thing.
 We train on all the eight examples here with context between one all the way up to context of
 block size. And we train on that not just for computational reasons, because we happen to have
 the sequence already or something like that. It's not just just for efficiency. It's also done to
 make the transformer network be used to seeing contexts all the way from as little as one all
 the way to block size. And we'd like the transformer to be used to seeing everything in between. And
 that's going to be useful later during inference, because while we're sampling, we can start to
 sampling generation with as little as one character of context. And then transformer knows how to
 predict next character with all the way up to just context of one. And so then it can predict
 everything up to block size. And after block size, we have to start truncating, because the
 transformer will never receive more than block size inputs when it's predicting the next character.
 Okay, so we've looked at the time dimension of the tensors that are going to be feeding into
 the transformer. There's one more dimension to care about, and that is the batch dimension.
 And so as we're sampling these chunks of text, we're going to be actually every time we're going
 to feed them into a transformer, we're going to have many batches of multiple chunks of text that
 are always like stacked up in a single tensor. And that's just done for efficiency, just so that
 we can keep the GPUs busy, because they are very good at parallel processing of data. And so we
 just want to process multiple chunks all at the same time. But those chunks are processed completely
 independently. They don't talk to each other, and so on. So let me basically just generalize this and
 introduce a batch dimension. Here's a chunk of code. Let me just run it, and then I'm going to
 explain what it does. So here, because we're going to start sampling random locations in the data
 sets to pull chunks from, I am setting the seed so that in the random number generator, so that
 the numbers I see here are going to be the same numbers you see later, if you try to reproduce this.
 Now the batch size here is how many independent sequences we are processing every forward,
 backward, pass of the transformer. The block size, as I explained, is the maximum context length
 to make those predictions. So let's say by size four, block size eight, and then here's how we get
 batch for any arbitrary split. If the split is a training split, then we're going to look at
 train data, otherwise, and val data. That gives us the data array. And then when I generate random
 positions to grab a chunk out of, I actually grab, I actually generate batch size number of random
 offsets. So because this is for, we are, I X is going to be a four numbers that are randomly
 generated between zero and len of data minus block size. So it's just random offsets into the training
 set. And then X is, as I explained, are the first block size characters starting at I.
 The Y's are the offset by one of that. So just add plus one. And then we're going to get those
 chunks for every one of integers I in I X, and use a torch dot stack to take all those
 one dimensional tensors as we saw here. And we're going to stack them up at rows.
 And so they all become a row in a four by eight tensor. So here's where I'm printing them.
 When I sample a batch XB and YB, the inputs the transformer now are the input X is the four by eight
 tensor for rows of eight columns. And each one of these is a chunk of the training set.
 And then the targets here are in the associated array Y, and they will come in to the transformer
 all the way at the end to create the loss function. So they will give us the correct answer for
 every single position inside X. And then these are the four independent rows. So spelled out,
 as we did before, this four by eight array contains a total of 32 examples. And they're
 completely independent as far as the transformers concerned. So when the input is 24, the target
 is 43, or rather 43 here in the Y array, when the input is 24, 43, the target is 58. When the input
 is 24, 43, 58, the target is five, etc. Or like when it is a 52, 58, one, the target is 58.
 Right. So you can sort of see this spelled out. These are the 32 independent examples
 packed in to a single batch of the input X. And then the desired targets are in Y.
 And so now this integer tensor of X is going to feed into the transformer. And that transformer
 is going to simultaneously process all these examples, and then look up the correct integers
 to predict in every one of these positions in the tensor Y. Okay, so now that we have our batch
 of input that we'd like to feed into a transformer, let's start basically feeding this into neural
 networks. Now we're going to start off with the simplest possible neural network, which in the
 case of language modeling, in my opinion, is the by Graham language model. And we've covered the
 by Graham language model in my make more series in a lot of depth. And so here I'm going to sort
 of go faster and let's just implement PyTorch module directly that implements the by Graham
 language model. So I'm importing the pytorch and end module for reproducibility. And then here
 I'm constructing a by Graham language model, which is a subclass of an end module. And then I'm
 calling it and I'm passing in the inputs and the targets. And I'm just printing. Now when the
 inputs and targets come here, you see that I'm just taking the index, the inputs X here, which I
 renamed to ID X. And I'm just passing them into this token embedding table. So what's going on here
 is that here in the constructor, we are creating a token embedding table. And it is of size vocab
 size by vocab size. And we're using an end up embedding, which is a very thin wrapper around
 basically a tensor of shape vocab size by vocab size. And what's happening here is that when we pass
 ID X here, every single integer in our input is going to refer to this embedding table and is going
 to pluck out a row of that embedding table corresponding to its index. So 24 here, we'll go to the
 embedding table and we'll pluck out the 24th row. And then 43 will go here and pluck out the 43rd row,
 etc. And then PyTorch is going to arrange all of this into a batch by time by channel tensor.
 In this case, batch is four, time is eight, and C, which is the channels is vocab size or 65.
 And so we're just going to pluck out all those rows, arrange them in a B by T by C.
 And now we're going to interpret this as the logits, which are basically the scores for the
 next character in a sequence. And so what's happening here is we are predicting what comes next based on
 just the individual identity of a single token. And you can do that because, I mean,
 currently the tokens are not talking to each other, and they're not seeing any context,
 except for they're just seeing themselves. So I'm a, I'm a token number five. And then I can
 actually make pretty decent predictions about what comes next just by knowing that I'm token five,
 because some characters know, sort follow other characters in technical scenarios. So we saw a
 lot of this in a lot more depth in the make more series. And here if I just run this, then we currently
 get the predictions, the scores, the logits for every one of the four by eight positions. Now that
 we've made predictions about what comes next, we'd like to evaluate the loss function. And so in
 make more series, we saw that a good way to measure a loss or like a quality of the predictions
 is to use the negative log likelihood loss, which is also implemented in PyTorch under the name
 cross entropy. So what we'd like to do here is loss is the cross entropy on the predictions
 and the targets. And so this measures the quality of the logits with respect to the targets. In other
 words, we have the identity of the next character. So how well are we predicting the next character
 based on the logits? And intuitively, the correct the correct dimension of logits,
 depending on whatever the target is, should have a very high number. And all the other
 dimensions should be very low number. Right? Now the issue is that this won't actually,
 this is what we want. We want to basically output the logits and the loss.
 This is what we want. But unfortunately, this won't actually run. We get an error message. But
 intuitively, we want to measure this. Now, when we go to the PyTorch cross entropy
 documentation here, we're trying to call the cross entropy in its functional form. So that
 means we don't have to create like a module for it. But here, we go to the documentation,
 you have to look into the details of how PyTorch expects these inputs. And basically,
 the issue here is PyTorch expects if you have multi dimensional input, which we do,
 because we have a B by T by C tensor, then it actually really wants the channels to be
 the second dimension here. So if you'll, so basically, it wants a B by C by T,
 instead of a B by T by C. And so it's just the details of how PyTorch treats these kinds of
 inputs. And so we don't actually want to deal with that. So what we're going to do is that is
 we need to basically reshape our logits. So here's what I like to do. I like to take
 basically give names to the dimensions. So logis dash shape is B by T by C and unpack those numbers.
 And then let's say that logits equals logis dot view. And we want it to be a B times C,
 B times T by C. So just a two dimensional array. Right, so we're going to take all the,
 we're going to take all of these positions here. And we're going to stretch them out in one
 dimensional sequence and preserve the channel dimension as the second dimension.
 So we're just kind of like stretching out the array. So it's two dimensional. And in that case,
 it's going to better conform to what PyTorch sort of expects in its dimensions. Now we have to do
 the same two targets, because currently targets are of shape B by T. And we want it to be just B
 times D. So one dimensional. Now, alternatively, you could always still just do minus one,
 because PyTorch will guess what this should be if you want to lay it out. But let me just be
 explicit and say few times D. Once we reshape this, it will match the cross entropy case.
 And then we should be able to evaluate our loss. Okay, so at that right now, and we can
 do loss. And so currently we see that the loss is 4.87. Now, because our, we have 65 possible
 vocabulary elements, we can actually guess at what the loss should be. And in particular,
 we covered negative log likelihood in a lot of detail, we are expecting log or long of
 one over 65, and negative of that. So we're expecting the loss to be about 4.17, but we're
 getting 4.87. And so that's telling us that the initial predictions are not super diffuse.
 They've got a little bit of entropy. And so we're guessing wrong. So yes, but actually we're
 able to evaluate the loss. Okay, so now that we can evaluate the quality of the model on some data,
 we'd like to also be able to generate from the model. So let's do the generation. Now I'm going
 to go again a little bit faster here, because I covered all this already in previous videos.
 So here's a generate function for the model.
 So we take some, we take the same kind of input IDX here. And basically, this is the current
 context of some characters in a batch, in some batch. So it's also B by T. And the job of generate
 is to basically take this B by T and extend it to be B by T plus one plus two plus three. And so
 it's just basically it continues the generation in all the batch dimensions in the time dimension.
 So that's its job. And we'll do that for max new tokens. So you can see here on the bottom,
 there's going to be some stuff here. But on the bottom, whatever is predicted is concatenated on
 top of the previous IDX along the first dimension, which is the time dimension to create a B by T
 plus one. So that becomes a new IDX. So the job of generate is to take a B by T and make it a B by
 T plus one plus two plus three, as many as we want maximum tokens. So this is the generation
 from the model. Now inside the generation, what are we doing? We're taking the current indices,
 we're getting the predictions. So we get those are the logits. And then the loss here is going to be
 ignored, because we're not we're not using that. And we have no targets that are sort of grown
 truth targets that we're going to be comparing with. Then once we get the logits, we are only
 focusing on the last step. So instead of a B by T by C, we're going to pluck out the negative one,
 the last element in the time dimension, because those are the predictions for what comes next.
 So that gives us the logits, which we then cover to probabilities via softmax. And then we use
 towards that multinomial to sample from those probabilities. And we ask PyTorch to give us one
 sample. And so IDX next will become a B by one, because in each one of the batch dimensions,
 we're going to have a single prediction for what comes next. So this num samples equals one,
 we'll make this be a one. And then we're going to take those integers that come from the sampling
 process, according to the probability distribution given here. And those integers can just concatenate
 it on top of the current sort of like running stream of integers. And this gives us a P by T
 plus one. And then we can return that. Now one thing here is you see how I'm calling
 self of IDX, which will end up going to the forward function. I'm not providing any targets. So
 currently this would give an error because targets is sort of like not given. So targets is to be
 optional. So targets is none by default. And then if targets is none, then there's no loss to create.
 So it's just losses none. But else all of this happens and we can create a loss. So this will
 make it so if we have the targets, we provide them and get a loss. If we have no targets,
 it will just get the logits. So this here will generate from the model. And let's take that for
 a ride now. Oops. So I have another coach on here, which will generate for the model from the model.
 And okay, this is kind of crazy. So maybe let me let me break this down. So these are the IDX,
 right? I'm creating a batch will be just one. Time will be just one. So I'm creating a little
 one by one tensor. And it's holding a zero. And the D type, the data type is integer. So zero is
 going to be how we kick off the generation. And remember that zero is, is the element standing
 for a new line character. So it's kind of like a reasonable thing to to feed in as the very first
 character in a sequence to be the new line. So it's going to be IDX, which we're going to feed in here.
 Then we're going to ask for 100 tokens. And then end that generate will continue that. Now because
 generate works on the level of batches, we then have to index into the zero throw to basically unplug
 the, the single batch dimension that exists. And then that gives us a time steps, just a one
 dimensional array of all the indices, which we will convert to simple Python list from PyTorch
 tensor, so that that can feed into our decode function and convert those integers into text.
 So let me bring this back. And we're generating 100 tokens. Let's run. And here's the generation
 that we achieved. So obviously, as garbage, and the reason it's garbage is because this is totally
 random model. So next up, we're going to want to train this model. Now one more thing I wanted to
 point out here is this function is written to be general. But it's kind of like ridiculous right
 now, because we're feeding in all this, we're building out this context. And we're concatenating
 it all. And we're always feeding it all into the model. But that's kind of ridiculous, because this
 is just a simple by-gram model. So to make, for example, this prediction about k, we only needed
 this w. But actually, what we fed into the model is we fed the entire sequence. And then we only
 looked at the very last piece and predicted k. So the only reason I'm writing it in this way is
 because right now this is a by-gram model. But I'd like to keep this function fixed. And I'd like
 it to work later when our characters actually basically look further in the history. And so
 right now the history is not used. So this looks silly. But eventually the history will be used.
 And so that's why we want to do it this way. So just a quick comment on that. So now we see that
 this is random. So let's train the model. So it becomes a bit less random. Okay, let's now train
 the model. So first what I'm going to do is I'm going to create a PyTorch optimization object.
 So here we are using the optimizer, Adam W. Now in a make more series, we've only ever used
 the cast gradient descent, the simplest possible optimizer, which you can get using the SGD instead.
 But I want to use Adam, which is a much more advanced and popular optimizer. And it works extremely
 well for typical good setting for the learning rate as roughly three in negative four. But for
 very, very small networks, like it's the case here, you can get away with much, much higher
 learning rates, running negative three or even higher, probably. But let me create the optimizer
 object, which will basically take the gradients and update the parameters using the gradients.
 And then here, our batch size up above was only four. So let me actually use something bigger,
 let's say 32. And then for some number of steps, we are sampling a new batch of data.
 We're evaluating the loss. We're zeroing out all the gradients from the previous step,
 getting the gradients for all the parameters, and then using those gradients to update our
 parameters. So typical training loop, as we saw in the make more series. So let me now run this
 for say 100 iterations, and let's see what kind of losses we're going to get.
 So we started around 4.7. And now we're going to down to like 4.6, 4.5, etc. So the optimization
 is definitely happening. But let's sort of try to increase number of iterations and only print at
 the end, because we probably will not train for a hundred. Okay, so we're down to 3.6 roughly.
 Roughly down to three.
 This is the most janky optimization.
 Okay, it's working. Let's just do 10,000. And then from here, we want to copy this.
 And hopefully we're going to get something reasonable. And of course, it's not going to be
 Shakespeare from a background model. But at least we see that the loss is improving. And
 hopefully we're expecting something a bit more reasonable. Okay, so we're down at about 2.5
 ish. Let's see what we get. Okay, dramatic improvement certainly on what we had here.
 So let me just increase the number of tokens. Okay, so we see that we're starting to get something
 at least like reasonable ish. Certainly not Shakespeare, but the model is making progress.
 So that is the simplest possible model. So now what I'd like to do is,
 obviously, this is a very simple model because the tokens are not talking to each other.
 So given the previous context of whatever was generated, we're only looking at the very last
 character to make the predictions about what comes next. So now these, now these tokens have
 to start talking to each other and figuring out what is in the context so that they can make
 better predictions for what comes next. And this is how we're going to kick off the transformer.
 Okay, so next, I took the code that we developed in this Jupyter notebook, and I converted it to
 be a script. And I'm doing this because I just want to simplify our intermediate work into just
 the final product that we have at this point. So in the top here, I put all the
 parameters that we defined. I introduced a few and I'm going to speak to that in a little bit.
 Otherwise, a lot of this should be recognizable. Reproducibility, read data, get the encoder and
 the decoder, create the train and test splits, use the kind of like data loader that gets a batch
 of the inputs and targets. This is new, and I'll talk about it in a second. Now this is the
 background language model that we developed, and it can forward and give us a logit and loss,
 and it can generate. And then here, we are creating the optimizer and this is the training loop.
 So everything here should look pretty familiar. Now, some of the small things that I added,
 number one, I added the ability to run on a GPU if you have it. So if you have a GPU, then you can,
 this will use CUDA instead of just CPU, and everything will be a lot more faster.
 Now, when device becomes CUDA, then we need to make sure that when we load the data, we move it
 to device. When we create the model, we want to move the model parameters to device. So as an example,
 here we have the in an embedding table, and it's got a dot weight inside it, which stores the,
 sort of lookup table. So that would be moved to the GPU, so that all the calculations here
 happen on the GPU, and they can be a lot faster. And then finally, here, when I'm creating the
 context that feeds it to generate, I have to make sure that I create on the device. Number two,
 what I introduced is the fact that here in the training loop, here I was just printing the
 loss.item inside the training loop. But this is a very noisy measurement of the current loss,
 because every batch will be more or less lucky. And so what I want to do usually is, I have an
 estimate loss function, and the estimate loss basically then goes up here. And it averages up
 the loss over multiple batches. So in particular, we're going to iterate eval iter times. And we're
 going to basically get our loss, and then we're going to get the average loss for both splits.
 And so this will be a lot less noisy. So here, what we call the estimate loss,
 we're going to report the pre accurate train and validation loss. Now, when we come back up,
 you'll notice a few things here, I'm setting the model to a valuation phase. And down here,
 I'm resetting it back to training phase. Now, right now for our model, as is, this doesn't
 actually do anything, because the only thing inside this model is this, and then dot embedding.
 And this, this network would behave both would behave the same in both evaluation mode and training
 mode. We have no dropout layers, we have no bathroom layers, etc. But it is a good practice
 to think through what mode your neural network is in, because some layers will have different
 behavior at inference time or training time. And there's also this context manager, TorchedUpNoGrad,
 and this is just telling PyTorch that everything that happens inside this function, we will not
 call dot backward on. And so PyTorch can be a lot more efficient with its memory use,
 because it doesn't have to store all the intermediate variables, because we're never going to call
 backward. And so it can, it can be a lot more memory efficient in that way. So also a good practice
 to tell PyTorch when we don't intend to do back propagation. So right now, this script is about
 120 lines of code of, and that's kind of our starter code. I'm calling it by gram.py, and I'm
 going to release it later. Now running this script gives us output in the terminal, and it looks
 something like this. It basically, as I ran this code, it was giving me the train loss and val loss,
 and we see that we convert to somewhere around 2.5 with the by-gram model. And then here's the
 sample that we produced at the end. And so we have everything packaged up in the script,
 and we're in a good position now to iterate on this. Okay, so we are almost ready to start
 writing our very first self-attention block for processing these tokens. Now, before we actually
 get there, I want to get you used to a mathematical trick that is used in the self-attention inside
 a transformer, and it's really just like at the heart of an efficient implementation of self-attention.
 And so I want to work with this toy example to just get used to this operation, and then it's
 going to make it much more clear once we actually get to it in the script again. So let's create a
 B by T by C, where B, T, and C are just 4, 8, and 2 in this toy example. And these are basically
 channels, and we have batches, and we have the time component, and we have some information at
 each point in the sequence. So C. Now, what we would like to do is we would like these tokens,
 so we have up to 8 tokens here in a batch, and these 8 tokens are currently not talking to each
 other, and we would like them to talk to each other. We'd like to couple them. And in particular,
 we want to couple them in a very specific way. So the token, for example, at the fifth location,
 it should not communicate with tokens in the sixth, seventh, and eighth location,
 because those are future tokens in the sequence. The token on the fifth location should only talk
 to the one in the fourth, third, second, and first. So information only flows from previous context
 to the current time step, and we cannot get any information from the future, because we are about
 to try to predict the future. So what is the easiest way for tokens to communicate? The easiest
 way I would say is, okay, if we're up to, if we're a fifth token, and I'd like to communicate with my
 past, the simplest way we can do that is to just do an average of all the preceding elements.
 So for example, if I'm the fifth token, I would like to take the channels that make up
 that our information at my step, but then also the channels from the fourth step,
 third step, second step, and the first step, I'd like to average those up, and then that would
 become sort of like a feature vector that summarizes me in the context of my history.
 Now, of course, just doing a sum or like an average is an extremely weak form of interaction,
 like this communication is extremely lossy. We've lost a ton of information about spatial
 arrangements of all those tokens, but that's okay for now. We'll see how we can bring that
 information back later. For now, what we would like to do is for every single batch element
 independently, for every teeth token in that sequence, we'd like to now calculate the average
 of all the vectors in all the previous tokens and also at this token. So let's write that out.
 I have a small snippet here, and instead of just fumbling around, let me just copy paste it and
 talk to it. So in other words, we're going to create x and b-o-w is short for bag of words,
 because bag of words is kind of like a term that people use when you are just averaging up
 things. So this is just a bag of words. Basically, there's a word stored on every one of these
 eight locations, and we're doing a bag of words, just averaging. So in the beginning, we're going
 to say that it's just initialized at zero. And then I'm doing a for loop here. So we're not being
 efficient yet. That's coming. But for now, we're just iterating over all the batch dimensions
 independently, iterating over time. And then the previous tokens are at this batch, a dimension,
 and then everything up to and including the teeth token. So when we slice out x in this way,
 x-pref becomes of shape how many elements there were in the past, and then of course,
 c. So all the two dimensional information from these little tokens. So that's the previous sort
 of chunk of tokens from my current sequence. And then I'm just doing the average or the mean
 over the zero dimension. So I'm averaging out the time here. And I'm just going to get a little
 c one dimensional vector, which I'm going to store in x bag of words. So I can run this.
 And this is not going to be very informative because let's see. So this is x of zero. So this
 is the zero with batch element, and then Expo at zero. Now, you see how the at the first location
 here, you see that the two are equal. And that's because it's we're just doing an average of this
 one token. But here, this one is now an average of these two. And now this one is an average of
 these three. And so on. So and this last one is the average of all of these elements. So vertical
 average, just averaging up all the tokens now gives this outcome here. So this is all well and good.
 But this is very inefficient. Now the trick is that we can be very, very efficient about doing this
 using matrix multiplication. So that's the mathematical trick. And let me show you what I mean. Let's
 work with the toy example here. Let me run it and I'll explain. I have a simple matrix here that is
 three by three of all ones, a matrix B of just random numbers. And it's a three by two. And a
 matrix C, which will be three by three multiply three by two, which will give out a three by two.
 So here we're just using matrix multiplication. So a multiply B gives us C.
 Okay, so how are these numbers in C achieved? Right. So this number in the top left is the first row
 of a dot product with the first column of B. And since all the row of A right now is all just once,
 then the dot product here with with this column of B is just going to do a sum of these of this
 column. So two plus six plus six is 14. The element here in the output of C is also the first column
 here. The first row of A multiplied now with the second column of B. So seven plus four plus five
 is 16. Now you see that there's repeating elements here. So this 14 again is because this row is
 again all once, and it's multiplying the first column of B. So you get 14. And this one is, and so on.
 So this last number here is the last row dot product last column. Now the trick here is the following.
 This is just a boring number of, it's just a boring array of all ones. But torch has this
 function called trail, which is short for a triangular, something like that. And you can
 wrap it in torch that once, and we'll just return the lower triangular portion of this. Okay.
 So now it will basically zero out these guys here. So we just get the lower triangular part.
 Well, what happens if we do that?
 So now we'll have a like this and be like this. And now what are we getting here and C? Well,
 what is this number? Well, this is the first row times the first column. And because this is zeros,
 these elements here are now ignored. So we just get it to. And then this number here is the first
 row times the second column. And because these are zeros, they get ignored. And it's just seven.
 The seven multiplies this one. But look what happened here, because this is one and then zeros,
 what ended up happening is we're just plucking out the row of this row of B. And that's what we got.
 Now here, we have one one zero. So here, one one zero dot product with these two columns will now
 give us two plus six, which is eight, and seven plus four, which is 11. And because this is one one one,
 we ended up with the addition of all of them. And so basically, depending on how many ones and
 zeros we have here, we are basically doing a sum currently of the variable number of these rows.
 And that gets deposited into C. So currently, we're doing sums, because these are ones,
 but we can also do average, right? And you can start to see how we could do average of the rows of B,
 sort of in incremental fashion. Because we don't have to, we can basically normalize these rows,
 so that they sum to one, and then we're going to get an average. So if we took a, and then we did
 a equals a divide, torch dot sum in the, of a, in the one dimension, and then let's keep them
 as true. So therefore, the broadcasting will work out. So if I read from this, you see now that these
 rows now sum to one. So this row is one, this row is 0.5, 0.5, 0. And here we get one thirds.
 And now when we do a multiply B, what are we getting? Here we are just getting the first row,
 first row. Here now we are getting the average of the first two rows.
 Okay, so two and six average is four, and four and seven averages five and five. And on the bottom
 here, we are now getting the average of these three rows. So the average of all of elements of B
 are now deposited here. And so you can see that by manipulating these elements of this multiplying
 matrix, and then multiplying it with any given matrix, we can do these averages in this incremental
 fashion, because we just get, and we can manipulate that based on the elements of A. Okay, so that's
 very convenient. So let's swing back up here and see how we can vectorize this and make it much
 more efficient using what we've learned. So in particular, we are going to produce an array A,
 but here I'm going to call it way short for weights. But this is our A. And this is how much of every
 row we want to average up, and it's going to be an average because you can see that these rows sum
 to one. So this is our A, and then our B in this example, of course, is X. So what's going to happen
 here now is that we are going to have an expo to. And this expo to is going to be way multiplying
 our X. So let's think this through way is T by T. And this is matrix multiplying in pytorch,
 a B by T by C. And it's giving us the what shape. So pytorch will come here and it will see that
 these shapes are not the same. So it will create a batch dimension here. And this is a batch
 matrix multiply. And so it will apply this matrix multiplication in all the batch elements
 in parallel and individually. And then for each batch element, there will be a T by T multiplying
 T by C exactly as we had below. So this will now create B by T by C. And X bow two will now
 become identical to expo. So we can see that torch dot all close of expo and expo two should be true.
 Now, so this kind of like commisses us that these are in fact the same. So expo and expo two,
 if I just print them, okay, we're not going to be able to, okay, we're not going to be able to
 just stare it down. But well, let me try expo basically just at the zero element and expo
 two at the zero element. So just the first batch. And we should see that this, that should be
 identical, which they are. Right. So what happened here, the trick is we were able to use batch
 matrix multiply to do this aggregation really. And it's a weighted aggregation. And the weights
 are specified in this T by T array. And we're basically doing weighted sums. And these weighted
 sums are according to the weights inside here, the take on sort of this triangular form. And so
 that means that a token at the teeth dimension will only get sort of information from the
 tokens preceding it. So that's exactly what we want. And finally, I would like to rewrite it in one
 more way. And we're going to see why that's useful. So this is the third version. And it's also
 identical to the first and second. But let me talk through it, it uses softmax. So trill here
 is this matrix, lower triangular ones. Way begins as all zero. Okay, so if I just print way in the
 beginning, it's all zero. Then I used masked fill. So what this is doing is weight that masked fill,
 it's all zeros. And I'm saying, for all the elements where trill is equal to equal zero,
 make them be negative infinity. So all the elements where trill is zero will become
 negative infinity. Now, so this is what we get. And then the final one here is softmax.
 So if I take a softmax along every single, so dim is negative one, so long every single row,
 if I do a softmax, what is that going to do? Well, softmax is, it's also like a normalization
 operation, right? And so spoiler alert, you get the exact same matrix. Let me bring back the softmax.
 And recall that in softmax, we're going to exponentiate every single one of these. And then
 we're going to divide by the sum. And so if we exponentiate every single element here, we're going to get a
 one. And here we're going to get basically zero, zero, zero, zero, everywhere else. And then when
 we normalize, we just get one. Here we're going to get one, one, and then zeros. And then softmax
 will again divide, and this will give us 0.5 and so on. And so this is also the same way to produce
 this mask. Now the reason that this is a bit more interesting, and the reason we're going to end
 up using it in self attention, is that these weights here begin with zero. And you can think of this
 as like an interaction strength, or like an affinity. So basically, it's telling us how much of each
 token from the past do we want to aggregate an average up. And then this line is saying,
 tokens from the past cannot communicate by setting them to negative infinity. We're saying that we
 will not aggregate anything from those tokens. And so basically, this then goes through softmax,
 and through the weighted, and this is the aggregation through matrix multiplication.
 And so what this is now is you can think of these as these zeros are currently just set by us to be
 zero. But a quick preview is that these affinities between the tokens are not going to be just constant
 at zero. They're going to be data dependent. These tokens are going to start looking at each other.
 And some tokens will find other tokens more or less interesting. And depending on what their
 values are, they're going to find each other interesting to different amounts. And I'm going to call
 those affinities, I think. And then here we are saying the future cannot communicate with the past.
 We're going to clamp them. And then when we normalize and some, we're going to aggregate
 sort of their values, depending on how interestingly they find each other. And so that's the preview
 for self attention. And basically, long story short from this entire section is that you can do
 weighted aggregations of your past elements by having by using matrix multiplication of a lower
 triangular fashion. And then the elements here in the lower triangular part are telling you how
 much of each element fuses into this position. So we're going to use this trick now to develop
 the self attention block. So first, let's get some quick preliminaries out of the way.
 First, the thing I'm kind of bothered by is that you see how we're passing them vocab size into
 the constructor. There's no need to do that because vocab size is already defined up top as a global
 variable. So there's no need to pass this stuff around. Next, what I want to do is I don't want
 to actually create, I want to create like a level of interaction here, where we don't directly go to
 the embedding for the logits. But instead, we go through this intermediate phase, because we're
 going to start making that bigger. So let me introduce a new variable and embed a short for
 number of embedding dimensions. So an embed here will be say 32. That was a suggestion from
 GitHub Copiled by the way. It also suggests 32, which is a good number. So this is an embedding
 table and only 32 dimensional embeddings. So then here, this is not going to give us logits directly.
 Instead, this is going to give us token embeddings. That's what I'm going to call it. And then to go
 from the token embeddings to the logits, we're going to need a linear layer. So self.lmhead,
 let's call it short for language modeling head, is an linear from an embed up to vocab size.
 And then when we swing over here, we're actually going to get the logits by exactly what the
 copilot says. Now we have to be careful here, because this C and this C are not equal.
 This is an embed C and this is vocab size. So let's just say that an embed is equal to C.
 And then this just creates one spurious layer of interaction through a linear layer. But this
 should basically run. So we see that this runs and this currently looks kind of spurious,
 but we're going to build on top of this. Now next up, so far, we've taken these in in the
 seas and we've encoded them based on the identity of the tokens inside ID X. The next thing that
 people very often do is that we're not just encoding the identity of these tokens, but also
 their position. So we're going to have a second position embedding table here. So self that position
 embedding table is an embedding of block size by an embed. And so each position from zero to
 block size minus one will also get its own embedding vector. And then here, first let me decode
 b by t from ID X dot shape. And then here, we're also going to have a positive bedding,
 which is the positional embedding. And these are this is tortoise arrange. So this will be
 basically just integers from zero to t minus one. And all of those integers from zero to t
 minus one get embedded through the table to create a T by C. And then here, this gets renamed to
 just say X and X will be the addition of the token embeddings with the positional embeddings.
 And here the broadcasting note will work out. So B by T by C plus T by C, this gets right aligned
 and new dimension of one gets added, and it gets broadcasted across batch. So at this point,
 X holds not just the token identities, but the positions at which these tokens occur.
 And this is currently not that useful, because of course, we just have a simple
 binary model. So it doesn't matter if you're on the fifth position, the second position,
 or wherever, it's all translation invariant at this stage. So this information currently
 wouldn't help. But as we work on the self attention block, we'll see that this starts to matter.
 Okay, so now we get the crux of self attention. So this is probably the most important part of
 this video to understand. We're going to implement a small self attention for a single individual
 head as they're called. So we start off with where we were. So all of this code is familiar.
 So right now I'm working with an example where I change the number of channels from two to 32. So
 we have a four by eight arrangement of tokens. And each token and the information that each token
 is currently 32 dimensional, but we just are working with random numbers. Now we saw here that
 the code as we had it before does a simple weight, simple average of all the past tokens
 and the current token. So it's just the previous information and current information is just being
 mixed together in an average. And that's what this code currently achieves. And it does so
 by creating this lower triangular structure, which allows us to mask out this way matrix that we
 create. So we mask it out and then we normalize it. And currently, when we initialize the
 affinities between all the different sort of tokens or nodes, I'm going to use those terms
 interchangeably. So when we initialize the affinities between all the different tokens to be zero,
 then we see that way gives us this structure where every single row has these uniform numbers.
 And so that's what that's what then in this matrix multiply makes it so that we're doing a
 simple average. Now, we don't actually want this to be all uniform, because different
 tokens will find different other tokens more or less interesting. And we want that to be data
 dependent. So for example, if I'm a vowel, then maybe I'm looking for consonants in my past. And
 maybe I want to know what those consonants are. And I want that information to flow to me. And so
 I want to now gather information from the past. But I want to do it in a data dependent way. And
 this is the problem that self attention solves. Now the way self attention solves this is the
 following. Every single node or every single token at each position will emit two vectors.
 It will emit a query and it will emit a key. Now the query vector, roughly speaking, is
 what am I looking for? And the key vector, roughly speaking, is what do I contain?
 And then the way we get affinities between these tokens now in a sequence is we basically just do
 a dot product between the keys and the queries. So my query dot products with all the keys of
 all the other tokens. And that dot product now becomes way. And so if the key and the query are
 sort of aligned, they will interact to a very high amount. And then I will get to learn more
 about that specific token, as opposed to any other token in the sequence. So let's implement this now.
 We're going to implement a single what's called head of self attention. So this is just one head.
 There's a hyper parameter involved with these heads, which is the head size. And then here I'm
 initializing linear modules, and I'm using bias equals false. So these are just going to apply
 matrix multiply with some fixed weights. And now let me produce a key and Q k and Q by forwarding
 these modules on X. So the size of this will now become B by T by 16, because that is the head size.
 And the same here B by T by 16. So this being that size. So you see here that when I forward
 this linear on top of my X, all the tokens in all the positions in the B by T arrangement,
 all of them in parallel and independently produce a key and a query. So no communication has happened
 yet. But the communication comes now, all the queries will dart product with all the keys.
 So basically what we want is we want way now or the affinities between these to be query multiplying
 key. But we have to be careful with we can't make sure it's multiplied this we actually need to
 transpose k, but we have to be also careful because these are when you have the batch dimension. So
 in particular, we want to transpose the last two dimensions, dimension negative one and dimension
 negative two. So negative two, negative one. And so this matrix multiply now will basically do the
 following B by T by 16. Matrix multiplies B by 16 by T to give us B by T by T. Right.
 So for every row of B, we're not going to have a T square matrix given us the affinities. And
 these are now the way. So they're not zeros. They are now coming from this dart product between
 the keys and the queries. So this can now run, I can I can run this. And the weighted aggregation
 now is a function in a data band and manner between the keys and queries of these nodes. So
 just inspecting what happened here, the way takes on this form. And you see that before way was
 just a constant. So it was applied in the same way to all the batch elements. But now every single
 batch elements will have different sort of way, because every single batch element contains different
 tokens at different positions. And so this is not a data dependent. So when we look at just the
 zero row, for example, in the input, these are the weights that came out. And so you can see now
 that they're not just exactly uniform. And in particular, as an example here for the last row,
 this was the eighth token. And the eighth token knows what content it has, and it knows at what
 position it's in. And now the eight token based on that creates a query. Hey, I'm looking for this
 kind of stuff. I'm a vowel, I'm on the eight position, I'm looking for any consonants at positions up to
 four. And then all the nodes get to emit keys. And maybe one of the channels could be I am a
 I am a consonant, and I am in the position up to four. And that key would have a high number in that
 specific channel. And that's how the query and the key when they dark product, they can find each other
 and create a high affinity. And when they have a high affinity, like say, this token was pretty
 interesting to to this eighth token. When they have a high affinity, then through the softmax,
 I will end up aggregating a lot of its information into my position. And so I'll get to learn a lot
 about it. Now, just this we're looking at way after this has already happened. Let me erase this
 operation as well. So let me erase the masking and the softmax, just to show you the under the hood
 internals and how that works. So without the masking and the softmax way comes out like this,
 right? This is the outputs of the dot products. And these are the raw outputs and they take on values
 from negative, you know, two to positive two, etc. So that's the raw interactions and raw
 affinities between all the nodes. But now if I'm a, if I'm a fifth node, I will not want to
 aggregate anything from the sixth node seventh node and the eighth node. So actually, we use the
 upper triangular masking. So those are not allowed to communicate. And now we actually want to have
 a nice distribution. So we don't want to aggregate negative point one one of this node that's crazy.
 So instead we exponentiate and normalize, and now we get a nice distribution that seems to want.
 And this is telling us now in the data, depending on manner, how much of information to aggregate
 from any of these tokens in the past. So that's way, and it's not zeros anymore, but it's calculated
 in this way. Now there's one more part to a single self attention head. And that is that when we
 do the aggregation, we don't actually aggregate the tokens exactly. We aggregate, we produce one
 more value here. And we call that the value. So in the same way that we produce t and query,
 we're also going to create a value. And then here, we don't aggregate x, we calculate a v,
 which is just achieved by propagating this linear on top of x again. And then we output
 way multiplied by v. So v is the elements that we aggregate, or the vector that we aggregate,
 instead of the raw x. And now of course, this will make it so that the output here of this single
 head will be 16 dimensional, because that is the head size. So you can think of x as kind of like
 private information to this token, if you think about it that way. So x is kind of private to this
 token. So I'm a fifth token, and I have some identity, and my information is kept in vector x.
 And now for the purposes of the single head, here's what I'm interested in. Here's what I have.
 And if you find me interesting, here's what I will communicate to you. And that's stored in v.
 And so v is the thing that gets aggregated for the purposes of this single head between the
 different nodes. And that's basically the self attention mechanism. This is, this is what it does.
 There are a few notes that I would like to make about attention. Number one, attention is a
 communication mechanism. You can really think about it as a communication mechanism, where you
 have a number of nodes in a directed graph, where basically you have edges pointed between nodes like
 this. And what happens is every node has some vector of information, and it gets to aggregate
 information via a weighted sum from all the nodes that point to it. And this is done in a data
 dependent manner. So depending on whatever data is actually stored at each node at any point in time.
 Now, our graph doesn't look like this. Our graph has a different structure. We have eight nodes,
 because the block size is eight, and there's always eight tokens. And the first node is only
 pointed to by itself. The second node is pointed to by the first node and itself,
 all the way up to the eighth node, which is pointed to by all the previous nodes and itself.
 And so that's the structure that our directed graph has, or happens to have, in autoregressive
 sort of scenario like language modeling. But in principle, attention can be applied to any
 arbitrary directed graph, and it's just a communication mechanism between the nodes.
 The second note is that, notice that there's no notion of space. So attention simply acts over
 like a set of vectors in this graph. And so by default, these nodes have no idea where they
 are positioned in the space. And that's why we need to encode them positionally and sort of give
 them some information that is anchored to a specific position so that they sort of know where they
 are. And this is different than, for example, from convolution, because if you run, for example,
 a convolution operation over some input, there is a very specific sort of layout of the information
 in space and the convolutional filters sort of act in space. And so it's not like an attention.
 An attention is just a set of vectors out there in space. They communicate. And if you want them
 to have a notion of space, you need to specifically add it, which is what we've done when we
 calculated the relative the positional encodings and added that information to the vectors.
 The next thing that I hope is very clear is that the elements across the batch dimension,
 which are independent examples, never talk to each other. They're always processed independently.
 And this is a bashed matrix multiply that applies basically a matrix multiplication,
 kind of imperil across the batch dimension. So maybe it would be more accurate to say that
 in this analogy of a directed graph, we really have, because the batch size is four, we really have
 four separate pools of eight nodes, and those eight nodes only talk to each other. But in total,
 there's like 32 nodes that are being processed. But there's sort of four separate pools of eight,
 you can look at it that way. The next note is that here in the case of language modeling,
 we have this specific structure of directed graph where the future tokens will not communicate
 to the past tokens. But this doesn't necessarily have to be the constraint in the general case.
 And in fact, in many cases, you may want to have all of the nodes talk to each other fully. So as
 an example, if you're doing sentiment analysis or something like that with a transformer, you might
 have a number of tokens, and you may want to have them all talk to each other fully, because later,
 you are predicting, for example, the sentiment of the sentence. And so it's okay for these
 nodes to talk to each other. And so in those cases, you will use an encoder block of self attention.
 And all it means that it's an encoder block is that you will delete this line of code,
 allowing all the nodes to completely talk to each other. What we're implementing here is
 sometimes called a decoder block. And it's called a decoder, because it is sort of like
 decoding language. And it's got this autoregressive format, where you have to mask with the triangle
 and matrix, so that nodes from the future never talk to the past, because they would give away
 the answer. And so basically, an encoder blocks, you would delete this, allow all the nodes to talk.
 In decoder blocks, this will always be present, so that you have this triangular structure.
 But both are allowed and attention doesn't care. Attention supports arbitrary connectivity
 between nodes. The next thing I wanted to comment on is you keep me, you keep hearing me say
 attention, self attention, etc. There's actually also something called cross attention. What is
 the difference? So basically, the reason this attention is self attention, is because the keys,
 queries, and the values are all coming from the same source from X. So the same source X produces
 keys, queries, and values. So these nodes are self attending. But in principle, attention is
 much more general than that. So for example, an encoder decoder transformers, you can have a case
 where the queries are produced from X. But the keys and the values come from a whole separate
 external source, and sometimes from encoder blocks that encode some context that we'd like to
 condition on. And so the keys and the values will actually come from a whole separate source.
 Those are nodes on the side. And here we're just producing queries. And we're reading off
 information from the side. So cross attention is used when there's a separate source of nodes,
 we'd like to pull information from into our nodes. And it's self attention if we just have nodes
 that would like to look at each other and talk to each other. So this attention here happens to be
 self attention. But in principle, attention is a lot more general. Okay, and the last note at this
 stage is if we come to the attention is only need paper here, we've already implemented attention.
 So given query key and value, we've multiplied the query on a key, we've soft maxed it, and then
 we are aggregating the values. There's one more thing that we're missing here, which is the dividing
 by one over square root of the head size, the decay here is the head size. Why aren't they doing
 this one is important. So they call it a scaled attention. And it's kind of like an important
 normalization to basically have the problem is if you have unit Gaussian inputs, so zero mean unit
 variance, k and q are unit Gaussian. And if you just do way naively, then you see that your way
 actually will be the variance will be on the order of head size, which in our case is 16.
 But if you multiply by one over head size square root, so this is square root, and this is one over,
 then the variance of way will be one. So we'll be preserved. Now, why is this important? You'll
 notice that way here will feed into softmax. And so it's really important, especially at
 initialization, that way be fairly diffuse. So in our case here, we sort of locked out here and way
 at a fairly diffuse numbers here. So like this, now the problem is that because of softmax,
 if weight takes on very positive and very negative numbers inside it, softmax will actually converge
 towards one hot vectors. And so I can illustrate that here. Say we are applying softmax to a tensor
 of values that are very close to zero, then we're going to get a diffuse thing out of softmax.
 But the moment I take the exact same thing and I start sharpening it, making it bigger by multiplying
 these numbers by eight, for example, you'll see that the softmax will start to sharpen. And in fact,
 it will sharpen towards the max. So it will sharpen towards whatever number here is the highest.
 And so basically we don't want these values to be too extreme, especially at initialization.
 Otherwise, softmax will be way too peaky. And you're basically aggregating information from like a
 single node. Every node just aggregates information from a single other node. That's not what we want,
 especially at initialization. And so the scaling is used just to control the variance at initialization.
 Okay, so having said all that, let's now take our self attention knowledge and let's take it for a spin.
 So here in the code, I created this head module and implements a single head of self attention.
 So you give it a head size. And then here it creates the key query and the value of linear layers,
 typically people don't use biases in these. So those are the linear projections that we're
 going to apply to all of our nodes. Now here, I'm creating this trill variable. Trill is not a
 parameter of the module. So in sort of pytorch naming conventions, this is called a buffer. It's not
 a parameter. And you have to call it, you have to assign it to the module using a register buffer.
 So that creates the trill, the trying lower triangular matrix. And over given the input x,
 this should look very familiar now. We calculate the keys, the queries, we calculate the attention
 scores in sideways. We normalize it. So we're using scaled attention here. Then we make sure that
 sure doesn't communicate with the past. So this makes it a decoder block. And then softmax and
 then aggregate the value in output. Then here in the language model, I'm creating a head in the
 constructor and I'm calling itself attention head. And the head size, I'm going to keep as the same
 and embed just for now. And then here, once we've encoded the information with the token embeddings
 and the position embeddings, we're simply going to feed it into the self attention head. And then
 the output of that is going to go into the decoder language modeling head and create the logits.
 So this is sort of the simplest way to plug in a self attention component into our network right
 now. I had to make one more change, which is that here in the generate, we have to make sure that our
 ID X that we feed into the model, because now we're using positional embeddings, we can never
 have more than block size coming in, because if ID X is more than block size, then our position
 embedding table is going to run out of scope, because it only has embeddings for up to block size.
 And so therefore I added some code here to crop the context that we're going to feed into
 self, so that we never pass in more than block size elements. So those are the changes and let's
 now train the network. Okay, so I also came up to the script here and I decreased the learning
 rate because the self attention can't tolerate very, very high learning rates. And then I also
 increased number of iterations because the learning rate is lower. And then I trained it and previously
 we were only able to get to up to 2.5. And now we are down to 2.4. So we definitely see a little bit
 improvement from 2.5 to 2.4 roughly, but the text is still not amazing. So clearly the self
 attention head is doing some useful communication, but we still have a long way to go. Okay, so now
 we've implemented the scale dot product attention. Now next up in the attention is all you need paper.
 There's something called multi head attention. And what is multi head attention? It's just applying
 multiple attentions in parallel and concatenating the results. So they have a little bit of diagram
 here. I don't know if this is super clear. It's really just multiple attentions in parallel.
 So let's implement that fairly straightforward. If we want a multi head attention, then we want
 multiple heads of self attention running in parallel. So in PyTorch, we can do this by simply creating
 multiple heads. So however many heads you want, and then what is the head size of each? And then
 we run all of them in parallel into a list and simply concatenate all of the outputs. And we're
 concatenating over the channel dimension. So the way this looks now is we don't have just a single
 attention that has a head size of 32. Because remember, an embed is 32. Instead of having one
 communication channel, we now have four communication channels in parallel. And each one of these
 communication channels typically will be smaller correspondingly. So because we have four communication
 channels, we want eight dimensional self attention. And so from each communication channel, we're
 going to together eight dimensional vectors. And then we have four of them. And that
 coordinates to give us 32, which is the original and embed. And so this is kind of similar to,
 if you're familiar with convolutions, this is kind of like a group convolution. Because basically,
 instead of having one large convolution, we do convolution in groups. And that's multi headed
 self attention. And so then here, we just use SA heads, self attention heads instead.
 Now I actually ran it and scrolling down. I ran the same thing. And then we now get this out to
 2.28 roughly. And the upper is still the generation is still not amazing. But clearly,
 the validation loss is improving, because we were at 2.4 just now. And so it helps to have multiple
 communication channels, because obviously these tokens have a lot to talk about. They want to
 find the consonants, the vowels, they want to find the vowels just from certain positions.
 They want to find any kinds of different things. And so it helps to create multiple independent
 channels of communication, gather lots of different types of data, and then decode the output.
 Now going back to the paper for a second, of course, I didn't explain this figure in full detail,
 but we are starting to see some components of what we've already implemented. We have the
 positional encodings, token encodings that add, we have the masked multi headed attention implemented.
 Now, here's another multi headed attention, which is a cross attention to an encoder, which we haven't
 we're not going to implement in this case. I'm going to come back to that later. But I want you to
 notice that there's a feed forward part here. And then this is grouped into a block that gets
 repeated again and again. Now the feed forward part here is just a simple multi later production.
 So the multi headed. So here position wise feed forward networks is just a simple little mop.
 So I want to start basically in a similar fashion, also adding computation into the network. And
 this computation is on a per node level. So I've already implemented it, and you can see the diff
 highlighted on the left here when I've added or changed things. Now, before we had the
 self multi headed self attention that did the communication, but we went way too fast to
 calculate the logits. So the tokens looked at each other, but didn't really have a lot of time to
 think on what they found from the other tokens. And so what I've implemented here is a little feed
 forward single layer. And this little layer is just a linear followed by a row nonlinearity.
 And that's that's it. So it's just a little layer. And then I call it feed forward.
 And embed. And then this feed forward is just called sequentially right after the self attention.
 So we self attend, then we feed forward. And you'll notice that the feed forward here,
 when it's applying linear, this is on a per token level. All the tokens do this independently.
 So the self attention is the communication. And then once they gathered all the data,
 now they need to think on that data individually. And so that's what feed forward is doing. And
 that's why I've added it here. Now, when I train this, the validation loss actually continues to go
 down now to 2.24, which is down from 2.28. The outputs still look kind of terrible, but at least
 we've improved the situation. And so as a preview, we're going to now start to interspers
 the communication with the computation. And that's also what the transformer does,
 when it has blocks that communicate and then compute, and it groups them and replicates them.
 Okay, so let me show you what we like to do. We'd like to do something like this. We have a block.
 And this block is basically this part here, except for the cross attention. Now, the block
 basically interspers is communication and the computation. The computation, the communication
 is done using multi headed self attention. And then the computation is done using a feed forward
 network on all the tokens independently. Now, what I've added here also is you'll notice
 this takes the number of embeddings in the embedding dimension and number of heads that we would
 like, which is kind of like group sizing group convolution. And I'm saying that number of heads
 we'd like is four. And so because this is 32, we calculate that because this 32, the number of heads
 should be four, the head size should be eight, so that everything sort of works out channel wise.
 So this is how the transformer structures, sort of the sizes typically. So the head size will become
 eight. And then this is how we want to interspers them. And then here, I'm trying to create blocks,
 which is just a sequential application of block block block. So that we're interspersing
 communication feed forward many, many times. And then finally, we decode. Now, actually try to run
 this. And the problem is this doesn't actually give a very good answer. And we're very good result.
 And the reason for that is we're starting to actually get like a pretty deep neural net. And
 deep neural nets suffer from optimization issues. And I think that's what we're kind of like
 slightly starting to run into. So we need one more idea that we can borrow from the
 transfer of paper to resolve those difficulties. Now, there are two optimizations that dramatically
 help with the depth of these networks, and make sure that the networks remain optimizable.
 Let's talk about the first one. The first one in this diagram is you see this arrow here,
 and then this arrow and this arrow. Those are skip connections or sometimes called residual
 connections. They come from this paper, the procedural learning from a direct condition from
 about 2015 that introduced the concept. Now, these are basically what it means is you transform the
 data, but then you have a skip connection with addition from the previous features. Now, the way
 I like to visualize it that I prefer is the following. Here, the computation happens from the top to
 bottom. And basically, you have this residual pathway, and you are free to fork off from the
 residual pathway, perform some computation, and then project back to the residual pathway via addition.
 And so you go from the inputs to the targets, only the plus and plus and plus. And the reason
 this is useful is because during that propagation, remember from our micro grad video earlier,
 addition distributes gradients equally to both of its branches that that fed us the input. And so
 the supervision or the gradients from the loss basically hop through every addition node all the
 way to the input, and then also fork off into the residual blocks. But basically, have this
 gradient superhighway that goes directly from the supervision all the way to the input, unimpeded.
 And then these residual blocks are usually initialized in the beginning, so they contribute
 very, very little, if anything, to the residual pathway. They are initialized that way. So in the
 beginning, they are almost kind of like not there. But then during the optimization, they come online
 over time, and they start to contribute. But at least at the initialization, you can go from
 directly supervision to the input gradient is unimpeded and just flows, and then the blocks
 over time kick in. And so that dramatically helps with the optimization. So let's implement this.
 So coming back to our block here, basically what we want to do is we want to do x equals x plus
 self attention and x equals x plus self that feed forward. So this is x, and then we fork off
 and do some communication and come back. And we fork off and we do some computation and come back.
 So those are residual connections. And then swinging back up here, we also have to introduce
 this projection. So and then that linear. And this is going to be from after we concatenate this,
 this is the size and embed. So this is the output of the self tension itself. But then we actually
 want the to apply the projection. And that's the result. So the projection is just a linear
 transformation of the outcome of this layer. So that's the projection back into the residual pathway.
 And then here in a feed forward, it's going to be the same thing. I could have a self that
 projection here as well. But let me just simplify it. And let me couple it inside the same sequential
 container. And so this is the projection layer going back into the residual pathway. And so
 that's, well, that's it. So now we can train this. So I'm putting within one more small change.
 When you look into the paper again, you see that the dimensionality of input and output is 512 for
 them. And they're saying that the inner layer here in the feed forward has dimensionality of 2048.
 So there's a multiplier of four. And so the inner layer of the feed forward network
 should be multiplied by four in terms of child sizes. So I came here and I multiplied four times
 embed here for the feed forward. And then from four times an embed, coming back down to an embed
 when we go back to the projection. So adding a bit of computation here and growing that layer
 that is in the residual block on the side of the residual pathway. And then I train this and we
 actually get down all the way to 2.08 validation loss. And we also see that network is starting
 to get big enough that our train loss is getting ahead of validation loss. So we start to see like
 a little bit of overfitting. And our our generations here are still not amazing. But at least you see
 that we can see like is here this now grief sync, like this starts to almost look like English.
 So yeah, we're starting to really get there. Okay, and the second innovation that is very
 helpful for optimizing very deep neural networks is right here. So we have this addition now,
 that's the residual part, but this norm is referring to something called layer norm.
 So layer norm is implemented in PyTorch. It's a paper that came out a while back here.
 And layer norm is very, very similar to bash norm. So remember back to our make more series part 3.
 We implemented bash normalization. And bash normalization basically just made sure
 that across the bash dimension, any individual neuron had unit Gaussian distribution. So it was
 zero mean and unit standard deviation, one standard deviation output. So what I did here is I'm
 copy pasting the bash norm 1D that we developed in our make more series. And see here we can
 initialize, for example, this module, and we can have a batch of 32, 100 dimensional vectors feeding
 through the bash room layer. So what this does is it guarantees that when we look at just the
 zero column, it's a zero mean one standard deviation. So it's normalizing every single column of this
 input. Now the rows are not going to be normalized by default, because we're just normalizing columns.
 So let's not implement layer norm. It's very complicated. Look, we come here, we change this
 from zero to one. So we don't normalize the columns, we normalize the rows. And now we've
 implemented layer norm. So now the columns are not going to be normalized. But the rows are going
 to be normalized for every individual example, it's 100 dimensional vector is normalized in this way.
 And because our computation does not span across examples, we can delete all of this buffers
 stuff, because we can always apply this operation and don't need to maintain any running buffers.
 So we don't need the buffers. We don't, there's no distinction between training and test time.
 And we don't need these running buffers. We do keep gamma and beta. We don't need the momentum.
 We don't care if it's training or not. And this is now a layer norm. And it normalizes the
 rows instead of the columns. And this here is identical to basically this here. So let's now
 implement layer norm in our transformer. Before I incorporated the layer norm, I just wanted to
 note that as I said, very few details about the transformer have changed in the last five years.
 But this is actually something that's likely the parts from the original paper. You see that the
 add and norm is applied after the transformation. But in now it is a bit more basically common to
 apply the layer norm before the transformation. So there's a reshuffling of the layer norms.
 So this is called the pre norm formulation. And that the one that we're going to implement as
 well. So select deviation from the original paper. Basically, we need to learn rooms. Layer norm one
 is an end dot layer norm. And we tell it how many words the embedding dimension. And we need
 the second layer norm. And then here, the layer rooms are applied immediately on X.
 So self that layer norm one, apply to an X, and self that layer number two applied on X.
 Before it goes into self attention and feed forward. And the size of the layer norm here is an
 embeds of 32. So when the layer norm is normalizing our features, it is the normalization here.
 Happens. The mean and the variance are taken over 32 numbers. So the batch and the time
 act as batch dimensions, both of them. So this is kind of like a per token transformation that
 just normalizes the features and makes them a unit mean, unit Gaussian at initialization.
 But of course, because these layer norms inside it have these gamma and beta trainable parameters.
 The layer normal eventually create outputs that might not be unit Gaussian. But the optimization
 will determine that. So for now, this is the this is incorporating the layer norms and let's train
 them up. Okay, so I let it run. And we see that we get down to 2.06, which is better than the
 previous 2.08. So a slight improvement by adding the layer norms. And I'd expect that they help
 even more if we have bigger and deeper network. One more thing I forgot to add is that there
 should be a layer norm here also typically, as at the end of the transformer and right before the
 final linear layer that decodes into vocabulary. So I added that as well. So at this stage, we
 actually have a pretty complete transformer according to the original paper. And it's a decoder only
 transformer. I'll talk about that in a second. But at this stage, the major pieces are in place.
 So we can try to scale this up and see how well we can push this number. Now, in order to scale
 up the model, I had to perform some cosmetic changes here to make it nicer. So I introduced
 this variable called in layer, which just specifies how many layers of the blocks we're going to have.
 I create a bunch of blocks and we have a new variable number of heads as well.
 I pulled out the layer norm here. And so this is identical. Now, one thing that I did briefly
 change is I added a dropout. So dropout is something that you can add right before the residual
 connection back, right before the connection back into the residual pathway. So we can drop out
 that as the last layer here. We can drop out here at the end of the multi-headed retention as well.
 And we can also drop out here when we calculate the, basically,
 affinities and after the softmax, we can drop out some of those. So we can randomly prevent
 some of the nodes from communicating. And so dropout comes from this paper from 2014 or so.
 And basically it takes your neural mat and it randomly, every forward backward pass,
 shuts off some subset of neurons. So randomly drops them to zero and trains without them.
 And what this does effectively is because the mask of what's being dropped out has changed
 every single forward backward pass, it ends up kind of training an ensemble of sub networks.
 And then at test time, everything is fully enabled and kind of all of those sub networks are merged
 into a single ensemble, if you can, if you want to think about it that way. So I would read the paper
 to get the full detail. For now, we're just going to stay on the level of this is a regularization
 technique. And I added it because I'm about to scale up the model quite a bit. And I was concerned
 about overfitting. So now when we scroll up to the top, we'll see that I changed a number of hyper
 parameters here about our neural mat. So I made the batch size be much larger, now 64. I changed
 the block size to be 256. So previously was just eight, eight characters of context. Now it is 256
 characters of context to predict the 257th. I brought down the learning rate a little bit,
 because the neural mat is now much bigger. So I brought down the learning way. The embedding
 dimension is now 384. And there are six heads. So 384 divide six means that every head is 64
 dimensional as it as a standard. And then there are also going to be six layers of that. And the
 dropout will be a point to so every forward backward pass 20% of all of these intermediate
 calculations are disabled and dropped to zero. And then I already trained this and I ran it. So
 drum roll, hello does it perform. So let me just scroll up here. We get a validation loss of 1.48,
 which is actually quite a bit of an improvement on what we had before, which I think was 2.07.
 So we went from 2.07 all the way down to 1.48 just by scaling up this neural mat with the code
 that we have. And this of course ran for a lot longer. This may be trained for I want to say
 about 15 minutes on my a 100 GPU. So that's a pretty good GPU. And if you don't have a GPU,
 you're not going to be able to reproduce this on a CPU. This would be I would not run this on a
 CPU or a MacBook or something like that. You'll have to bring down the number of layers and the
 embedding dimension and so on. But in about 15 minutes, we can get this kind of a result. And
 I'm printing some of the Shakespeare here. But what I did also is I printed 10,000 characters,
 so a lot more and I wrote them to a file. And so here we see some of the outputs.
 So it's a lot more recognizable as the input text file. So the input text file just for reference
 looked like this. So there's always like someone speaking in this matter. And
 our predictions now take on that form. Except of course they're non-susical when you actually read
 them. So it is every crimty be house. Oh, those preparation. We give heed.
 You know, oh, sent me you mighty lord. Anyway, so you can read through this. It's non-susical,
 of course, but this is just a transformer trained on the character level for 1 million characters
 that come from Shakespeare. So there's sort of like blabbers on in Shakespeare like manner,
 but it doesn't of course make sense at this scale. But I think I think still a pretty good
 demonstration of what's possible. So now I think that kind of like concludes the programming section
 of this video. We basically kind of did a pretty good job in implementing this transformer,
 but the picture doesn't exactly match up to what we've done. So what's going on with all these
 additional parts here? So let me finish explaining this architecture and why it looks so funky.
 Basically what's happening here is what we implemented here is a decoder only transformer.
 So there's no component here. This part is called the encoder and there's no cross attention block
 here. Our block only has a self attention and the feed forward. So it is missing this third in
 between piece here. This piece does cross attention. So we don't have it and we don't have the encoder.
 We just have the decoder. And the reason we have a decoder only is because we are just generating
 text and it's unconditioned on anything. We're just blabbering on according to a given data set.
 What makes it a decoder is that we are using the triangular mask in our transformer. So it has this
 autoregressive property where we can just go and sample from it. So the fact that it's using the
 triangular mask to mask out the attention makes it a decoder and it can be used for language
 modeling. Now the reason that the original paper had an encoder decoder architecture is because it
 is a machine translation paper. So it is concerned with a different setting in particular. It expects
 some tokens that encode say for example French and then it is expected to decode the translation
 in English. So typically these here are special tokens. So you are expected to read in this and
 condition on it. And then you start off the generation with a special token called start.
 So this is a special new token that you introduce and always place in the beginning.
 And then the network is expected to output neural networks are awesome and then a special end token
 to finish the generation. So this part here will be decoded exactly as we have done it. Neural
 networks are awesome. We'll be identical to what we did. But unlike what we did they want to condition
 the generation on some additional information. And in that case this additional information is
 the French sentence that they should be translating. So what they do now is they bring the encoder.
 Now the encoder reads this part here. So we're all going to take the part of French and we're going
 to create tokens from it exactly as we've seen in our video. And we're going to put a transformer
 on it. But there's going to be no triangular mask. And so all the tokens are allowed to talk to
 each other as much as they want. And they're just encoding whatever's the content of this French
 sentence. Once they've encoded it, they've basically come out in the top here. And then what happens
 here is in our decoder, which does the language modeling, there's an additional connection here
 to the outputs of the encoder. And that is brought in through cross attention. So the queries are
 still generated from X. But now the keys and the values are coming from the side. The keys and
 the values are coming from the top generated by the nodes that came outside of the decoder.
 And those tops, the keys and the values there, the top of it feed in on a side into every single
 block of the decoder. And so that's why there's an additional cross attention. And really what
 it's doing is it's conditioning the decoding, not just on the past of this current decoding,
 but also on having seen the full fully encoded French prompt sort of. And so it's an encoded
 decoder model, which is why we have those two transformers, an additional block, and so on.
 So we did not do this because we have no we have nothing to encode. There's no conditioning. We just
 have a text file and we just want to imitate it. And that's why we are using a decoder only
 transformer exactly as done in GPT. Okay, so now I wanted to do a very brief walkthrough of the
 energy PT, which you can find on my GitHub. And now GPT is basically two files of interest.
 There's trained up by and modeled up by trained up by is all the boilerplate code for training the
 network. It is basically all the stuff that we had here is the training loop. It's just that it's
 a lot more complicated because we're saving and loading checkpoints and pre trained weights. And
 we are decaying the learning rate and compiling the model and using distributed training across
 multiple nodes or GPUs. So the training that pogates a little bit more hairy complicated.
 There's more options, etc. But the model that I should look very, very similar to what we've done
 here. In fact, the model is is almost identical. So first, here we have the causal self attention
 block. And all of this should look very, very recognizable to you. We're producing queries, keys,
 values, we're doing dot products, we're masking, applying softmax, optionally dropping out. And
 here we are pulling the values. What is different here is that in our code, I have separated out
 the multi headed attention into just a single individual head. And then here I have multiple
 heads and I explicitly concatenate them. Whereas here, all of it is implemented in a
 batched manner inside a single causal self attention. And so we don't just have a B and a T and a C
 dimension. We also end up with a fourth dimension, which is the heads. And so it just gets a lot more
 sort of hairy because we have four dimensional array tensors now. But it is equivalent mathematically.
 So the exact same thing is happening as what we have. It's just a bit more efficient because all
 the heads are now treated as a batch dimension as well. Then we have the multiple perceptron.
 It's using the gallon on linearity, which is defined here, instead of relu. And this is done
 just because opening I used it and I want to be able to load their checkpoints. The blocks of the
 transformer are identical, the communicate and the compute phase as we saw. And then the GPT will
 be identical. We have the position encodings, token encodings, the blocks, the layer norm at the end,
 the final linear layer. And this should look all very recognizable. And there's a bit more here
 because I'm loading checkpoints and stuff like that. I'm separating out the parameters into those
 should that should be weight decayed and those that shouldn't. But the generate function should
 also be very, very similar. So a few details are different, but you should definitely be able to
 look at this file and be able to understand a lot of the pieces now. So let's now bring things
 back to chat GPT. What would it look like if we wanted to train chat GPT ourselves? And how does
 it relate to what we learned today? Well, to train in chat GPT, there are roughly two stages.
 First is the pre-training stage and then the fine tuning stage. In the pre-training stage,
 we are training on the large chunk of internet and just trying to get a first decoder-only
 transformer to babble text. So it's very, very similar to what we've done ourselves.
 Except we've done like a tiny little baby pre-training step. And so in our case,
 this is how you print a number of parameters. I printed it and it's about 10 million. So
 this transformer that I created here to create a little Shakespeare transformer was about 10
 million parameters. Our dataset is roughly 1 million characters, so roughly 1 million tokens.
 But you have to remember that opening eyes is different vocabulary. They're not on the
 character level. They use these sub-word chunks of words. And so they have a vocabulary of 50,000
 roughly elements. And so their sequences are a bit more condensed. So our dataset, the Shakespeare
 dataset would be probably around 300,000 tokens in the opening eye vocabulary roughly.
 So we trained about 10 million parameter model on roughly 300,000 tokens. Now when you go to the
 GPT-3 paper and you look at the transformers that they train, they train to a number of
 transformers of different sizes. But the biggest transformer here has 175 billion parameters.
 So ours is again 10 million. They used this number of layers in a transformer. This is the end in bed.
 This is the number of heads. And this is the head size. And then this is the batch size. So
 ours was 65. And the learning rate is similar. Now when they train this transformer, they trained
 on 300 billion tokens. So again, remember ours is about 300,000. So this is about a million fold
 increase. And this number would not be even that large by today's standards. You'd be going up
 one trillion above. So they are training a significantly larger model
 on a good chunk of the internet. And that is the pre-training stage. But otherwise,
 these hyperparameters should be fairly recognizable to you. And the architecture is actually like
 nearly identical to what we implemented ourselves. But of course, it's a massive infrastructure
 challenge to train this. You're talking about typically thousands of GPUs having to talk to
 each other to train models of this size. So that's just the pre-training stage. Now, after you complete
 the pre-training stage, you don't get something that responds to your questions with answers,
 and it's not helpful and etc. You get a document complete. So it babbles, but it doesn't babble
 Shakespeare in babbles internet. It will create arbitrary news articles and documents, and it
 will try to complete documents because that's what it's trained for. It's trying to complete the
 sequence. So when you give it a question, it would just potentially just give you more questions. It
 would follow with more questions. It will do whatever it looks like some close document would
 do in the training data on the internet. And so who knows, you're getting kind of like undefined
 behavior. It might basically answer to questions with other questions. It might ignore your question.
 It might just try to complete some news article. It's totally unaligned, as we say. So the second
 fine-tuning stage is to actually align it to be an assistant. And this is the second stage.
 And so this chat GPT blog post from OpenAI talks a little bit about how this stage is achieved.
 We basically, there's roughly three steps to this stage. So what they do here is they start to
 collect training data that looks specifically like what an assistant would do. So the
 are documents that have the format where the question is on top and then an answer is below.
 And they have a large number of these, but probably not on the order of the internet.
 This is probably on the order of maybe thousands of examples. And so they then fine-tune the model
 to basically only focus on documents that look like that. And so you're starting to slowly align
 it. So it's going to expect a question at the top and it's going to expect to complete the answer.
 And these very, very large models are very sample efficient during their fine-tuning.
 So this actually somehow works. But that's just step one. That's just fine-tuning. So then they
 actually have more steps where, okay, the second step is you let the model respond and then different
 readers look at the different responses and rank them for their preferences to which one is better
 than the other. They use that to train the reward model. So they can predict basically using a
 different network how much of any candidate response would be desirable. And then once they have a
 reward model, they run PPO, which is a form of policy gradient reinforcement learning optimizer
 to fine-tune this sampling policy so that the answers that GPT now generates are expected to
 score a high reward according to the reward model. And so basically there's a whole lining
 stage here or fine-tuning stage. It's got multiple steps in between there as well. And it takes the
 model from being a document completer to a question-answer. And that's like a whole separate stage.
 A lot of this data is not available publicly. It is internal to open AI. And it's much harder
 to replicate this stage. And so that's roughly what would give you a chat GPT. And nano GPT focuses
 on the pre-training stage. Okay, and that's everything that I wanted to cover today. So we trained to
 summarize a decoder-only transformer following this famous paper "Attention is All You Need" from 2017.
 And so that's basically a GPT. We trained it on a tiny Shakespeare and got sensible results.
 All of the training code is roughly 200 lines of code. I will be releasing this code base.
 So also it comes with all the Git log commits along the way as we built it up.
 In addition to this code, I'm going to release the notebook, of course, the Google collab.
 And I hope that gave you a sense for how you can train these models like, say, GPT-3.
 That will be architecturally basically identical to what we have. But they are somewhere between 10,000
 and 1 million times bigger depending on how you count. And so that's all I have for now.
 We did not talk about any of the fine-tuning stages that typically go on top of this.
 So if you're interested in something that's not just language modeling, but you actually want to,
 you know, say, perform tasks, or you want them to be aligned in a specific way,
 or you want to detect sentiment or anything like that. Basically, anytime you don't want
 something that's just a document completer, you have to complete further stages of fine-tuning,
 which we did not cover. And that could be simple supervised fine-tuning, or it can be something
 more fancy like we see in chat GPT, where we actually train a reward model and then do rounds of BPO
 to align it with respect to the reward model. So there's a lot more that can be done on top of it.
 I think for now we're starting to get to about two hours mark. So I'm going to kind of finish here.
 I hope you enjoyed the lecture. And yeah, go forth and transform. See you later.
