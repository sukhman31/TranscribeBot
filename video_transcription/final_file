hello my name is andre
and i've been training deep neural
networks for a bit more than a decade
and in this lecture i'd like to show you
what neural network training looks like
under the hood so in particular we are
going to start with a blank jupiter
notebook and by the end of this lecture
we will define and train in neural net
and you'll get to see everything that
goes on under the hood and exactly
sort of how that works on an intuitive
level
now specifically what i would like to do
is i would like to take you through
building of micrograd now micrograd is
this library that i released on github
about two years ago but at the time i
only uploaded the source code and you'd
have to go in by yourself and really
figure out how it works
so in this lecture i will take you
through it step by step and kind of
comment on all the pieces of it so what
is micrograd and why is it interesting
good
um
micrograd is basically an autograd
engine autograd is short for automatic
gradient and really what it does is it
implements backpropagation now
backpropagation is this algorithm that
allows you to efficiently evaluate the
gradient of
some kind of a loss function with
respect to the weights of a neural
network and what that allows us to do
then is we can iteratively tune the
weights of that neural network to
minimize the loss function and therefore
improve the accuracy of the network so
back propagation would be at the
mathematical core of any modern deep
neural network library like say pytorch
or jaxx
so the functionality of microgrant is i
think best illustrated by an example so
if we just scroll down here
you'll see that micrograph basically
allows you to build out mathematical
expressions
and um here what we are doing is we have
an expression that we're building out
where you have two inputs a and b
and you'll see that a and b are negative
four and two but we are wrapping those
values into this value object that we
are going to build out as part of
micrograd
so this value object will wrap the
numbers themselves
and then we are going to build out a
mathematical expression here where a and
b are transformed into c d and
eventually e f and g
and i'm showing some of the functions
some of the functionality of micrograph
and the operations that it supports so
you can add two value objects you can
multiply them you can raise them to a
constant power you can offset by one
negate squash at zero
square divide by constant divide by it
etc
and so we're building out an expression
graph with with these two inputs a and b
and we're creating an output value of g
and micrograd will in the background
build out this entire mathematical
expression so it will for example know
that c is also a value
c was a result of an addition operation
and the
child nodes of c are a and b because the
and will maintain pointers to a and b
value objects so we'll basically know
exactly how all of this is laid out
and then not only can we do what we call
the forward pass where we actually look
at the value of g of course that's
pretty straightforward we will access
that using the dot data attribute and so
the output of the forward pass the value
of g is 24.7 it turns out but the big
deal is that we can also take this g
value object and we can call that
backward
and this will basically uh initialize
back propagation at the node g
and what backpropagation is going to do
is it's going to start at g and it's
going to go backwards through that
expression graph and it's going to
recursively apply the chain rule from
calculus
and what that allows us to do then is
we're going to evaluate basically the
derivative of g with respect to all the
internal nodes
like e d and c but also with respect to
the inputs a and b
and then we can actually query this
derivative of g with respect to a for
example that's a dot grad in this case
it happens to be 138 and the derivative
of g with respect to b
which also happens to be here 645
and this derivative we'll see soon is
very important information because it's
telling us how a and b are affecting g
through this mathematical expression so
in particular
a dot grad is 138 so if we slightly
nudge a and make it slightly larger
138 is telling us that g will grow and
the slope of that growth is going to be
138
and the slope of growth of b is going to
be 645. so that's going to tell us about
how g will respond if a and b get
tweaked a tiny amount in a positive
direction
okay
now you might be confused about what
this expression is that we built out
here and this expression by the way is
completely meaningless i just made it up
i'm just flexing about the kinds of
operations that are supported by
micrograd
what we actually really care about are
neural networks but it turns out that
neural networks are just mathematical
expressions just like this one but
actually slightly bit less crazy even
neural networks are just a mathematical
expression they take the input data as
an input and they take the weights of a
neural network as an input and it's a
mathematical expression and the output
are your predictions of your neural net
or the loss function we'll see this in a
bit but basically neural networks just
happen to be a certain class of
mathematical expressions
but back propagation is actually
significantly more general it doesn't
actually care about neural networks at
all it only tells us about arbitrary
mathematical expressions and then we
happen to use that machinery for
training of neural networks now one more
note i would like to make at this stage
is that as you see here micrograd is a
scalar valued auto grant engine so it's
working on the you know level of
individual scalars like negative four
and two and we're taking neural nets and
we're breaking them down all the way to
these atoms of individual scalars and
all the little pluses and times and it's
just excessive and so obviously you
would never be doing any of this in
production it's really just put down for
pedagogical reasons because it allows us
to not have to deal with these
n-dimensional tensors that you would use
in modern deep neural network library so
this is really done so that you
understand and refactor out back
propagation and chain rule and
understanding of neurologic training
and then if you actually want to train
bigger networks you have to be using
these tensors but none of the math
changes this is done purely for
efficiency we are basically taking scale
value
all the scale values we're packaging
them up into tensors which are just
arrays of these scalars and then because
we have these large arrays we're making
operations on those large arrays that
allows us to take advantage of the
parallelism in a computer and all those
operations can be done in parallel and
then the whole thing runs faster but
really none of the math changes and
that's done purely for efficiency so i
don't think that it's pedagogically
useful to be dealing with tensors from
scratch uh and i think and that's why i
fundamentally wrote micrograd because
you can understand how things work uh at
the fundamental level and then you can
speed it up later okay so here's the fun
part my claim is that micrograd is what
you need to train your networks and
everything else is just efficiency so
you'd think that micrograd would be a
very complex piece of code and that
turns out to not be the case
so if we just go to micrograd
and you'll see that there's only two
files here in micrograd this is the
actual engine it doesn't know anything
about neural nuts and this is the entire
neural nets library
on top of micrograd so engine and nn.pi
so the actual backpropagation autograd
engine
that gives you the power of neural
networks is literally
100 lines of code of like very simple
python
which we'll understand by the end of
this lecture
and then nn.pi
this neural network library built on top
of the autograd engine
um is like a joke it's like
we have to define what is a neuron and
then we have to define what is the layer
of neurons and then we define what is a
multi-layer perceptron which is just a
sequence of layers of neurons and so
it's just a total joke
so basically
there's a lot of power that comes from
only 150 lines of code
and that's all you need to understand to
understand neural network training and
everything else is just efficiency and
of course there's a lot to efficiency
but fundamentally that's all that's
happening okay so now let's dive right
in and implement micrograph step by step
the first thing i'd like to do is i'd
like to make sure that you have a very
good understanding intuitively of what a
derivative is and exactly what
information it gives you so let's start
with some basic imports that i copy
paste in every jupiter notebook always
and let's define a function a scalar
valued function
f of x
as follows
so i just make this up randomly i just
want to scale a valid function that
takes a single scalar x and returns a
single scalar y
and we can call this function of course
so we can pass in say 3.0 and get 20
back
now we can also plot this function to
get a sense of its shape you can tell
from the mathematical expression that
this is probably a parabola it's a
quadratic
and so if we just uh create a set of um
um
scale values that we can feed in using
for example a range from negative five
to five in steps of 0.25
so this is so axis is just from negative
5 to 5 not including 5 in steps of 0.25
and we can actually call this function
on this numpy array as well so we get a
set of y's if we call f on axis
and these y's are basically
also applying a function on every one of
these elements independently
and we can plot this using matplotlib so
plt.plot x's and y's and we get a nice
parabola so previously here we fed in
3.0 somewhere here and we received 20
back which is here the y coordinate so
now i'd like to think through
what is the derivative
of this function at any single input
point x
right so what is the derivative at
different points x of this function now
if you remember back to your calculus
class you've probably derived
derivatives so we take this mathematical
expression 3x squared minus 4x plus 5
and you would write out on a piece of
paper and you would you know apply the
product rule and all the other rules and
derive the mathematical expression of
the great derivative of the original
function and then you could plug in
different texts and see what the
derivative is
we're not going to actually do that
because no one in neural networks
actually writes out the expression for
the neural net it would be a massive
expression um it would be you know
thousands tens of thousands of terms no
one actually derives the derivative of
course and so we're not going to take
this kind of like a symbolic approach
instead what i'd like to do is i'd like
to look at the definition of derivative
and just make sure that we really
understand what derivative is measuring
what it's telling you about the function
and so if we just look up derivative
we see that
okay so this is not a very good
definition of derivative this is a
definition of what it means to be
differentiable
but if you remember from your calculus
it is the limit as h goes to zero of f
of x plus h minus f of x over h so
basically what it's saying is if you
slightly bump up you're at some point x
that you're interested in or a and if
you slightly bump up
you know you slightly increase it by
small number h
how does the function respond with what
sensitivity does it respond what is the
slope at that point does the function go
up or does it go down and by how much
and that's the slope of that function
the
the slope of that response at that point
and so we can basically evaluate
the derivative here numerically by
taking a very small h of course the
definition would ask us to take h to
zero we're just going to pick a very
small h 0.001
and let's say we're interested in point
3.0 so we can look at f of x of course
as 20
and now f of x plus h
so if we slightly nudge x in a positive
direction how is the function going to
respond
and just looking at this do you expect
do you expect f of x plus h to be
slightly greater than 20 or do you
expect to be slightly lower than 20
and since this 3 is here and this is 20
if we slightly go positively the
function will respond positively so
you'd expect this to be slightly greater
than 20. and now by how much it's
telling you the
sort of the
the strength of that slope right the the
size of the slope so f of x plus h minus
f of x this is how much the function
responded
in the positive direction and we have to
normalize by the
run so we have the rise over run to get
the slope so this of course is just a
numerical approximation of the slope
because we have to make age very very
small to converge to the exact amount
now if i'm doing too many zeros
at some point
i'm gonna get an incorrect answer
because we're using floating point
arithmetic and the representations of
all these numbers in computer memory is
finite and at some point we get into
trouble
so we can converse towards the right
answer with this approach
but basically um at 3 the slope is 14.
and you can see that by taking 3x
squared minus 4x plus 5 and
differentiating it in our head
so 3x squared would be
6 x minus 4
and then we plug in x equals 3 so that's
18 minus 4 is 14. so this is correct
so that's
at 3. now how about the slope at say
negative 3
would you expect would you expect for
the slope
now telling the exact value is really
hard but what is the sign of that slope
so at negative three
if we slightly go in the positive
direction at x the function would
actually go down and so that tells you
that the slope would be negative so
we'll get a slight number below
below 20. and so if we take the slope we
expect something negative
negative 22. okay
and at some point here of course the
slope would be zero now for this
specific function i looked it up
previously and it's at point two over
three
so at roughly two over three
uh that's somewhere here
um
this derivative be zero
so basically at that precise point
yeah
at that precise point if we nudge in a
positive direction the function doesn't
respond this stays the same almost and
so that's why the slope is zero okay now
let's look at a bit more complex case
so we're going to start you know
complexifying a bit so now we have a
function
here
with output variable d
that is a function of three scalar
inputs a b and c
so a b and c are some specific values
three inputs into our expression graph
and a single output d
and so if we just print d we get four
and now what i have to do is i'd like to
again look at the derivatives of d with
respect to a b and c
and uh think through uh again just the
intuition of what this derivative is
telling us
so in order to evaluate this derivative
we're going to get a bit hacky here
we're going to again have a very small
value of h
and then we're going to fix the inputs
at some
values that we're interested in
so these are the this is the point abc
at which we're going to be evaluating
the the
derivative of d with respect to all a b
and c at that point
so there are the inputs and now we have
d1 is that expression
and then we're going to for example look
at the derivative of d with respect to a
so we'll take a and we'll bump it by h
and then we'll get d2 to be the exact
same function
and now we're going to print um
you know f1
d1 is d1
d2 is d2
and print slope
so the derivative or slope
here will be um
of course
d2
minus d1 divide h
so d2 minus d1 is how much the function
increased
uh when we bumped
the uh
the specific input that we're interested
in by a tiny amount
and
this is then normalized by h
to get the slope
so
um
yeah
so this so if i just run this we're
going to print
d1
which we know is four
now d2 will be bumped a will be bumped
by h
so let's just think through
a little bit uh what d2 will be uh
printed out here
in particular
d1 will be four
will d2 be a number slightly greater
than four or slightly lower than four
and that's going to tell us the sl the
the sign of the derivative
so
we're bumping a by h
b as minus three c is ten
so you can just intuitively think
through this derivative and what it's
doing a will be slightly more positive
and but b is a negative number
so if a is slightly more positive
because b is negative three
we're actually going to be adding less
to d
so you'd actually expect that the value
of the function will go down
so let's just see this
yeah and so we went from 4
to 3.9996
and that tells you that the slope will
be negative
and then
uh will be a negative number
because we went down
and then
the exact number of slope will be
exact amount of slope is negative 3.
and you can also convince yourself that
negative 3 is the right answer
mathematically and analytically because
if you have a times b plus c and you are
you know you have calculus then
differentiating a times b plus c with
respect to a gives you just b
and indeed the value of b is negative 3
which is the derivative that we have so
you can tell that that's correct
so now if we do this with b
so if we bump b by a little bit in a
positive direction we'd get different
slopes so what is the influence of b on
the output d
so if we bump b by a tiny amount in a
positive direction then because a is
positive
we'll be adding more to d
right
so um and now what is the what is the
sensitivity what is the slope of that
addition
and it might not surprise you that this
should be
2
and y is a 2 because d of d
by db differentiating with respect to b
would be would give us a
and the value of a is two so that's also
working well
and then if c gets bumped a tiny amount
in h
by h
then of course a times b is unaffected
and now c becomes slightly bit higher
what does that do to the function it
makes it slightly bit higher because
we're simply adding c
and it makes it slightly bit higher by
the exact same amount that we added to c
and so that tells you that the slope is
one
that will be the
the rate at which
d will increase as we scale
c
okay so we now have some intuitive sense
of what this derivative is telling you
about the function and we'd like to move
to neural networks now as i mentioned
neural networks will be pretty massive
expressions mathematical expressions so
we need some data structures that
maintain these expressions and that's
what we're going to start to build out
now
so we're going to
build out this value object that i
showed you in the readme page of
micrograd
so let me copy paste a skeleton of the
first very simple value object
so class value takes a single
scalar value that it wraps and keeps
track of
and that's it so
we can for example do value of 2.0 and
then we can
get we can look at its content and
python will internally
use the wrapper function
to uh return
uh this string oops
like that
so this is a value object with data
equals two that we're creating here
now we'd like to do is like we'd like to
be able to
have not just like two values
but we'd like to do a bluffy right we'd
like to add them
so currently you would get an error
because python doesn't know how to add
two value objects so we have to tell it
so here's
addition
so you have to basically use these
special double underscore methods in
python to define these operators for
these objects so if we call um
the uh if we use this plus operator
python will internally call a dot add of
b
that's what will happen internally and
so b will be the other and
self will be a
and so we see that what we're going to
return is a new value object and it's
just it's going to be wrapping
the plus of
their data
but remember now because data is the
actual like numbered python number so
this operator here is just the typical
floating point plus addition now it's
not an addition of value objects
and will return a new value so now a
plus b should work and it should print
value of
negative one
because that's two plus minus three
there we go
okay let's now implement multiply
just so we can recreate this expression
here
so multiply i think it won't surprise
you will be fairly similar
so instead of add we're going to be
using mul
and then here of course we want to do
times
and so now we can create a c value
object which will be 10.0 and now we
should be able to do a times b well
let's just do a times b first
um
[Music]
that's value of negative six now
and by the way i skipped over this a
little bit suppose that i didn't have
the wrapper function here
then it's just that you'll get some kind
of an ugly expression so what wrapper is
doing is it's providing us a way to
print out like a nicer looking
expression in python
uh so we don't just have something
cryptic we actually are you know it's
value of
negative six so this gives us a times
and then this we should now be able to
add c to it because we've defined and
told the python how to do mul and add
and so this will call this will
basically be equivalent to a dot
small
of b
and then this new value object will be
dot add
of c
and so let's see if that worked
yep so that worked well that gave us
four which is what we expect from before
and i believe we can just call them
manually as well there we go so
yeah
okay so now what we are missing is the
connective tissue of this expression as
i mentioned we want to keep these
expression graphs so we need to know and
keep pointers about what values produce
what other values
so here for example we are going to
introduce a new variable which we'll
call children and by default it will be
an empty tuple
and then we're actually going to keep a
slightly different variable in the class
which we'll call underscore prev which
will be the set of children
this is how i done i did it in the
original micrograd looking at my code
here i can't remember exactly the reason
i believe it was efficiency but this
underscore children will be a tuple for
convenience but then when we actually
maintain it in the class it will be just
this set yeah i believe for efficiency
um
so now
when we are creating a value like this
with a constructor children will be
empty and prep will be the empty set but
when we're creating a value through
addition or multiplication we're going
to feed in the children of this value
which in this case is self and other
so those are the children
here
so now we can do d dot prev
and we'll see that the children of the
we now know are this value of negative 6
and value of 10 and this of course is
the value resulting from a times b and
the c value which is 10.
now the last piece of information we
don't know so we know that the children
of every single value but we don't know
what operation created this value
so we need one more element here let's
call it underscore pop
and by default this is the empty set for
leaves
and then we'll just maintain it here
and now the operation will be just a
simple string and in the case of
addition it's plus in the case of
multiplication is times
so now we
not just have d dot pref we also have a
d dot up
and we know that d was produced by an
addition of those two values and so now
we have the full
mathematical expression uh and we're
building out this data structure and we
know exactly how each value came to be
by word expression and from what other
values
now because these expressions are about
to get quite a bit larger we'd like a
way to nicely visualize these
expressions that we're building out so
for that i'm going to copy paste a bunch
of slightly scary code that's going to
visualize this these expression graphs
for us
so here's the code and i'll explain it
in a bit but first let me just show you
what this code does
basically what it does is it creates a
new function drawdot that we can call on
some root node
and then it's going to visualize it so
if we call drawdot on d
which is this final value here that is a
times b plus c
it creates something like this so this
is d
and you see that this is a times b
creating an integrated value plus c
gives us this output node d
so that's dried out of d
and i'm not going to go through this in
complete detail you can take a look at
graphless and its api uh graphis is a
open source graph visualization software
and what we're doing here is we're
building out this graph and graphis
api and
you can basically see that trace is this
helper function that enumerates all of
the nodes and edges in the graph
so that just builds a set of all the
nodes and edges and then we iterate for
all the nodes and we create special node
objects
for them in
using dot node
and then we also create edges using dot
dot edge
and the only thing that's like slightly
tricky here is you'll notice that i
basically add these fake nodes which are
these operation nodes so for example
this node here is just like a plus node
and
i create these
special op nodes here
and i connect them accordingly so these
nodes of course are not actual
nodes in the original graph
they're not actually a value object the
only value objects here are the things
in squares those are actual value
objects or representations thereof and
these op nodes are just created in this
drawdot routine so that it looks nice
let's also add labels to these graphs
just so we know what variables are where
so let's create a special underscore
label
um
or let's just do label
equals empty by default and save it in
each node
and then here we're going to do label as
a
label is the
label a c
and then
let's create a special um
e equals a times b
and e dot label will be e
it's kind of naughty
and e will be e plus c
and a d dot label will be
d
okay so nothing really changes i just
added this new e function
a new e variable
and then here when we are
printing this
i'm going to print the label here so
this will be a percent s
bar
and this will be end.label
and so now
we have the label on the left here so it
says a b creating e and then e plus c
creates d
just like we have it here
and finally let's make this expression
just one layer deeper
so d will not be the final output node
instead after d we are going to create a
new value object
called f we're going to start running
out of variables soon f will be negative
2.0
and its label will of course just be f
and then l capital l will be the output
of our graph
and l will be p times f
okay
so l will be negative eight is the
output
so
now we don't just draw a d we draw l
okay
and somehow the label of
l was undefined oops all that label has
to be explicitly sort of given to it
there we go so l is the output
so let's quickly recap what we've done
so far
we are able to build out mathematical
expressions using only plus and times so
far
they are scalar valued along the way
and we can do this forward pass
and build out a mathematical expression
so we have multiple inputs here a b c
and f
going into a mathematical expression
that produces a single output l
and this here is visualizing the forward
pass so the output of the forward pass
is negative eight that's the value
now what we'd like to do next is we'd
like to run back propagation
and in back propagation we are going to
start here at the end and we're going to
reverse
and calculate the gradient along along
all these intermediate values
and really what we're computing for
every single value here
um we're going to compute the derivative
of that node with respect to l
so
the derivative of l with respect to l is
just uh one
and then we're going to derive what is
the derivative of l with respect to f
with respect to d with respect to c with
respect to e
with respect to b and with respect to a
and in the neural network setting you'd
be very interested in the derivative of
basically this loss function l
with respect to the weights of a neural
network
and here of course we have just these
variables a b c and f
but some of these will eventually
represent the weights of a neural net
and so we'll need to know how those
weights are impacting
the loss function so we'll be interested
basically in the derivative of the
output with respect to some of its leaf
nodes and those leaf nodes will be the
weights of the neural net
and the other leaf nodes of course will
be the data itself but usually we will
not want or use the derivative of the
loss function with respect to data
because the data is fixed but the
weights will be iterated on
using the gradient information so next
we are going to create a variable inside
the value class that maintains the
derivative of l with respect to that
value
and we will call this variable grad
so there's a data and there's a
self.grad
and initially it will be zero and
remember that zero is basically means no
effect so at initialization we're
assuming that every value does not
impact does not affect the out the
output
right because if the gradient is zero
that means that changing this variable
is not changing the loss function
so by default we assume that the
gradient is zero
and then
now that we have grad and it's 0.0
we are going to be able to visualize it
here after data so here grad is 0.4 f
and this will be in that graph
and now we are going to be showing both
the data and the grad
initialized at zero
and we are just about getting ready to
calculate the back propagation
and of course this grad again as i
mentioned is representing
the derivative of the output in this
case l with respect to this value so
with respect to so this is the
derivative of l with respect to f with
respect to d and so on so let's now fill
in those gradients and actually do back
propagation manually so let's start
filling in these gradients and start all
the way at the end as i mentioned here
first we are interested to fill in this
gradient here so what is the derivative
of l with respect to l
in other words if i change l by a tiny
amount of h
how much does
l change
it changes by h so it's proportional and
therefore derivative will be one
we can of course measure these or
estimate these numerical gradients
numerically just like we've seen before
so if i take this expression
and i create a def lol function here
and put this here now the reason i'm
creating a gating function hello here is
because i don't want to pollute or mess
up the global scope here this is just
kind of like a little staging area and
as you know in python all of these will
be local variables to this function so
i'm not changing any of the global scope
here
so here l1 will be l
and then copy pasting this expression
we're going to add a small amount h
in for example a
right and this would be measuring the
derivative of l with respect to a
so here this will be l2
and then we want to print this
derivative so print
l2 minus l1 which is how much l changed
and then normalize it by h so this is
the rise over run
and we have to be careful because l is a
value node so we actually want its data
um
so that these are floats dividing by h
and this should print the derivative of
l with respect to a because a is the one
that we bumped a little bit by h
so what is the
derivative of l with respect to a
it's six
okay and obviously
if we change l by h
then that would be
here effectively
this looks really awkward but changing l
by h
you see the derivative here is 1. um
that's kind of like the base case of
what we are doing here
so basically we cannot come up here and
we can manually set l.grad to one this
is our manual back propagation
l dot grad is one and let's redraw
and we'll see that we filled in grad as
1 for l
we're now going to continue the back
propagation so let's here look at the
derivatives of l with respect to d and f
let's do a d first
so what we are interested in if i create
a markdown on here is we'd like to know
basically we have that l is d times f
and we'd like to know what is uh d
l by d d
what is that
and if you know your calculus uh l is d
times f so what is d l by d d it would
be f
and if you don't believe me we can also
just derive it because the proof would
be fairly straightforward uh we go to
the
definition of the derivative which is f
of x plus h minus f of x divide h
as a limit limit of h goes to zero of
this kind of expression so when we have
l is d times f
then increasing d by h
would give us the output of b plus h
times f
that's basically f of x plus h right
minus d times f
and then divide h and symbolically
expanding out here we would have
basically d times f plus h times f minus
t times f divide h
and then you see how the df minus df
cancels so you're left with h times f
divide h
which is f
so in the limit as h goes to zero of
you know
derivative
definition we just get f in the case of
d times f
so
symmetrically
dl by d
f will just be d
so what we have is that f dot grad
we see now is just the value of d
which is 4.
and we see that
d dot grad
is just uh the value of f
and so the value of f is negative two
so we'll set those manually
let me erase this markdown node and then
let's redraw what we have
okay
and let's just make sure that these were
correct so we seem to think that dl by
dd is negative two so let's double check
um let me erase this plus h from before
and now we want the derivative with
respect to f
so let's just come here when i create f
and let's do a plus h here and this
should print the derivative of l with
respect to f so we expect to see four
yeah and this is four up to floating
point
funkiness
and then dl by dd
should be f which is negative two
grad is negative two
so if we again come here and we change d
d dot data plus equals h right here
so we expect so we've added a little h
and then we see how l changed and we
expect to print
uh negative two
there we go
so we've numerically verified what we're
doing here is what kind of like an
inline gradient check gradient check is
when we
are deriving this like back propagation
and getting the derivative with respect
to all the intermediate results and then
numerical gradient is just you know
estimating it using small step size
now we're getting to the crux of
backpropagation so this will be the most
important node to understand because if
you understand the gradient for this
node you understand all of back
propagation and all of training of
neural nets basically
so we need to derive dl by bc
in other words the derivative of l with
respect to c
because we've computed all these other
gradients already
now we're coming here and we're
continuing the back propagation manually
so we want dl by dc and then we'll also
derive dl by de
now here's the problem
how do we derive dl
by dc
we actually know the derivative l with
respect to d so we know how l assessed
it to d
but how is l sensitive to c so if we
wiggle c how does that impact l
through d
so we know dl by dc
and we also here know how c impacts d
and so just very intuitively if you know
the impact that c is having on d and the
impact that d is having on l
then you should be able to somehow put
that information together to figure out
how c impacts l
and indeed this is what we can actually
do so in particular we know just
concentrating on d first let's look at
how what is the derivative basically of
d with respect to c so in other words
what is dd by dc
so here we know that d is c times c plus
e
that's what we know and now we're
interested in dd by dc
if you just know your calculus again and
you remember that differentiating c plus
e with respect to c you know that that
gives you
1.0
and we can also go back to the basics
and derive this because again we can go
to our f of x plus h minus f of x
divide by h
that's the definition of a derivative as
h goes to zero
and so here
focusing on c and its effect on d
we can basically do the f of x plus h
will be
c is incremented by h plus e
that's the first evaluation of our
function minus
c plus e
and then divide h
and so what is this
uh just expanding this out this will be
c plus h plus e minus c minus e
divide h and then you see here how c
minus c cancels e minus e cancels we're
left with h over h which is 1.0
and so
by symmetry also d d by d
e
will be 1.0 as well
so basically the derivative of a sum
expression is very simple and and this
is the local derivative so i call this
the local derivative because we have the
final output value all the way at the
end of this graph and we're now like a
small node here
and this is a little plus node
and it the little plus node doesn't know
anything about the rest of the graph
that it's embedded in all it knows is
that it did a plus it took a c and an e
added them and created d
and this plus note also knows the local
influence of c on d or rather rather the
derivative of d with respect to c and it
also
knows the derivative of d with respect
to e but that's not what we want that's
just a local derivative what we actually
want is d l by d c and l could l is here
just one step away but in a general case
this little plus note is could be
embedded in like a massive graph
so
again we know how l impacts d and now we
know how c and e impact d how do we put
that information together to write dl by
dc and the answer of course is the chain
rule in calculus
and so um
i pulled up a chain rule here from
kapedia
and
i'm going to go through this very
briefly so chain rule
wikipedia sometimes can be very
confusing and calculus can
can be very confusing like this is the
way i
learned
chain rule and it was very confusing
like what is happening it's just
complicated so i like this expression
much better
if a variable z depends on a variable y
which itself depends on the variable x
then z depends on x as well obviously
through the intermediate variable y
in this case the chain rule is expressed
as
if you want dz by dx
then you take the dz by dy and you
multiply it by d y by dx
so the chain rule fundamentally is
telling you
how
we chain these
uh derivatives together
correctly so to differentiate through a
function composition
we have to apply a multiplication
of
those derivatives
so that's really what chain rule is
telling us
and there's a nice little intuitive
explanation here which i also think is
kind of cute the chain rule says that
knowing the instantaneous rate of change
of z with respect to y and y relative to
x allows one to calculate the
instantaneous rate of change of z
relative to x
as a product of those two rates of
change
simply the product of those two
so here's a good one
if a car travels twice as fast as
bicycle and the bicycle is four times as
fast as walking man
then the car travels two times four
eight times as fast as demand
and so this makes it very clear that the
correct thing to do sort of
is to multiply
so
cars twice as fast as bicycle and
bicycle is four times as fast as man
so the car will be eight times as fast
as the man and so we can take these
intermediate rates of change if you will
and multiply them together
and that justifies the
chain rule intuitively so have a look at
chain rule about here really what it
means for us is there's a very simple
recipe for deriving what we want which
is dl by dc
and what we have so far
is we know
want
and we know
what is the
impact of d on l so we know d l by
d d the derivative of l with respect to
d d we know that that's negative two
and now because of this local
reasoning that we've done here we know
dd by d
c
so how does c impact d and in
particular this is a plus node so the
local derivative is simply 1.0 it's very
simple
and so
the chain rule tells us that dl by dc
going through this intermediate variable
will just be simply d l by
d
times
dd by dc
that's chain rule
so this is identical to what's happening
here
except
z is rl
y is our d and x is rc
so we literally just have to multiply
these
and because
these local derivatives like dd by dc
are just one
we basically just copy over dl by dd
because this is just times one
so what does it do so because dl by dd
is negative two what is dl by dc
well it's the local gradient 1.0 times
dl by dd which is negative two
so literally what a plus node does you
can look at it that way is it literally
just routes the gradient
because the plus nodes local derivatives
are just one and so in the chain rule
one times
dl by dd
is um
is uh is just dl by dd and so that
derivative just gets routed to both c
and to e in this case
so basically um we have that that grad
or let's start with c since that's the
one we looked at
is
negative two times one
negative two
and in the same way by symmetry e that
grad will be negative two that's the
claim so we can set those
we can redraw
and you see how we just assign negative
to negative two so this backpropagating
signal which is carrying the information
of like what is the derivative of l with
respect to all the intermediate nodes
we can imagine it almost like flowing
backwards through the graph and a plus
node will simply distribute the
derivative to all the leaf nodes sorry
to all the children nodes of it
so this is the claim and now let's
verify it so let me remove the plus h
here from before
and now instead what we're going to do
is we're going to increment c so c dot
data will be credited by h
and when i run this we expect to see
negative 2
negative 2. and then of course for e
so e dot data plus equals h and we
expect to see negative 2.
simple
so those are the derivatives of these
internal nodes
and now we're going to recurse our way
backwards again
and we're again going to apply the chain
rule so here we go our second
application of chain rule and we will
apply it all the way through the graph
we just happen to only have one more
node remaining
we have that d l
by d e
as we have just calculated is negative
two so we know that
so we know the derivative of l with
respect to e
and now we want dl
by
da
right
and the chain rule is telling us that
that's just dl by de
negative 2
times the local gradient so what is the
local gradient basically d e
by d a
we have to look at that
so i'm a little times node
inside a massive graph
and i only know that i did a times b and
i produced an e
so now what is d e by d a and d e by d b
that's the only thing that i sort of
know about that's my local gradient
so
because we have that e's a times b we're
asking what is d e by d a
and of course we just did that here we
had a
times so i'm not going to rederive it
but if you want to differentiate this
with respect to a you'll just get b
right the value of b
which in this case is negative 3.0
so
basically we have that dl by da
well let me just do it right here we
have that a dot grad and we are applying
chain rule here
is d l by d e which we see here is
negative two
times
what is d e by d a
it's the value of b which is negative 3.
that's it
and then we have b grad is again dl by
de
which is negative 2
just the same way
times
what is d e by d
um db
is the value of a which is 2.2.0
as the value of a
so these are our claimed derivatives
let's
redraw
and we see here that
a dot grad turns out to be 6 because
that is negative 2 times negative 3
and b dot grad is negative 4
times sorry is negative 2 times 2 which
is negative 4.
so those are our claims let's delete
this and let's verify them
we have
a here a dot data plus equals h
so the claim is that
a dot grad is six
let's verify
six
and we have beta data
plus equals h
so nudging b by h
and looking at what happens
we claim it's negative four
and indeed it's negative four plus minus
again float oddness
um
and uh
that's it this
that was the manual
back propagation
uh all the way from here to all the leaf
nodes and we've done it piece by piece
and really all we've done is as you saw
we iterated through all the nodes one by
one and locally applied the chain rule
we always know what is the derivative of
l with respect to this little output and
then we look at how this output was
produced this output was produced
through some operation and we have the
pointers to the children nodes of this
operation
and so in this little operation we know
what the local derivatives are and we
just multiply them onto the derivative
always
so we just go through and recursively
multiply on the local derivatives and
that's what back propagation is is just
a recursive application of chain rule
backwards through the computation graph
let's see this power in action just very
briefly what we're going to do is we're
going to
nudge our inputs to try to make l go up
so in particular what we're doing is we
want a.data we're going to change it
and if we want l to go up that means we
just have to go in the direction of the
gradient so
a
should increase in the direction of
gradient by like some small step amount
this is the step size
and we don't just want this for ba but
also for b
also for c
also for f
those are
leaf nodes which we usually have control
over
and if we nudge in direction of the
gradient we expect a positive influence
on l
so we expect l to go up
positively
so it should become less negative it
should go up to say negative you know
six or something like that
uh it's hard to tell exactly and we'd
have to rewrite the forward pass so let
me just um
do that here
um
this would be the forward pass f would
be unchanged this is effectively the
forward pass and now if we print l.data
we expect because we nudged all the
values all the inputs in the rational
gradient we expected a less negative l
we expect it to go up
so maybe it's negative six or so let's
see what happens
okay negative seven
and uh this is basically one step of an
optimization that we'll end up running
and really does gradient just give us
some power because we know how to
influence the final outcome and this
will be extremely useful for training
knowledge as well as you'll see
so now i would like to do one more uh
example of manual backpropagation using
a bit more complex and uh useful example
we are going to back propagate through a
neuron
so
we want to eventually build up neural
networks and in the simplest case these
are multilateral perceptrons as they're
called so this is a two layer neural net
and it's got these hidden layers made up
of neurons and these neurons are fully
connected to each other
now biologically neurons are very
complicated devices but we have very
simple mathematical models of them
and so this is a very simple
mathematical model of a neuron you have
some inputs axis
and then you have these synapses that
have weights on them so
the w's are weights
and then
the synapse interacts with the input to
this neuron multiplicatively so what
flows to the cell body
of this neuron is w times x
but there's multiple inputs so there's
many w times x's flowing into the cell
body
the cell body then has also like some
bias
so this is kind of like the
inert innate sort of trigger happiness
of this neuron so this bias can make it
a bit more trigger happy or a bit less
trigger happy regardless of the input
but basically we're taking all the w
times x
of all the inputs adding the bias and
then we take it through an activation
function
and this activation function is usually
some kind of a squashing function
like a sigmoid or 10h or something like
that so as an example
we're going to use the 10h in this
example
numpy has a
np.10h
so
we can call it on a range
and we can plot it
this is the 10h function and you see
that the inputs as they come in
get squashed on the y coordinate here so
um
right at zero we're going to get exactly
zero and then as you go more positive in
the input
then you'll see that the function will
only go up to one and then plateau out
and so if you pass in very positive
inputs we're gonna cap it smoothly at
one and on the negative side we're gonna
cap it smoothly to negative one
so that's 10h
and that's the squashing function or an
activation function and what comes out
of this neuron is just the activation
function applied to the dot product of
the weights and the
inputs
so let's
write one out
um
i'm going to copy paste because
i don't want to type too much
but okay so here we have the inputs
x1 x2 so this is a two-dimensional
neuron so two inputs are going to come
in
these are thought out as the weights of
this neuron
weights w1 w2 and these weights again
are the synaptic strengths for each
input
and this is the bias of the neuron
b
and now we want to do is according to
this model we need to multiply x1 times
w1
and x2 times w2
and then we need to add bias on top of
it
and it gets a little messy here but all
we are trying to do is x1 w1 plus x2 w2
plus b
and these are multiply here
except i'm doing it in small steps so
that we actually have pointers to all
these intermediate nodes so we have x1
w1 variable x times x2 w2 variable and
i'm also labeling them
so n is now
the cell body raw
raw
activation without
the activation function for now
and this should be enough to basically
plot it so draw dot of n
gives us x1 times w1 x2 times w2
being added
then the bias gets added on top of this
and this n
is this sum
so we're now going to take it through an
activation function
and let's say we use the 10h
so that we produce the output
so what we'd like to do here is we'd
like to do the output and i'll call it o
is um
n dot 10h
okay but we haven't yet written the 10h
now the reason that we need to implement
another 10h function here is that
tanh is a
hyperbolic function and we've only so
far implemented a plus and the times and
you can't make a 10h out of just pluses
and times
you also need exponentiation so 10h is
this kind of a formula here
you can use either one of these and you
see that there's exponentiation involved
which we have not implemented yet for
our low value node here so we're not
going to be able to produce 10h yet and
we have to go back up and implement
something like it
now one option here
is we could actually implement um
exponentiation
right and we could return the x of a
value instead of a 10h of a value
because if we had x then we have
everything else that we need so um
because we know how to add and we know
how to
um
we know how to add and we know how to
multiply so we'd be able to create 10h
if we knew how to x
but for the purposes of this example i
specifically wanted to
show you
that we don't necessarily need to have
the most atomic pieces
in
um
in this value object we can actually
like create functions at arbitrary
points of abstraction they can be
complicated functions but they can be
also very very simple functions like a
plus and it's totally up to us the only
thing that matters is that we know how
to differentiate through any one
function so we take some inputs and we
make an output the only thing that
matters it can be arbitrarily complex
function as long as you know how to
create the local derivative if you know
the local derivative of how the inputs
impact the output then that's all you
need so we're going to cluster up
all of this expression and we're not
going to break it down to its atomic
pieces we're just going to directly
implement tanh
so let's do that
depth nh
and then out will be a value
of
and we need this expression here so
um
let me actually
copy paste
let's grab n which is a cell.theta
and then this
i believe is the tan h
math.x of
two
no n
n minus one over
two n plus one
maybe i can call this x
just so that it matches exactly
okay and now
this will be t
and uh children of this node there's
just one child
and i'm wrapping it in a tuple so this
is a tuple of one object just self
and here the name of this operation will
be 10h
and we're going to return that
okay
so now valley should be implementing 10h
and now we can scroll all the way down
here
and we can actually do n.10 h and that's
going to return the tanhd
output of n
and now we should be able to draw it out
of o not of n
so let's see how that worked
there we go
n went through 10 h
to produce this output
so now tan h is a
sort of
our little micro grad supported node
here as an operation
and as long as we know the derivative of
10h
then we'll be able to back propagate
through it now let's see this 10h in
action currently it's not squashing too
much because the input to it is pretty
low so if the bias was increased to say
eight
then we'll see that what's flowing into
the 10h now is
two
and 10h is squashing it to 0.96 so we're
already hitting the tail of this 10h and
it will sort of smoothly go up to 1 and
then plateau out over there
okay so now i'm going to do something
slightly strange i'm going to change
this bias from 8 to this number
6.88 etc
and i'm going to do this for specific
reasons because we're about to start
back propagation
and i want to make sure that our numbers
come out nice they're not like very
crazy numbers they're nice numbers that
we can sort of understand in our head
let me also add a pose label
o is short for output here
so that's zero
okay so
0.88 flows into 10 h comes out 0.7 so on
so now we're going to do back
propagation and we're going to fill in
all the gradients
so what is the derivative o with respect
to
all the
inputs here and of course in the typical
neural network setting what we really
care about the most is the derivative of
these neurons on the weights
specifically the w2 and w1 because those
are the weights that we're going to be
changing part of the optimization
and the other thing that we have to
remember is here we have only a single
neuron but in the neural natives
typically have many neurons and they're
connected
so this is only like a one small neuron
a piece of a much bigger puzzle and
eventually there's a loss function that
sort of measures the accuracy of the
neural net and we're back propagating
with respect to that accuracy and trying
to increase it
so let's start off by propagation here
in the end
what is the derivative of o with respect
to o the base case sort of we know
always is that the gradient is just 1.0
so let me fill it in
and then let me
split out
the drawing function
here
and then here cell
clear this output here okay
so now when we draw o we'll see that oh
that grad is one
so now we're going to back propagate
through the tan h
so to back propagate through 10h we need
to know the local derivative of 10h
so if we have that
o is 10 h of
n
then what is d o by d n
now what you could do is you could come
here and you could take this expression
and you could
do your calculus derivative taking
um and that would work but we can also
just scroll down wikipedia here
into a section that hopefully tells us
that derivative uh
d by dx of 10 h of x is
any of these i like this one 1 minus 10
h square of x
so this is 1 minus 10 h
of x squared
so basically what this is saying is that
d o by d n
is
1 minus 10 h
of n
squared
and we already have 10 h of n that's
just o
so it's one minus o squared
so o is the output here so the output is
this number
data
is this number
and then
what this is saying is that do by dn is
1 minus
this squared so
one minus of that data squared
is 0.5 conveniently
so the local derivative of this 10 h
operation here is 0.5
and
so that would be d o by d n
so
we can fill in that in that grad
is 0.5 we'll just fill in
so this is exactly 0.5 one half
so now we're going to continue the back
propagation
this is 0.5 and this is a plus node
so how is backprop going to what is that
going to do here
and if you remember our previous example
a plus is just a distributor of gradient
so this gradient will simply flow to
both of these equally and that's because
the local derivative of this operation
is one for every one of its nodes so 1
times 0.5 is 0.5
so therefore we know that
this node here which we called this
its grad is just 0.5
and we know that b dot grad is also 0.5
so let's set those and let's draw
so 0.5
continuing we have another plus
0.5 again we'll just distribute it so
0.5 will flow to both of these
so we can set
theirs
x2w2 as well that grad is 0.5
and let's redraw pluses are my favorite
uh operations to back propagate through
because
it's very simple
so now it's flowing into these
expressions is 0.5 and so really again
keep in mind what the derivative is
telling us at every point in time along
here this is saying that
if we want the output of this neuron to
increase
then
the influence on these expressions is
positive on the output both of them are
positive
contribution to the output
so now back propagating to x2 and w2
first
this is a times node so we know that the
local derivative is you know the other
term
so if we want to calculate x2.grad
then
can you think through what it's going to
be
so x2.grad will be
w2.data
times this x2w2
by grad right
and
w2.grad will be
x2 that data times x2w2.grad
right so that's the local piece of chain
rule
let's set them and let's redraw
so here we see that the gradient on our
weight 2 is 0 because x2 data was 0
right but x2 will have the gradient 0.5
because data here was 1.
and so what's interesting here right is
because the input x2 was 0 then because
of the way the times works
of course this gradient will be zero and
think about intuitively why that is
derivative always tells us the influence
of
this on the final output if i wiggle w2
how is the output changing
it's not changing because we're
multiplying by zero
so because it's not changing there's no
derivative and zero is the correct
answer
because we're
squashing it at zero
and let's do it here point five should
come here and flow through this times
and so we'll have that x1.grad is
can you think through a little bit what
what
this should be
the local derivative of times
with respect to x1 is going to be w1
so w1 is data times
x1 w1 dot grad
and w1.grad will be x1.data times
x1 w2 w1 with graph
let's see what those came out to be
so this is 0.5 so this would be negative
1.5 and this would be 1.
and we've back propagated through this
expression these are the actual final
derivatives so if we want this neuron's
output to increase
we know that what's necessary is that
w2 we have no gradient w2 doesn't
actually matter to this neuron right now
but this neuron this weight should uh go
up
so if this weight goes up then this
neuron's output would have gone up and
proportionally because the gradient is
one okay so doing the back propagation
manually is obviously ridiculous so we
are now going to put an end to this
suffering and we're going to see how we
can implement uh the backward pass a bit
more automatically we're not going to be
doing all of it manually out here
it's now pretty obvious to us by example
how these pluses and times are back
property ingredients so let's go up to
the value
object and we're going to start
codifying what we've seen
in the examples below
so we're going to do this by storing a
special cell dot backward
and underscore backward and this will be
a function which is going to do that
little piece of chain rule at each
little node that compute that took
inputs and produced output uh we're
going to store
how we are going to chain the the
outputs gradient into the inputs
gradients
so by default
this will be a function
that uh doesn't do anything
so um
and you can also see that here in the
value in micrograb
so
with this backward function by default
doesn't do anything
this is an empty function
and that would be sort of the case for
example for a leaf node for leaf node
there's nothing to do
but now if when we're creating these out
values these out values are an addition
of self and other
and so we will want to sell set
outs backward to be
the function that propagates the
gradient
so
let's define what should happen
and we're going to store it in a closure
let's define what should happen when we
call
outs grad
for in addition
our job is to take
outs grad and propagate it into self's
grad and other grad so basically we want
to sell self.grad to something
and we want to set others.grad to
something
okay
and the way we saw below how chain rule
works we want to take the local
derivative times
the
sort of global derivative i should call
it which is the derivative of the final
output of the expression with respect to
out's data
with respect to out
so
the local derivative of self in an
addition is 1.0
so it's just 1.0 times
outs grad
that's the chain rule
and others.grad will be 1.0 times
outgrad
and what you basically what you're
seeing here is that outscrad
will simply be copied onto selfs grad
and others grad as we saw happens for an
addition operation
so we're going to later call this
function to propagate the gradient
having done an addition
let's now do multiplication we're going
to also define that backward
and we're going to set its backward to
be backward
and we want to chain outgrad into
self.grad
and others.grad
and this will be a little piece of chain
rule for multiplication
so we'll have
so what should this be
can you think through
so what is the local derivative
here the local derivative was
others.data
and then
oops others.data and the times of that
grad that's channel
and here we have self.data times of that
grad
that's what we've been doing
and finally here for 10 h
left backward
and then we want to set out backwards to
be just backward
and here we need to
back propagate we have out that grad and
we want to chain it into self.grad
and salt.grad will be
the local derivative of this operation
that we've done here which is 10h
and so we saw that the local the
gradient is 1 minus the tan h of x
squared which here is t
that's the local derivative because
that's t is the output of this 10 h so 1
minus t squared is the local derivative
and then gradient um
has to be multiplied because of the
chain rule
so outgrad is chained through the local
gradient into salt.grad
and that should be basically it so we're
going to redefine our value node
we're going to swing all the way down
here
and we're going to
redefine
our expression
make sure that all the grads are zero
okay
but now we don't have to do this
manually anymore
we are going to basically be calling the
dot backward in the right order
so
first we want to call os
dot backwards
so o was the outcome of 10h
right so calling all that those who's
backward
will be
this function this is what it will do
now we have to be careful because
there's a times out.grad
and out.grad remember is initialized to
zero
so here we see grad zero so as a base
case we need to set both.grad to 1.0
to initialize this with 1
and then once this is 1 we can call oda
backward
and what that should do is it should
propagate this grad through 10h
so the local derivative times
the global derivative which is
initialized at one so
this should
um
a dope
so i thought about redoing it but i
figured i should just leave the error in
here because it's pretty funny why is
anti-object not callable
uh it's because
i screwed up we're trying to save these
functions so this is correct
this here
we don't want to call the function
because that returns none these
functions return none we just want to
store the function
so let me redefine the value object
and then we're going to come back in
redefine the expression draw a dot
everything is great o dot grad is one
o dot grad is one and now
now this should work of course
okay so all that backward should
this grant should now be 0.5 if we
redraw and if everything went correctly
0.5 yay
okay so now we need to call ns.grad
and it's not awkward sorry
ends backward
so that seems to have worked
so instead backward routed the gradient
to both of these so this is looking
great
now we could of course called uh called
b grad
beat up backwards sorry
what's gonna happen
well b doesn't have it backward b is
backward
because b is a leaf node
b's backward is by initialization the
empty function
so nothing would happen but we can call
call it on it
but when we call
this one
it's backward
then we expect this 0.5 to get further
routed
right so there we go 0.5.5
and then finally
we want to call
it here on x2 w2
and on x1 w1
do both of those
and there we go
so we get 0 0.5 negative 1.5 and 1
exactly as we did before but now
we've done it through
calling that backward um
sort of manually
so we have the lamp one last piece to
get rid of which is us calling
underscore backward manually so let's
think through what we are actually doing
um
we've laid out a mathematical expression
and now we're trying to go backwards
through that expression
um so going backwards through the
expression just means that we never want
to call a dot backward for any node
before
we've done a sort of um everything after
it
so we have to do everything after it
before we're ever going to call that
backward on any one node we have to get
all of its full dependencies everything
that it depends on has to
propagate to it before we can continue
back propagation so this ordering of
graphs can be achieved using something
called topological sort
so topological sort
is basically a laying out of a graph
such that all the edges go only from
left to right basically
so here we have a graph it's a directory
a cyclic graph a dag
and this is two different topological
orders of it i believe where basically
you'll see that it's laying out of the
notes such that all the edges go only
one way from left to right
and implementing topological sort you
can look in wikipedia and so on i'm not
going to go through it in detail
but basically this is what builds a
topological graph
we maintain a set of visited nodes and
then we are
going through starting at some root node
which for us is o that's where we want
to start the topological sort
and starting at o we go through all of
its children and we need to lay them out
from left to right
and basically this starts at o
if it's not visited then it marks it as
visited and then it iterates through all
of its children
and calls build topological on them
and then uh after it's gone through all
the children it adds itself
so basically
this node that we're going to call it on
like say o is only going to add itself
to the topo list after all of the
children have been processed and that's
how this function is guaranteeing
that you're only going to be in the list
once all your children are in the list
and that's the invariant that is being
maintained so if we built upon o and
then inspect this list
we're going to see that it ordered our
value objects
and the last one
is the value of 0.707 which is the
output
so this is o and then this is n
and then all the other nodes get laid
out before it
so that builds the topological graph and
really what we're doing now is we're
just calling dot underscore backward on
all of the nodes in a topological order
so if we just reset the gradients
they're all zero
what did we do
we started by
setting o dot grad
to b1
that's the base case
then we built the topological order
and then we went for node
in
reversed
of topo
now
in in the reverse order because this
list goes from
you know we need to go through it in
reversed order
so starting at o
note that backward
and this should be
it
there we go
those are the correct derivatives
finally we are going to hide this
functionality
so i'm going to
copy this and we're going to hide it
inside the valley class because we don't
want to have all that code lying around
so instead of an underscore backward
we're now going to define an actual
backward so that's backward without the
underscore
and that's going to do all the stuff
that we just arrived
so let me just clean this up a little
bit so
we're first going to
build a topological graph
starting at self
so build topo of self
will populate the topological order into
the topo list which is a local variable
then we set self.grad to be one
and then for each node in the reversed
list so starting at us and going to all
the children
underscore backward
and
that should be it so
save
come down here
redefine
[Music]
okay all the grands are zero
and now what we can do is oh that
backward without the underscore
and
there we go
and that's uh that's back propagation
place for one neuron
now we shouldn't be too happy with
ourselves actually because we have a bad
bug um and we have not surfaced the bug
because of some specific conditions that
we are we have to think about right now
so here's the simplest case that shows
the bug
say i create a single node a
and then i create a b that is a plus a
and then i called backward
so what's going to happen is a is 3
and then a b is a plus a so there's two
arrows on top of each other here
then we can see that b is of course the
forward pass works
b is just
a plus a which is six
but the gradient here is not actually
correct
that we calculate it automatically
and that's because
um
of course uh
just doing calculus in your head the
derivative of b with respect to a
should be uh two
one plus one
it's not one
intuitively what's happening here right
so b is the result of a plus a and then
we call backward on it
so let's go up and see what that does
um
b is a result of addition
so out as
b and then when we called backward what
happened is
self.grad was set
to one
and then other that grad was set to one
but because we're doing a plus a
self and other are actually the exact
same object
so we are overriding the gradient we are
setting it to one and then we are
setting it again to one and that's why
it stays
at one
so that's a problem
there's another way to see this in a
little bit more complicated expression
so here we have
a and b
and then uh d will be the multiplication
of the two and e will be the addition of
the two
and
then we multiply e times d to get f and
then we called fda backward
and these gradients if you check will be
incorrect
so fundamentally what's happening here
again is
basically we're going to see an issue
anytime we use a variable more than once
until now in these expressions above
every variable is used exactly once so
we didn't see the issue
but here if a variable is used more than
once what's going to happen during
backward pass we're backpropagating from
f to e to d so far so good but now
equals it backward and it deposits its
gradients to a and b but then we come
back to d
and call backward and it overwrites
those gradients at a and b
so that's obviously a problem
and the solution here if you look at
the multivariate case of the chain rule
and its generalization there
the solution there is basically that we
have to accumulate these gradients these
gradients add
and so instead of setting those
gradients
we can simply do plus equals we need to
accumulate those gradients
plus equals plus equals
plus equals
plus equals
and this will be okay remember because
we are initializing them at zero so they
start at zero
and then any
contribution
that flows backwards
will simply add
so now if we redefine
this one
because the plus equals this now works
because a.grad started at zero and we
called beta backward we deposit one and
then we deposit one again and now this
is two which is correct
and here this will also work and we'll
get correct gradients
because when we call eta backward we
will deposit the gradients from this
branch and then we get to back into
detail backward it will deposit its own
gradients and then those gradients
simply add on top of each other and so
we just accumulate those gradients and
that fixes the issue okay now before we
move on let me actually do a bit of
cleanup here and delete some of these
some of this intermediate work so
we're not gonna need any of this now
that we've derived all of it
um
we are going to keep this because i want
to come back to it
delete the 10h
delete our morning example
delete the step
delete this keep the code that draws
and then delete this example
and leave behind only the definition of
value
and now let's come back to this
non-linearity here that we implemented
the tanh now i told you that we could
have broken down 10h into its explicit
atoms in terms of other expressions if
we had the x function so if you remember
tan h is defined like this and we chose
to develop tan h as a single function
and we can do that because we know its
derivative and we can back propagate
through it
but we can also break down tan h into
and express it as a function of x and i
would like to do that now because i want
to prove to you that you get all the
same results and all those ingredients
but also because it forces us to
implement a few more expressions it
forces us to do exponentiation addition
subtraction division and things like
that and i think it's a good exercise to
go through a few more of these
okay so let's scroll up
to the definition of value
and here one thing that we currently
can't do is we can do like a value of
say 2.0
but we can't do you know here for
example we want to add constant one and
we can't do something like this
and we can't do it because it says
object has no attribute data that's
because a plus one comes right here to
add
and then other is the integer one and
then here python is trying to access
one.data and that's not a thing and
that's because basically one is not a
value object and we only have addition
for value objects so as a matter of
convenience so that we can create
expressions like this and make them make
sense
we can simply do something like this
basically
we let other alone if other is an
instance of value but if it's not an
instance of value we're going to assume
that it's a number like an integer float
and we're going to simply wrap it in in
value and then other will just become
value of other and then other will have
a data attribute and this should work so
if i just say this predefined value then
this should work
there we go okay now let's do the exact
same thing for multiply because we can't
do something like this
again
for the exact same reason so we just
have to go to mole and if other is
not a value then let's wrap it in value
let's redefine value and now this works
now here's a kind of unfortunate and not
obvious part a times two works we saw
that but two times a is that gonna work
you'd expect it to right but actually it
will not
and the reason it won't is because
python doesn't know
like when when you do a times two
basically um so a times two python will
go and it will basically do something
like a dot mul
of two that's basically what it will
call but to it 2 times a is the same as
2 dot mol of a
and it doesn't 2 can't multiply
value and so it's really confused about
that
so instead what happens is in python the
way this works is you are free to define
something called the r mold
and our mole
is kind of like a fallback so if python
can't do 2 times a it will check if um
if by any chance a knows how to multiply
two and that will be called into our
mole
so because python can't do two times a
it will check is there an our mole in
value and because there is it will now
call that
and what we'll do here is we will swap
the order of the operands so basically
two times a will redirect to armel and
our mole will basically call a times two
and that's how that will work
so
redefining now with armor two times a
becomes four okay now looking at the
other elements that we still need we
need to know how to exponentiate and how
to divide so let's first the explanation
to the exponentiation part we're going
to introduce
a single
function x here
and x is going to mirror 10h in the
sense that it's a simple single function
that transforms a single scalar value
and outputs a single scalar value
so we pop out the python number we use
math.x to exponentiate it create a new
value object
everything that we've seen before the
tricky part of course is how do you
propagate through e to the x
and
so here you can potentially pause the
video and think about what should go
here
okay so basically we need to know what
is the local derivative of e to the x so
d by d x of e to the x is famously just
e to the x and we've already just
calculated e to the x and it's inside
out that data so we can do up that data
times
and
out that grad that's the chain rule
so we're just chaining on to the current
running grad
and this is what the expression looks
like it looks a little confusing but
this is what it is and that's the
exponentiation
so redefining we should now be able to
call a.x
and
hopefully the backward pass works as
well okay and the last thing we'd like
to do of course is we'd like to be able
to divide
now
i actually will implement something
slightly more powerful than division
because division is just a special case
of something a bit more powerful
so in particular just by rearranging
if we have some kind of a b equals
value of 4.0 here we'd like to basically
be able to do a divide b and we'd like
this to be able to give us 0.5
now division actually can be reshuffled
as follows if we have a divide b that's
actually the same as a multiplying one
over b
and that's the same as a multiplying b
to the power of negative one
and so what i'd like to do instead is i
basically like to implement the
operation of x to the k for some
constant uh k so it's an integer or a
float um and we would like to be able to
differentiate this and then as a special
case uh negative one will be division
and so i'm doing that just because uh
it's more general and um yeah you might
as well do it that way so basically what
i'm saying is we can redefine
uh division
which we will put here somewhere
yeah we can put it here somewhere what
i'm saying is that we can redefine
division so self-divide other
can actually be rewritten as self times
other to the power of negative one
and now
a value raised to the power of negative
one we have now defined that
so
here's
so we need to implement the pow function
where am i going to put the power
function maybe here somewhere
this is the skeleton for it
so this function will be called when we
try to raise a value to some power and
other will be that power
now i'd like to make sure that other is
only an int or a float usually other is
some kind of a different value object
but here other will be forced to be an
end or a float otherwise the math
won't work for
for or try to achieve in the specific
case that would be a different
derivative expression if we wanted other
to be a value
so here we create the output value which
is just uh you know this data raised to
the power of other and other here could
be for example negative one that's what
we are hoping to achieve
and then uh this is the backwards stub
and this is the fun part which is what
is the uh chain rule expression here for
back for um
back propagating through the power
function where the power is to the power
of some kind of a constant
so this is the exercise and maybe pause
the video here and see if you can figure
it out yourself as to what we should put
here
okay so
you can actually go here and look at
derivative rules as an example and we
see lots of derivatives that you can
hopefully know from calculus in
particular what we're looking for is the
power rule
because that's telling us that if we're
trying to take d by dx of x to the n
which is what we're doing here
then that is just n times x to the n
minus 1
right
okay
so
that's telling us about the local
derivative of this power operation
so all we want here
basically n is now other
and self.data is x
and so this now becomes
other which is n times
self.data
which is now a python in torah float
it's not a valley object we're accessing
the data attribute
raised
to the power of other minus one or n
minus one
i can put brackets around this but this
doesn't matter because
power takes precedence over multiply and
python so that would have been okay
and that's the local derivative only but
now we have to chain it and we change
just simply by multiplying by output
grad that's chain rule
and this should technically work
and we're going to find out soon but now
if we
do this this should now work
and we get 0.5 so the forward pass works
but does the backward pass work and i
realize that we actually also have to
know how to subtract so
right now a minus b will not work
to make it work we need one more
piece of code here
and
basically this is the
subtraction and the way we're going to
implement subtraction is we're going to
implement it by addition of a negation
and then to implement negation we're
gonna multiply by negative one so just
again using the stuff we've already
built and just um expressing it in terms
of what we have and a minus b is now
working okay so now let's scroll again
to this expression here for this neuron
and let's just
compute the backward pass here once
we've defined o
and let's draw it
so here's the gradients for all these
leaf nodes for this two-dimensional
neuron that has a 10h that we've seen
before so now what i'd like to do is i'd
like to break up this 10h
into this expression here
so let me copy paste this
here
and now instead of we'll preserve the
label
and we will change how we define o
so in particular we're going to
implement this formula here
so we need e to the 2x
minus 1 over e to the x plus 1. so e to
the 2x we need to take 2 times n and we
need to exponentiate it that's e to the
two x and then because we're using it
twice let's create an intermediate
variable e
and then define o as
e plus one over
e minus one over e plus one
e minus one over e plus one
and that should be it and then we should
be able to draw that of o
so now before i run this what do we
expect to see
number one we're expecting to see a much
longer
graph here because we've broken up 10h
into a bunch of other operations
but those operations are mathematically
equivalent and so what we're expecting
to see is number one the same
result here so the forward pass works
and number two because of that
mathematical equivalence we expect to
see the same backward pass and the same
gradients on these leaf nodes so these
gradients should be identical
so let's run this
so number one let's verify that instead
of a single 10h node we have now x and
we have plus we have times negative one
uh this is the division
and we end up with the same forward pass
here
and then the gradients we have to be
careful because they're in slightly
different order potentially the
gradients for w2x2 should be 0 and 0.5
w2 and x2 are 0 and 0.5
and w1 x1 are 1 and negative 1.5
1 and negative 1.5
so that means that both our forward
passes and backward passes were correct
because this turned out to be equivalent
to
10h before
and so the reason i wanted to go through
this exercise is number one we got to
practice a few more operations and uh
writing more backwards passes and number
two i wanted to illustrate the point
that
the um
the level at which you implement your
operations is totally up to you you can
implement backward passes for tiny
expressions like a single individual
plus or a single times
or you can implement them for say
10h
which is a kind of a potentially you can
see it as a composite operation because
it's made up of all these more atomic
operations but really all of this is
kind of like a fake concept all that
matters is we have some kind of inputs
and some kind of an output and this
output is a function of the inputs in
some way and as long as you can do
forward pass and the backward pass of
that little operation it doesn't matter
what that operation is
and how composite it is
if you can write the local gradients you
can chain the gradient and you can
continue back propagation so the design
of what those functions are is
completely up to you
so now i would like to show you how you
can do the exact same thing by using a
modern deep neural network library like
for example pytorch which i've roughly
modeled micrograd
by
and so
pytorch is something you would use in
production and i'll show you how you can
do the exact same thing but in pytorch
api so i'm just going to copy paste it
in and walk you through it a little bit
this is what it looks like
so we're going to import pi torch and
then we need to define these
value objects like we have here
now micrograd is a scalar valued
engine so we only have scalar values
like 2.0 but in pi torch everything is
based around tensors and like i
mentioned tensors are just n-dimensional
arrays of scalars
so that's why things get a little bit
more complicated here i just need a
scalar value to tensor a tensor with
just a single element
but by default when you work with
pytorch you would use um
more complicated tensors like this so if
i import pytorch
then i can create tensors like this and
this tensor for example is a two by
three array
of scalar
scalars
in a single compact representation so we
can check its shape we see that it's a
two by three array
and so on
so this is usually what you would work
with um in the actual libraries so here
i'm creating
a tensor that has only a single element
2.0
and then i'm casting it to be double
because python is by default using
double precision for its floating point
numbers so i'd like everything to be
identical by default the data type of
these tensors will be float32 so it's
only using a single precision float so
i'm casting it to double
so that we have float64 just like in
python
so i'm casting to double and then we get
something similar to value of two the
next thing i have to do is because these
are leaf nodes by default pytorch
assumes that they do not require
gradients so i need to explicitly say
that all of these nodes require
gradients
okay so this is going to construct
scalar valued one element tensors
make sure that fighters knows that they
require gradients now by default these
are set to false by the way because of
efficiency reasons because usually you
would not want gradients for leaf nodes
like the inputs to the network and this
is just trying to be efficient in the
most common cases
so once we've defined all of our values
in python we can perform arithmetic just
like we can here in microgradlend so
this will just work and then there's a
torch.10h also
and when we get back is a tensor again
and we can
just like in micrograd it's got a data
attribute and it's got grant attributes
so these tensor objects just like in
micrograd have a dot data and a dot grad
and
the only difference here is that we need
to call it that item because otherwise
um pi torch
that item basically takes
a single tensor of one element and it
just returns that element stripping out
the tensor
so let me just run this and hopefully we
are going to get this is going to print
the forward pass
which is 0.707
and this will be the gradients which
hopefully are
0.5 0 negative 1.5 and 1.
so if we just run this
there we go
0.7 so the forward pass agrees and then
point five zero negative one point five
and one
so pi torch agrees with us
and just to show you here basically o
here's a tensor with a single element
and it's a double
and we can call that item on it to just
get the single number out
so that's what item does and o is a
tensor object like i mentioned and it's
got a backward function just like we've
implemented
and then all of these also have a dot
graph so like x2 for example in the grad
and it's a tensor and we can pop out the
individual number with that actin
so basically
torches torch can do what we did in
micrograph is a special case when your
tensors are all single element tensors
but the big deal with pytorch is that
everything is significantly more
efficient because we are working with
these tensor objects and we can do lots
of operations in parallel on all of
these tensors
but otherwise what we've built very much
agrees with the api of pytorch
okay so now that we have some machinery
to build out pretty complicated
mathematical expressions we can also
start building out neural nets and as i
mentioned neural nets are just a
specific class of mathematical
expressions
so we're going to start building out a
neural net piece by piece and eventually
we'll build out a two-layer multi-layer
layer perceptron as it's called and i'll
show you exactly what that means
let's start with a single individual
neuron we've implemented one here but
here i'm going to implement one that
also subscribes to the pytorch api in
how it designs its neural network
modules
so just like we saw that we can like
match the api of pytorch
on the auto grad side we're going to try
to do that on the neural network modules
so here's class neuron
and just for the sake of efficiency i'm
going to copy paste some sections that
are relatively straightforward
so the constructor will take
number of inputs to this neuron which is
how many inputs come to a neuron so this
one for example has three inputs
and then it's going to create a weight
there is some random number between
negative one and one for every one of
those inputs
and a bias that controls the overall
trigger happiness of this neuron
and then we're going to implement a def
underscore underscore call
of self and x some input x
and really what we don't do here is w
times x plus b
where w times x here is a dot product
specifically
now if you haven't seen
call
let me just return 0.0 here for now the
way this works now is we can have an x
which is say like 2.0 3.0 then we can
initialize a neuron that is
two-dimensional
because these are two numbers and then
we can feed those two numbers into that
neuron to get an output
and so when you use this notation n of x
python will use call
so currently call just return 0.0
now we'd like to actually do the forward
pass of this neuron instead
so we're going to do here first is we
need to basically multiply all of the
elements of w with all of the elements
of x pairwise we need to multiply them
so the first thing we're going to do is
we're going to zip up
celta w and x
and in python zip takes two iterators
and it creates a new iterator that
iterates over the tuples of the
corresponding entries
so for example just to show you we can
print this list
and still return 0.0 here
sorry
so we see that these w's are paired up
with the x's w with x
and now what we want to do is
for w i x i in
we want to multiply w times
w wi times x i
and then we want to sum all of that
together
to come up with an activation
and add also subnet b on top
so that's the raw activation and then of
course we need to pass that through a
non-linearity so what we're going to be
returning is act.10h
and here's out
so
now we see that we are getting some
outputs and we get a different output
from a neuron each time because we are
initializing different weights and by
and biases
and then to be a bit more efficient here
actually sum by the way takes a second
optional parameter which is the start
and by default the start is zero so
these elements of this sum will be added
on top of zero to begin with but
actually we can just start with cell dot
b
and then we just have an expression like
this
and then the generator expression here
must be parenthesized in python
there we go
yep so now we can forward a single
neuron next up we're going to define a
layer of neurons so here we have a
schematic for a mlb
so we see that these mlps each layer
this is one layer has actually a number
of neurons and they're not connected to
each other but all of them are fully
connected to the input
so what is a layer of neurons it's just
it's just a set of neurons evaluated
independently
so
in the interest of time i'm going to do
something fairly straightforward here
it's um
literally a layer is just a list of
neurons
and then how many neurons do we have we
take that as an input argument here how
many neurons do you want in your layer
number of outputs in this layer
and so we just initialize completely
independent neurons with this given
dimensionality and when we call on it we
just independently
evaluate them so now instead of a neuron
we can make a layer of neurons they are
two-dimensional neurons and let's have
three of them
and now we see that we have three
independent evaluations of three
different neurons
right
okay finally let's complete this picture
and define an entire multi-layer
perceptron or mlp
and as we can see here in an mlp these
layers just feed into each other
sequentially
so let's come here and i'm just going to
copy the code here in interest of time
so an mlp is very similar
we're taking the number of inputs
as before but now instead of taking a
single n out which is number of neurons
in a single layer we're going to take a
list of an outs and this list defines
the sizes of all the layers that we want
in our mlp
so here we just put them all together
and then iterate over consecutive pairs
of these sizes and create layer objects
for them
and then in the call function we are
just calling them sequentially so that's
an mlp really
and let's actually re-implement this
picture so we want three input neurons
and then two layers of four and an
output unit
so
we want
a three-dimensional input say this is an
example input we want three inputs into
two layers of four and one output
and this of course is an mlp
and there we go that's a forward pass of
an mlp
to make this a little bit nicer you see
how we have just a single element but
it's wrapped in a list because layer
always returns lists
circle for convenience
return outs at zero if len out is
exactly a single element
else return fullest
and this will allow us to just get a
single value out at the last layer that
only has a single neuron
and finally we should be able to draw
dot of n of x
and
as you might imagine
these expressions are now getting
relatively involved
so this is an entire mlp that we're
defining now
all the way until a single output
okay
and so obviously you would never
differentiate on pen and paper these
expressions but with micrograd we will
be able to back propagate all the way
through this
and back propagate
into
these weights of all these neurons so
let's see how that works okay so let's
create ourselves a very simple
example data set here
so this data set has four examples
and so we have four possible
inputs into the neural net
and we have four desired targets so we'd
like the neural net to assign
or output 1.0 when it's fed this example
negative one when it's fed these
examples and one when it's fed this
example so it's a very simple binary
classifier neural net basically that we
would like here
now let's think what the neural net
currently thinks about these four
examples we can just get their
predictions
um basically we can just call n of x for
x in axis
and then we can
print
so these are the outputs of the neural
net on those four examples
so
the first one is 0.91 but we'd like it
to be one so we should push this one
higher this one we want to be higher
this one says 0.88 and we want this to
be negative one
this is 0.8 we want it to be negative
one
and this one is 0.8 we want it to be one
so how do we make the neural net and how
do we tune the weights
to
better predict the desired targets
and the trick used in deep learning to
achieve this is to
calculate a single number that somehow
measures the total performance of your
neural net and we call this single
number the loss
so the loss
first
is is a single number that we're going
to define that basically measures how
well the neural net is performing right
now we have the intuitive sense that
it's not performing very well because
we're not very much close to this
so the loss will be high and we'll want
to minimize the loss
so in particular in this case what we're
going to do is we're going to implement
the mean squared error loss
so this is doing is we're going to
basically iterate um
for y ground truth
and y output in zip of um
wise and white red so we're going to
pair up the
ground truths with the predictions
and this zip iterates over tuples of
them
and for each
y ground truth and y output we're going
to subtract them
and square them
so let's first see what these losses are
these are individual loss components
and so basically for each
one of the four
we are taking the prediction and the
ground truth we are subtracting them and
squaring them
so because
this one is so close to its target 0.91
is almost one
subtracting them gives a very small
number
so here we would get like a negative
point one and then squaring it
just makes sure
that regardless of whether we are more
negative or more positive we always get
a positive
number instead of squaring we should we
could also take for example the absolute
value we need to discard the sign
and so you see that the expression is
ranged so that you only get zero exactly
when y out is equal to y ground truth
when those two are equal so your
prediction is exactly the target you are
going to get zero
and if your prediction is not the target
you are going to get some other number
so here for example we are way off and
so that's why the loss is quite high
and the more off we are the greater the
loss will be
so we don't want high loss we want low
loss
and so the final loss here will be just
the sum
of all of these
numbers
so you see that this should be zero
roughly plus zero roughly
but plus
seven
so loss should be about seven
here
and now we want to minimize the loss we
want the loss to be low
because if loss is low
then every one of the predictions is
equal to its target
so the loss the lowest it can be is zero
and the greater it is the worse off the
neural net is predicting
so now of course if we do lost that
backward
something magical happened when i hit
enter
and the magical thing of course that
happened is that we can look at
end.layers.neuron and that layers at say
like the the first layer
that neurons at zero
because remember that mlp has the layers
which is a list
and each layer has a neurons which is a
list and that gives us an individual
neuron
and then it's got some weights
and so we can for example look at the
weights at zero
um
oops it's not called weights it's called
w
and that's a value but now this value
also has a groud because of the backward
pass
and so we see that because this gradient
here on this particular weight of this
particular neuron of this particular
layer is negative
we see that its influence on the loss is
also negative so slightly increasing
this particular weight of this neuron of
this layer would make the loss go down
and we actually have this information
for every single one of our neurons and
all their parameters actually it's worth
looking at also the draw dot loss by the
way
so previously we looked at the draw dot
of a single neural neuron forward pass
and that was already a large expression
but what is this expression we actually
forwarded
every one of those four examples and
then we have the loss on top of them
with the mean squared error
and so this is a really massive graph
because this graph that we've built up
now
oh my gosh this graph that we've built
up now
which is kind of excessive it's
excessive because it has four forward
passes of a neural net for every one of
the examples and then it has the loss on
top
and it ends with the value of the loss
which was 7.12
and this loss will now back propagate
through all the four forward passes all
the way through just every single
intermediate value of the neural net
all the way back to of course the
parameters of the weights which are the
input
so these weight parameters here are
inputs to this neural net
and
these numbers here these scalars are
inputs to the neural net
so if we went around here
we'll probably find
some of these examples this 1.0
potentially maybe this 1.0 or you know
some of the others and you'll see that
they all have gradients as well
the thing is these gradients on the
input data are not that useful to us
and that's because the input data seems
to be not changeable it's it's a given
to the problem and so it's a fixed input
we're not going to be changing it or
messing with it even though we do have
gradients for it
but some of these gradients here
will be for the neural network
parameters the ws and the bs and those
we of course we want to change
okay so now we're going to want some
convenience code to gather up all of the
parameters of the neural net so that we
can operate on all of them
simultaneously and every one of them we
will nudge a tiny amount
based on the gradient information
so let's collect the parameters of the
neural net all in one array
so let's create a parameters of self
that just
returns celta w which is a list
concatenated with
a list of self.b
so this will just return a list
list plus list just you know gives you a
list
so that's parameters of neuron and i'm
calling it this way because also pi
torch has a parameters on every single
and in module
and uh it does exactly what we're doing
here it just returns the
parameter tensors for us as the
parameter scalars
now layer is also a module so it will
have parameters
itself
and basically what we want to do here is
something like this like
params is here and then for
neuron in salt out neurons
we want to get neuron.parameters
and we want to params.extend
right so these are the parameters of
this neuron and then we want to put them
on top of params so params dot extend
of peace
and then we want to return brands
so this is way too much code so actually
there's a way to simplify this which is
return
p
for neuron in self
neurons
for
p in neuron dot parameters
so it's a single list comprehension in
python you can sort of nest them like
this and you can um
then create
uh the desired
array so this is these are identical
we can take this out
and then let's do the same here
def parameters
self
and return
a parameter for layer in self dot layers
for
p in layer dot parameters
and that should be good
now let me pop out this so
we don't re-initialize our network
because we need to re-initialize
our
okay so unfortunately we will have to
probably re-initialize the network
because we just add functionality
because this class of course we i want
to get all the and that parameters but
that's not going to work because this is
the old class
okay
so unfortunately we do have to
reinitialize the network which will
change some of the numbers
but let me do that so that we pick up
the new api we can now do in the
parameters
and these are all the weights and biases
inside the entire neural net
so in total this mlp has 41 parameters
and
now we'll be able to change them
if we recalculate the loss here we see
that unfortunately we have slightly
different
predictions and slightly different laws
but that's okay
okay so we see that this neurons
gradient is slightly negative we can
also look at its data right now
which is 0.85 so this is the current
value of this neuron and this is its
gradient on the loss
so what we want to do now is we want to
iterate for every p in
n dot parameters so for all the 41
parameters in this neural net
we actually want to change p data
slightly
according to the gradient information
okay so
dot dot to do here
but this will be basically a tiny update
in this gradient descent scheme in
gradient descent we are thinking of the
gradient as a vector pointing in the
direction
of
increased
loss
and so
in gradient descent we are modifying
p data
by a small step size in the direction of
the gradient so the step size as an
example could be like a very small
number like 0.01 is the step size times
p dot grad
right
but we have to think through some of the
signs here
so uh
in particular working with this specific
example here
we see that if we just left it like this
then this neuron's value
would be currently increased by a tiny
amount of the gradient
the grain is negative so this value of
this neuron would go slightly down it
would become like 0.8 you know four or
something like that
but if this neuron's value goes lower
that would actually
increase the loss
that's because
the derivative of this neuron is
negative so increasing
this makes the loss go down so
increasing it is what we want to do
instead of decreasing it so basically
what we're missing here is we're
actually missing a negative sign
and again this other interpretation
and that's because we want to minimize
the loss we don't want to maximize the
loss we want to decrease it
and the other interpretation as i
mentioned is you can think of the
gradient vector
so basically just the vector of all the
gradients
as pointing in the direction of
increasing
the loss but then we want to decrease it
so we actually want to go in the
opposite direction
and so you can convince yourself that
this sort of plug does the right thing
here with the negative because we want
to minimize the loss
so if we nudge all the parameters by
tiny amount
then we'll see that
this data will have changed a little bit
so now this neuron
is a tiny amount greater
value so 0.854 went to 0.857
and that's a good thing because slightly
increasing this neuron
uh
data makes the loss go down according to
the gradient and so the correct thing
has happened sign wise
and so now what we would expect of
course is that
because we've changed all these
parameters we expect that the loss
should have gone down a bit
so we want to re-evaluate the loss let
me basically
this is just a data definition that
hasn't changed but the forward pass here
of the network we can recalculate
and actually let me do it outside here
so that we can compare the two loss
values
so here if i recalculate the loss
we'd expect the new loss now to be
slightly lower than this number so
hopefully what we're getting now is a
tiny bit lower than 4.84
4.36
okay and remember the way we've arranged
this is that low loss means that our
predictions are matching the targets so
our predictions now are probably
slightly closer to the
targets and now all we have to do is we
have to iterate this process
so again um we've done the forward pass
and this is the loss
now we can lost that backward
let me take these out and we can do a
step size
and now we should have a slightly lower
loss 4.36 goes to 3.9
and okay so
we've done the forward pass here's the
backward pass
nudge
and now the loss is 3.66
3.47
and you get the idea we just continue
doing this and this is uh gradient
descent we're just iteratively doing
forward pass backward pass update
forward pass backward pass update and
the neural net is improving its
predictions
so here if we look at why pred now
like red
we see that um
this value should be getting closer to
one
so this value should be getting more
positive these should be getting more
negative and this one should be also
getting more positive so if we just
iterate this
a few more times
actually we may be able to afford go to
go a bit faster let's try a slightly
higher learning rate
oops okay there we go so now we're at
0.31
if you go too fast by the way if you try
to make it too big of a step you may
actually overstep
it's overconfidence because again
remember we don't actually know exactly
about the loss function the loss
function has all kinds of structure and
we only know about the very local
dependence of all these parameters on
the loss but if we step too far
we may step into you know a part of the
loss that is completely different
and that can destabilize training and
make your loss actually blow up even
so the loss is now 0.04 so actually the
predictions should be really quite close
let's take a look
so you see how this is almost one
almost negative one almost one we can
continue going
uh so
yep backward
update
oops there we go so we went way too fast
and um
we actually overstepped
so we got two uh too eager where are we
now oops
okay
seven e negative nine so this is very
very low loss
and the predictions
are basically perfect
so somehow we
basically we were doing way too big
updates and we briefly exploded but then
somehow we ended up getting into a
really good spot so usually this
learning rate and the tuning of it is a
subtle art you want to set your learning
rate if it's too low you're going to
take way too long to converge but if
it's too high the whole thing gets
unstable and you might actually even
explode the loss
depending on your loss function
so finding the step size to be just
right it's it's a pretty subtle art
sometimes when you're using sort of
vanilla gradient descent
but we happen to get into a good spot we
can look at
n-dot parameters
so this is the setting of weights and
biases
that makes our network
predict
the desired targets
very very close
and
basically we've successfully trained
neural net
okay let's make this a tiny bit more
respectable and implement an actual
training loop and what that looks like
so this is the data definition that
stays this is the forward pass
um so
for uh k in range you know we're going
to
take a bunch of steps
first you do the forward pass
we validate the loss
let's re-initialize the neural net from
scratch
and here's the data
and we first do before pass then we do
the backward pass
and then we do an update that's gradient
descent
and then we should be able to iterate
this and we should be able to print the
current step
the current loss um let's just print the
sort of
number of the loss
and
that should be it
and then the learning rate 0.01 is a
little too small 0.1 we saw is like a
little bit dangerously too high let's go
somewhere in between
and we'll optimize this for
not 10 steps but let's go for say 20
steps
let me erase all of this junk
and uh let's run the optimization
and you see how we've actually converged
slower in a more controlled manner and
got to a loss that is very low
so
i expect white bread to be quite good
there we go
um
and
that's it
okay so this is kind of embarrassing but
we actually have a really terrible bug
in here and it's a subtle bug and it's a
very common bug and i can't believe i've
done it for the 20th time in my life
especially on camera and i could have
reshot the whole thing but i think it's
pretty funny and you know you get to
appreciate a bit what um working with
neural nets maybe
is like sometimes
we are guilty of
come bug i've actually tweeted
the most common neural net mistakes a
long time ago now
uh and
i'm not really
gonna explain any of these except for we
are guilty of number three you forgot to
zero grad
before that backward what is that
basically what's happening and it's a
subtle bug and i'm not sure if you saw
it
is that
all of these
weights here have a dot data and a dot
grad
and that grad starts at zero
and then we do backward and we fill in
the gradients
and then we do an update on the data but
we don't flush the grad
it stays there
so when we do the second
forward pass and we do backward again
remember that all the backward
operations do a plus equals on the grad
and so these gradients just
add up and they never get reset to zero
so basically we didn't zero grad so
here's how we zero grad before
backward
we need to iterate over all the
parameters
and we need to make sure that p dot grad
is set to zero
we need to reset it to zero just like it
is in the constructor
so remember all the way here for all
these value nodes grad is reset to zero
and then all these backward passes do a
plus equals from that grad
but we need to make sure that
we reset these graphs to zero so that
when we do backward
all of them start at zero and the actual
backward pass accumulates um
the loss derivatives into the grads
so this is zero grad in pytorch
and uh
we will slightly get we'll get a
slightly different optimization let's
reset the neural net
the data is the same this is now i think
correct
and we get a much more
you know we get a much more
slower descent
we still end up with pretty good results
and we can continue this a bit more
to get down lower
and lower
and lower
yeah
so the only reason that the previous
thing worked it's extremely buggy um the
only reason that worked is that
this is a very very simple problem
and it's very easy for this neural net
to fit this data
and so the grads ended up accumulating
and it effectively gave us a massive
step size and it made us converge
extremely fast
but basically now we have to do more
steps to get to very low values of loss
and get wipe red to be really good we
can try to
step a bit greater
yeah we're gonna get closer and closer
to one minus one and one
so
working with neural nets is sometimes
tricky because
uh
you may have lots of bugs in the code
and uh your network might actually work
just like ours worked
but chances are is that if we had a more
complex problem then actually this bug
would have made us not optimize the loss
very well and we were only able to get
away with it because
the problem is very simple
so let's now bring everything together
and summarize what we learned
what are neural nets neural nets are
these mathematical expressions
fairly simple mathematical expressions
in the case of multi-layer perceptron
that take
input as the data and they take input
the weights and the parameters of the
neural net mathematical expression for
the forward pass followed by a loss
function and the loss function tries to
measure the accuracy of the predictions
and usually the loss will be low when
your predictions are matching your
targets or where the network is
basically behaving well so we we
manipulate the loss function so that
when the loss is low the network is
doing what you want it to do on your
problem
and then we backward the loss
use backpropagation to get the gradient
and then we know how to tune all the
parameters to decrease the loss locally
but then we have to iterate that process
many times in what's called the gradient
descent
so we simply follow the gradient
information and that minimizes the loss
and the loss is arranged so that when
the loss is minimized the network is
doing what you want it to do
and yeah so we just have a blob of
neural stuff and we can make it do
arbitrary things and that's what gives
neural nets their power um
it's you know this is a very tiny
network with 41 parameters
but you can build significantly more
complicated neural nets with billions
at this point almost trillions of
parameters and it's a massive blob of
neural tissue simulated neural tissue
roughly speaking
and you can make it do extremely complex
problems and these neurons then have all
kinds of very fascinating emergent
properties
in
when you try to make them do
significantly hard problems as in the
case of gpt for example
we have massive amounts of text from the
internet and we're trying to get a
neural net to predict to take like a few
words and try to predict the next word
in a sequence that's the learning
problem
and it turns out that when you train
this on all of internet the neural net
actually has like really remarkable
emergent properties but that neural net
would have hundreds of billions of
parameters
but it works on fundamentally the exact
same principles
the neural net of course will be a bit
more complex but otherwise the
value in the gradient is there
and would be identical and the gradient
descent would be there and would be
basically identical but people usually
use slightly different updates this is a
very simple stochastic gradient descent
update
um
and the loss function would not be mean
squared error they would be using
something called the cross-entropy loss
for predicting the next token so there's
a few more details but fundamentally the
neural network setup and neural network
training is identical and pervasive and
now you understand intuitively
how that works under the hood in the
beginning of this video i told you that
by the end of it you would understand
everything in micrograd and then we'd
slowly build it up let me briefly prove
that to you
so i'm going to step through all the
code that is in micrograd as of today
actually potentially some of the code
will change by the time you watch this
video because i intend to continue
developing micrograd
but let's look at what we have so far at
least init.pi is empty when you go to
engine.pi that has the value
everything here you should mostly
recognize so we have the data.grad
attributes we have the backward function
uh we have the previous set of children
and the operation that produced this
value
we have addition multiplication and
raising to a scalar power
we have the relu non-linearity which is
slightly different type of nonlinearity
than 10h that we used in this video
both of them are non-linearities and
notably 10h is not actually present in
micrograd as of right now but i intend
to add it later
with the backward which is identical and
then all of these other operations which
are built up on top of operations here
so values should be very recognizable
except for the non-linearity used in
this video
um there's no massive difference between
relu and 10h and sigmoid and these other
non-linearities they're all roughly
equivalent and can be used in mlps so i
use 10h because it's a bit smoother and
because it's a little bit more
complicated than relu and therefore it's
stressed a little bit more the
local gradients and working with those
derivatives which i thought would be
useful
and then that pi is the neural networks
library as i mentioned so you should
recognize identical implementation of
neuron layer and mlp
notably or not so much
we have a class module here there is a
parent class of all these modules i did
that because there's an nn.module class
in pytorch and so this exactly matches
that api and end.module and pytorch has
also a zero grad which i've refactored
out here
so that's the end of micrograd really
then there's a test
which you'll see
basically creates
two chunks of code one in micrograd and
one in pi torch and we'll make sure that
the forward and the backward pass agree
identically
for a slightly less complicated
expression a slightly more complicated
expression everything
agrees so we agree with pytorch on all
of these operations
and finally there's a demo.ipymb here
and it's a bit more complicated binary
classification demo than the one i
covered in this lecture so we only had a
tiny data set of four examples um here
we have a bit more complicated example
with lots of blue points and lots of red
points and we're trying to again build a
binary classifier to distinguish uh two
dimensional points as red or blue
it's a bit more complicated mlp here
with it's a bigger mlp
the loss is a bit more complicated
because
it supports batches
so because our dataset was so tiny we
always did a forward pass on the entire
data set of four examples but when your
data set is like a million examples what
we usually do in practice is we chair we
basically pick out some random subset we
call that a batch and then we only
process the batch forward backward and
update so we don't have to forward the
entire training set
so this supports batching because
there's a lot more examples here
we do a forward pass the loss is
slightly more different this is a max
margin loss that i implement here
the one that we used was the mean
squared error loss because it's the
simplest one
there's also the binary cross entropy
loss all of them can be used for binary
classification and don't make too much
of a difference in the simple examples
that we looked at so far
there's something called l2
regularization used here this has to do
with generalization of the neural net
and controls the overfitting in machine
learning setting but i did not cover
these concepts and concepts in this
video potentially later
and the training loop you should
recognize so forward backward with zero
grad
and update and so on you'll notice that
in the update here the learning rate is
scaled as a function of number of
iterations and it
shrinks
and this is something called learning
rate decay so in the beginning you have
a high learning rate and as the network
sort of stabilizes near the end you
bring down the learning rate to get some
of the fine details in the end
and in the end we see the decision
surface of the neural net and we see
that it learns to separate out the red
and the blue area based on the data
points
so that's the slightly more complicated
example and then we'll demo that hyper
ymb that you're free to go over
but yeah as of today that is micrograd i
also wanted to show you a little bit of
real stuff so that you get to see how
this is actually implemented in
production grade library like by torch
uh so in particular i wanted to show i
wanted to find and show you the backward
pass for 10h in pytorch so here in
micrograd we see that the backward
password 10h is one minus t square
where t is the output of the tanh of x
times of that grad which is the chain
rule so we're looking for something that
looks like this
now
i went to pytorch um which has an open
source github codebase and uh i looked
through a lot of its code
and honestly i i i spent about 15
minutes and i couldn't find 10h
and that's because these libraries
unfortunately they grow in size and
entropy and if you just search for 10h
you get apparently 2 800 results and 400
and 406 files so i don't know what these
files are doing honestly
and why there are so many mentions of
10h but unfortunately these libraries
are quite complex they're meant to be
used not really inspected um
eventually i did stumble on someone
who tries to change the 10 h backward
code for some reason
and someone here pointed to the cpu
kernel and the kuda kernel for 10 inch
backward
so this so basically depends on if
you're using pi torch on a cpu device or
on a gpu which these are different
devices and i haven't covered this but
this is the 10 h backwards kernel
for uh cpu
and the reason it's so large is that
number one this is like if you're using
a complex type which we haven't even
talked about if you're using a specific
data type of b-float 16 which we haven't
talked about
and then if you're not then this is the
kernel and deep here we see something
that resembles our backward pass so they
have a times one minus
b square uh so this b
b here must be the output of the 10h and
this is the health.grad so here we found
it
uh deep inside
pi torch from this location for some
reason inside binaryops kernel when 10h
is not actually a binary op
and then this is the gpu kernel
we're not complex
we're
here and here we go with one line of
code
so we did find it but basically
unfortunately these codepieces are very
large and
micrograd is very very simple but if you
actually want to use real stuff uh
finding the code for it you'll actually
find that difficult
i also wanted to show you a little
example here where pytorch is showing
you how can you can register a new type
of function that you want to add to
pytorch as a lego building block
so here if you want to for example add a
gender polynomial 3
here's how you could do it you will
register it as a class that
subclasses storage.org that function
and then you have to tell pytorch how to
forward your new function
and how to backward through it
so as long as you can do the forward
pass of this little function piece that
you want to add and as long as you know
the the local derivative the local
gradients which are implemented in the
backward pi torch will be able to back
propagate through your function and then
you can use this as a lego block in a
larger lego castle of all the different
lego blocks that pytorch already has
and so that's the only thing you have to
tell pytorch and everything would just
work and you can register new types of
functions
in this way following this example
and that is everything that i wanted to
cover in this lecture
so i hope you enjoyed building out
micrograd with me i hope you find it
interesting insightful
and
yeah i will post a lot of the links
that are related to this video in the
video description below i will also
probably post a link to a discussion
forum
or discussion group where you can ask
questions related to this video and then
i can answer or someone else can answer
your questions and i may also do a
follow-up video that answers some of the
most common questions
but for now that's it i hope you enjoyed
it if you did then please like and
subscribe so that youtube knows to
feature this video to more people
and that's it for now i'll see you later
now here's the problem
we know
dl by
wait what is the problem
and that's everything i wanted to cover
in this lecture
so i hope
you enjoyed us building up microcraft
micro crab
okay now let's do the exact same thing
for multiply because we can't do
something like a times two
oops
i know what happened therehi everyone hope you're well
and next up what i'd like to do is i'd
like to build out make more
like micrograd before it make more is a
repository that i have on my github
webpage
you can look at it
but just like with micrograd i'm going
to build it out step by step and i'm
going to spell everything out so we're
going to build it out slowly and
together
now what is make more
make more as the name suggests
makes more of things that you give it
so here's an example
names.txt is an example dataset to make
more
and when you look at names.txt you'll
find that it's a very large data set of
names
so
here's lots of different types of names
in fact i believe there are 32 000 names
that i've sort of found randomly on the
government website
and if you train make more on this data
set it will learn to make more of things
like this
and in particular in this case that will
mean more things that sound name-like
but are actually unique names
and maybe if you have a baby and you're
trying to assign name maybe you're
looking for a cool new sounding unique
name make more might help you
so here are some example generations
from the neural network
once we train it on our data set
so here's some example
unique names that it will generate
dontel
irot
zhendi
and so on and so all these are sound
name like but they're not of course
names
so under the hood make more is a
character level language model so what
that means is that it is treating every
single line here as an example and
within each example it's treating them
all as sequences of individual
characters so r e e s e is this example
and that's the sequence of characters
and that's the level on which we are
building out make more and what it means
to be a character level language model
then is that it's just uh sort of
modeling those sequences of characters
and it knows how to predict the next
character in the sequence
now we're actually going to implement a
large number of character level language
models in terms of the neural networks
that are involved in predicting the next
character in a sequence so very simple
bi-gram and back of work models
multilingual perceptrons recurrent
neural networks all the way to modern
transformers in fact the transformer
that we will build will be basically the
equivalent transformer to gpt2 if you
have heard of gpt uh so that's kind of a
big deal it's a modern network and by
the end of the series you will actually
understand how that works um on the
level of characters now to give you a
sense of the extensions here uh after
characters we will probably spend some
time on the word level so that we can
generate documents of words not just
little you know segments of characters
but we can generate entire large much
larger documents
and then we're probably going to go into
images and image text
networks such as dolly stable diffusion
and so on but for now we have to start
here character level language modeling
let's go
so like before we are starting with a
completely blank jupiter notebook page
the first thing is i would like to
basically load up the dataset names.txt
so we're going to open up names.txt for
reading
and we're going to read in everything
into a massive string
and then because it's a massive string
we'd only like the individual words and
put them in the list
so let's call split lines
on that string
to get all of our words as a python list
of strings
so basically we can look at for example
the first 10 words
and we have that it's a list of emma
olivia eva and so on
and if we look at
the top of the page here that is indeed
what we see
um
so that's good
this list actually makes me feel that
this is probably sorted by frequency
but okay so
these are the words now we'd like to
actually like learn a little bit more
about this data set let's look at the
total number of words we expect this to
be roughly 32 000
and then what is the for example
shortest word
so min of
length of each word for w inwards
so the shortest word will be length
two
and max of one w for w in words so the
longest word will be
15 characters
so let's now think through our very
first language model
as i mentioned a character level
language model is predicting the next
character in a sequence given already
some concrete sequence of characters
before it
now we have to realize here is that
every single word here like isabella is
actually quite a few examples packed in
to that single word
because what is an existence of a word
like isabella in the data set telling us
really it's saying that
the character i is a very likely
character to come first in the sequence
of a name
the character s is likely to come
after i
the character a is likely to come after
is
the character b is very likely to come
after isa and so on all the way to a
following isabel
and then there's one more example
actually packed in here
and that is that
after there's isabella
the word is very likely to end
so that's one more sort of explicit
piece of information that we have here
that we have to be careful with
and so there's a lot backed into a
single individual word in terms of the
statistical structure of what's likely
to follow in these character sequences
and then of course we don't have just an
individual word we actually have 32 000
of these and so there's a lot of
structure here to model
now in the beginning what i'd like to
start with is i'd like to start with
building a bi-gram language model
now in the bigram language model we're
always working with just
two characters at a time
so we're only looking at one character
that we are given and we're trying to
predict the next character in the
sequence
so um what characters are likely to
follow are what characters are likely to
follow a and so on and we're just
modeling that kind of a little local
structure
and we're forgetting the fact that we
may have a lot more information we're
always just looking at the previous
character to predict the next one so
it's a very simple and weak language
model but i think it's a great place to
start
so now let's begin by looking at these
bi-grams in our data set and what they
look like and these bi-grams again are
just two characters in a row
so for w in words
each w here is an individual word a
string
we want to iterate uh for
we're going to iterate this word
with consecutive characters so two
characters at a time sliding it through
the word now a interesting nice way cute
way to do this in python by the way is
doing something like this for character
one character two in zip off
w and w at one
one column
print
character one character two
and let's not do all the words let's
just do the first three words and i'm
going to show you in a second how this
works
but for now basically as an example
let's just do the very first word alone
emma
you see how we have a emma and this will
just print e m m m a
and the reason this works is because w
is the string emma w at one column is
the string mma
and zip
takes two iterators and it pairs them up
and then creates an iterator over the
tuples of their consecutive entries
and if any one of these lists is shorter
than the other then it will just
halt and return
so basically that's why we return em mmm
ma
but then because this iterator second
one here runs out of elements zip just
ends and that's why we only get these
tuples so pretty cute
so these are the consecutive elements in
the first word now we have to be careful
because we actually have more
information here than just these three
examples as i mentioned we know that e
is the is very likely to come first and
we know that a in this case is coming
last
so one way to do this is basically we're
going to create
a special array here all
characters
and um we're going to hallucinate a
special start token here
i'm going to
call it like special start
so this is a list of one element
plus
w
and then plus a special end character
and the reason i'm wrapping the list of
w here is because w is a string emma
list of w will just have the individual
characters in the list
and then
doing this again now but not iterating
over w's but over the characters
will give us something like this
so e is likely so this is a bigram of
the start character and e and this is a
bigram of the
a and the special end character
and now we can look at for example what
this looks like for
olivia or eva
and indeed we can actually
potentially do this for the entire data
set but we won't print that that's going
to be too much
but these are the individual character
diagrams and we can print them
now in order to learn the statistics
about which characters are likely to
follow other characters the simplest way
in the bigram language models is to
simply do it by counting
so we're basically just going to count
how often any one of these combinations
occurs in the training set
in these words
so we're going to need some kind of a
dictionary that's going to maintain some
counts for every one of these diagrams
so let's use a dictionary b
and this will map these bi-grams so
bi-gram is a tuple of character one
character two
and then b at bi-gram
will be b dot get of bi-gram
which is basically the same as b at
bigram
but in the case that bigram is not in
the dictionary b we would like to by
default return to zero
plus one
so this will basically add up all the
bigrams and count how often they occur
let's get rid of printing
or rather
let's keep the printing and let's just
inspect what b is in this case
and we see that many bi-grams occur just
a single time this one allegedly
occurred three times
so a was an ending character three times
and that's true for all of these words
all of emma olivia and eva and with a
so that's why this occurred three times
now let's do it for all the words
oops i should not have printed
i'm going to erase that
let's kill this
let's just run
and now b will have the statistics of
the entire data set
so these are the counts across all the
words of the individual pie grams
and we could for example look at some of
the most common ones and least common
ones
this kind of grows in python but the way
to do this the simplest way i like is we
just use b dot items
b dot items returns
the tuples of
key value in this case the keys are
the character diagrams and the values
are the counts
and so then what we want to do is we
want to do
sorted of this
but by default sort is on the first
on the first item of a tuple but we want
to sort by the values which are the
second element of a tuple that is the
key value
so we want to use the key
equals lambda
that takes the key value
and returns
the key value at the one not at zero but
at one which is the count so we want to
sort by the count
of these elements
and actually we wanted to go backwards
so here we have is
the bi-gram q and r occurs only a single
time
dz occurred only a single time
and when we sort this the other way
around
we're going to see the most likely
bigrams so we see that n was
very often an ending character
many many times and apparently n almost
always follows an a
and that's a very likely combination as
well
so
this is kind of the individual counts
that we achieve over the entire data set
now it's actually going to be
significantly more convenient for us to
keep this information in a
two-dimensional array instead of a
python dictionary
so
we're going to store this information
in a 2d array
and
the rows are going to be the first
character of the bigram and the columns
are going to be the second character and
each entry in this two-dimensional array
will tell us how often that first
character files the second character in
the data set
so in particular the array
representation that we're going to use
or the library is that of pytorch
and pytorch is a deep
learning neural network framework but
part of it is also this torch.tensor
which allows us to create
multi-dimensional arrays and manipulate
them very efficiently
so
let's import pytorch which you can do by
import torch
and then we can create
arrays
so let's create a array of zeros
and we give it a
size of this array let's create a three
by five array as an example
and
this is a three by five array of zeros
and by default you'll notice a.d type
which is short for data type is float32
so these are single precision floating
point numbers
because we are going to represent counts
let's actually use d type as torch dot
and 32
so these are
32-bit integers
so now you see that we have integer data
inside this tensor
now tensors allow us to really
manipulate all the individual entries
and do it very efficiently
so for example if we want to change this
bit
we have to index into the tensor and in
particular here this is the first row
and the
because it's zero indexed so this is row
index one and column index zero one two
three
so a at one comma three we can set that
to one
and then a we'll have a 1 over there
we can of course also do things like
this so now a will be 2 over there
or 3.
and also we can for example say a 0 0 is
5
and then a will have a 5 over here
so that's how we can index into the
arrays now of course the array that we
are interested in is much much bigger so
for our purposes we have 26 letters of
the alphabet
and then we have two special characters
s and e
so uh we want 26 plus 2 or 28 by 28
array
and let's call it the capital n because
it's going to represent sort of the
counts
let me erase this stuff
so that's the array that starts at zeros
28 by 28
and now let's copy paste this
here
but instead of having a dictionary b
which we're going to erase we now have
an n
now the problem here is that we have
these characters which are strings but
we have to now
um basically index into a
um array and we have to index using
integers so we need some kind of a
lookup table from characters to integers
so let's construct such a character
array
and the way we're going to do this is
we're going to take all the words which
is a list of strings
we're going to concatenate all of it
into a massive string so this is just
simply the entire data set as a single
string
we're going to pass this to the set
constructor which takes this massive
string
and throws out duplicates because sets
do not allow duplicates
so set of this will just be the set of
all the lowercase characters
and there should be a total of 26 of
them
and now we actually don't want a set we
want a list
but we don't want a list sorted in some
weird arbitrary way we want it to be
sorted
from a to z
so sorted list
so those are our characters
now what we want is this lookup table as
i mentioned so let's create a special
s2i i will call it
um s is string or character and this
will be an s2i mapping
for
is in enumerate of these characters
so enumerate basically gives us this
iterator over the integer index and the
actual element of the list and then we
are mapping the character to the integer
so s2i
is a mapping from a to 0 b to 1 etc all
the way from z to 25
and that's going to be useful here but
we actually also have to specifically
set that s will be 26
and s to i at e will be 27 right because
z was 25.
so those are the lookups and now we can
come here and we can map
both character 1 and character 2 to
their integers
so this will be s2i at character 1
and ix2 will be s2i of character 2.
and now we should be able to
do this line but using our array so n at
x1 ix2 this is the two-dimensional array
indexing i've shown you before
and honestly just plus equals one
because everything starts at
zero
so this should
work
and give us a large 28 by 28 array
of all these counts so
if we print n
this is the array but of course it looks
ugly so let's erase this ugly mess and
let's try to visualize it a bit more
nicer
so for that we're going to use a library
called matplotlib
so matplotlib allows us to create
figures so we can do things like plt
item show of the counter array
so this is the 28x28 array
and this is structure but even this i
would say is still pretty ugly
so we're going to try to create a much
nicer visualization of it and i wrote a
bunch of code for that
the first thing we're going to need is
we're going to need to invert
this array here this dictionary so s2i
is mapping from s to i
and in i2s we're going to reverse this
dictionary so iterator of all the items
and just reverse that array
so i2s
maps inversely from 0 to a 1 to b etc
so we'll need that
and then here's the code that i came up
with to try to make this a little bit
nicer
we create a figure
we plot
n
and then we do and then we visualize a
bunch of things later let me just run it
so you get a sense of what this is
okay
so you see here that we have
the array spaced out
and every one of these is basically like
b follows g zero times
b follows h 41 times
um so a follows j 175 times
and so what you can see that i'm doing
here is first i show that entire array
and then i iterate over all the
individual little cells here
and i create a character string here
which is the inverse mapping i2s of the
integer i and the integer j so those are
the bi-grams in a character
representation
and then i plot just the diagram text
and then i plot the number of times that
this bigram occurs
now the reason that there's a dot item
here is because when you index into
these arrays these are torch tensors
you see that we still get a tensor back
so the type of this thing you'd think it
would be just an integer 149 but it's
actually a torch.tensor
and so
if you do dot item then it will pop out
that in individual integer
so it will just be 149.
so that's what's happening there and
these are just some options to make it
look nice
so what is the structure of this array
we have all these counts and we see that
some of them occur often and some of
them do not occur often
now if you scrutinize this carefully you
will notice that we're not actually
being very clever
that's because when you come over here
you'll notice that for example we have
an entire row of completely zeros and
that's because the end character
is never possibly going to be the first
character of a bi-gram because we're
always placing these end tokens all at
the end of the diagram
similarly we have entire columns zeros
here because the s
character will never possibly be the
second element of a bigram because we
always start with s and we end with e
and we only have the words in between
so we have an entire column of zeros an
entire row of zeros and in this little
two by two matrix here as well the only
one that can possibly happen is if s
directly follows e
that can be non-zero if we have a word
that has no letters so in that case
there's no letters in the word it's an
empty word and we just have s follows e
but the other ones are just not possible
and so we're basically wasting space and
not only that but the s and the e are
getting very crowded here
i was using these brackets because
there's convention and natural language
processing to use these kinds of
brackets to denote special tokens
but we're going to use something else
so let's fix all this and make it
prettier
we're not actually going to have two
special tokens we're only going to have
one special token
so
we're going to have n by n
array of 27 by 27 instead
instead of having two
we will just have one and i will call it
a dot
okay
let me swing this over here
now one more thing that i would like to
do is i would actually like to make this
special character half position zero
and i would like to offset all the other
letters off i find that a little bit
more
pleasing
so
we need a plus one here so that the
first character which is a will start at
one
so s2i
will now be a starts at one and dot is 0
and
i2s of course we're not changing this
because i2s just creates a reverse
mapping and this will work fine so 1 is
a 2 is b
0 is dot
so we've reversed that here
we have
a dot and a dot
this should work fine
make sure i start at zeros
count
and then here we don't go up to 28 we go
up to 27
and this should just work
okay
so we see that dot never happened it's
at zero because we don't have empty
words
then this row here now is just uh very
simply the um
counts for all the first letters so
uh j starts a word h starts a word i
starts a word etc and then these are all
the ending
characters
and in between we have the structure of
what characters follow each other
so this is the counts array of our
entire
data set so this array actually has all
the information necessary for us to
actually sample from this bigram
uh character level language model
and um roughly speaking what we're going
to do is we're just going to start
following these probabilities and these
counts and we're going to start sampling
from the from the model
so in the beginning of course
we start with the dot the start token
dot
so to sample the first character of a
name we're looking at this row here
so we see that we have the counts and
those concepts terminally are telling us
how often any one of these characters is
to start a word
so if we take this n
and we grab the first row
we can do that by using just indexing as
zero
and then using this notation column for
the rest of that row
so n zero colon
is indexing into the zeroth
row and then it's grabbing all the
columns
and so this will give us a
one-dimensional array
of the first row so zero four four ten
you know zero four four ten one three oh
six one five four two etc it's just the
first row the shape of this
is 27 it's just the row of 27
and the other way that you can do this
also is you just you don't need to
actually give this
you just grab the zeroth row like this
this is equivalent
now these are the counts
and now what we'd like to do is we'd
like to basically um sample from this
since these are the raw counts we
actually have to convert this to
probabilities
so we create a probability vector
so we'll take n of zero
and we'll actually convert this to float
first
okay so these integers are converted to
float
floating point numbers and the reason
we're creating floats is because we're
about to normalize these counts
so to create a probability distribution
here we want to divide
we basically want to do p p p divide p
that sum
and now we get a vector of smaller
numbers and these are now probabilities
so of course because we divided by the
sum the sum of p now is 1.
so this is a nice proper probability
distribution it sums to 1 and this is
giving us the probability for any single
character to be the first
character of a word
so now we can try to sample from this
distribution to sample from these
distributions we're going to use
storch.multinomial which i've pulled up
here
so torch.multinomial returns uh
samples from the multinomial probability
distribution which is a complicated way
of saying you give me probabilities and
i will give you integers which are
sampled
according to the property distribution
so this is the signature of the method
and to make everything deterministic
we're going to use a generator object in
pytorch
so this makes everything deterministic
so when you run this on your computer
you're going to the exact get the exact
same results that i'm getting here on my
computer
so let me show you how this works
here's the deterministic way of creating
a torch generator object
seeding it with some number that we can
agree on
so that seeds a generator gets gives us
an object g
and then we can pass that g
to a function
that creates um
here random numbers twerk.rand creates
random numbers three of them
and it's using this generator object to
as a source of randomness
so
without normalizing it
i can just print
this is sort of like numbers between 0
and 1 that are random according to this
thing and whenever i run it again
i'm always going to get the same result
because i keep using the same generator
object which i'm seeing here
and then if i divide
to normalize i'm going to get a nice
probability distribution of just three
elements
and then we can use torsion multinomial
to draw samples from it so this is what
that looks like
tertiary multinomial we'll take the
torch tensor
of probability distributions
then we can ask for a number of samples
let's say 20.
replacement equals true means that when
we draw an element
we will uh we can draw it and then we
can put it back into the list of
eligible indices to draw again
and we have to specify replacement as
true because by default uh for some
reason it's false
and i think
you know it's just something to be
careful with
and the generator is passed in here so
we're going to always get deterministic
results the same results so if i run
these two
we're going to get a bunch of samples
from this distribution
now you'll notice here that the
probability for the
first element in this tensor is 60
so in these 20 samples we'd expect 60 of
them to be zero
we'd expect thirty percent of them to be
one
and because the the element index two
has only ten percent probability very
few of these samples should be two and
indeed we only have a small number of
twos
and we can sample as many as we'd like
and the more we sample the more
these numbers should um roughly have the
distribution here
so we should have lots of zeros
half as many um
ones and we should have um three times
as few
oh sorry s few ones and three times as
few uh
twos
so you see that we have very few twos we
have some ones and most of them are zero
so that's what torsion multinomial is
doing
for us here
we are interested in this row we've
created this
p here
and now we can sample from it
so if we use the same
seed
and then we sample from this
distribution let's just get one sample
then we see that the sample is say 13.
so this will be the index
and let's you see how it's a tensor that
wraps 13 we again have to use that item
to pop out that integer
and now index would be just the number
13.
and of course the um we can do
we can map the i2s of ix to figure out
exactly which character
we're sampling here we're sampling m
so we're saying that the first character
is
in our generation
and just looking at the road here
m was drawn and you we can see that m
actually starts a large number of words
uh m
started 2 500 words out of 32 000 words
so almost
a bit less than 10 percent of the words
start with them so this was actually a
fairly likely character to draw
um
so that would be the first character of
our work and now we can continue to
sample more characters because now we
know that m started
m is already sampled
so now to draw the next character we
will come back here and we will look for
the row
that starts with m
so you see m
and we have a row here
so we see that m dot is
516 m a is this many and b is this many
etc so these are the counts for the next
row and that's the next character that
we are going to now generate
so i think we are ready to actually just
write out the loop because i think
you're starting to get a sense of how
this is going to go
the um
we always begin at
index 0 because that's the start token
and then while true
we're going to grab the row
corresponding to index
that we're currently on so that's p
so that's n array at ix
converted to float is rp
then we normalize
this p to sum to one
i accidentally ran the infinite loop we
normalize p to something one
then we need this generator object
now we're going to initialize up here
and we're going to draw a single sample
from this distribution
and then this is going to tell us what
index is going to be next
if the index sampled is
0
then that's now the end token
so we will break
otherwise we are going to print
s2i of ix
i2s
and uh that's pretty much it we're just
uh this should work okay more
so that's that's the name that we've
sampled we started with m the next step
was o then r and then dot
and this dot we it here as well
so
let's now do this a few times
so let's actually create an
out list here
and instead of printing we're going to
append
so out that append this character
and then here let's just print it at the
end so let's just join up all the outs
and we're just going to print more okay
now we're always getting the same result
because of the generator
so if we want to do this a few times we
can go for i in range
10 we can sample 10 names
and we can just do that 10 times
and these are the names that we're
getting out
let's do 20.
i'll be honest with you this doesn't
look right
so i started a few minutes to convince
myself that it actually is right
the reason these samples are so terrible
is that bigram language model
is actually look just like really
terrible
we can generate a few more here
and you can see that they're kind of
like their name like a little bit like
yanu o'reilly etc but they're just like
totally messed up um
and i mean the reason that this is so
bad like we're generating h as a name
but you have to think through
it from the model's eyes it doesn't know
that this h is the very first h all it
knows is that h was previously and now
how likely is h the last character well
it's somewhat
likely and so it just makes it last
character it doesn't know that there
were other things before it or there
were not other things before it and so
that's why it's generating all these
like
nonsense names
another way to do this is
to convince yourself that this is
actually doing something reasonable even
though it's so terrible is
these little piece here are 27 right
like 27.
so how about if we did something like
this
instead of p having any structure
whatsoever
how about if p was just
torch dot once
of 27
by default this is a float 32 so this is
fine divide 27
so what i'm doing here is this is the
uniform distribution which will make
everything equally likely
and we can sample from that so let's see
if that does any better
okay so it's
this is what you have from a model that
is completely untrained where everything
is equally likely so it's obviously
garbage and then if we have a trained
model which is trained on just bi-grams
this is what we get so you can see that
it is more name-like it is actually
working it's just um
my gram is so terrible and we have to do
better now next i would like to fix an
inefficiency that we have going on here
because what we're doing here is we're
always fetching a row of n from the
counts matrix up ahead
and then we're always doing the same
things we're converting to float and
we're dividing and we're doing this
every single iteration of this loop and
we just keep renormalizing these rows
over and over again and it's extremely
inefficient and wasteful so what i'd
like to do is i'd like to actually
prepare a matrix capital p that will
just have the probabilities in it so in
other words it's going to be the same as
the capital n matrix here of counts but
every single row will have the row of
probabilities uh that is normalized to 1
indicating the probability distribution
for the next character given the
character before it
um as defined by which row we're in
so basically what we'd like to do is
we'd like to just do it up front here
and then we would like to just use that
row here so here we would like to just
do p equals p of ix instead
okay
the other reason i want to do this is
not just for efficiency but also i would
like us to practice
these n-dimensional tensors and i'd like
us to practice their manipulation and
especially something that's called
broadcasting that we'll go into in a
second
we're actually going to have to become
very good at these tensor manipulations
because if we're going to build out all
the way to transformers we're going to
be doing some pretty complicated um
array operations for efficiency and we
need to really understand that and be
very good at it
so intuitively what we want to do is we
first want to grab the floating point
copy of n
and i'm mimicking the line here
basically
and then we want to divide all the rows
so that they sum to 1.
so we'd like to do something like this p
divide p dot sum
but
now we have to be careful
because p dot sum actually
produces a sum
sorry equals and that float copy
p dot sum produces a um
sums up all of the counts of this entire
matrix n and gives us a single number of
just the summation of everything so
that's not the way we want to define
divide we want to simultaneously and in
parallel divide all the rows
by their respective sums
so what we have to do now is we have to
go into documentation for torch.sum
and we can scroll down here to a
definition that is relevant to us which
is where we don't only provide an input
array that we want to sum but we also
provide the dimension along which we
want to sum
and in particular we want to sum up
over rows
right
now one more argument that i want you to
pay attention to here is the keep them
is false
if keep them is true then the output
tensor is of the same size as input
except of course the dimension along
which is summed which will become just
one
but if you pass in keep them as false
then this dimension is squeezed out and
so torch.sum not only does the sum and
collapses dimension to be of size one
but in addition it does what's called a
squeeze where it squeezes out it
squeezes out that dimension
so
basically what we want here is we
instead want to do p dot sum of some
axis
and in particular notice that p dot
shape is 27 by 27
so when we sum up across axis zero then
we would be taking the zeroth dimension
and we would be summing across it
so when keep them as true
then this thing will not only give us
the counts across um
along the columns
but notice that basically the shape of
this is 1 by 27 we just get a row vector
and the reason we get a row vector here
again is because we passed in zero
dimension so this zero dimension becomes
one and we've done a sum
and we get a row and so basically we've
done the sum
this way
vertically and arrived at just a single
1 by 27
vector of counts
what happens when you take out keep them
is that we just get 27. so it squeezes
out that dimension and we just get
a one-dimensional vector of size 27.
now we don't actually want
one by 27 row vector because that gives
us the counts or the sums across
the columns
we actually want to sum the other way
along dimension one and you'll see that
the shape of this is 27 by one so it's a
column vector it's a 27 by one
vector of counts
okay
and that's because what's happened here
is that we're going horizontally and
this 27 by 27 matrix becomes a 27 by 1
array
now you'll notice by the way that um the
actual numbers
of these counts are identical
and that's because this special array of
counts here comes from bi-gram
statistics and actually it just so
happens by chance
or because of the way this array is
constructed that the sums along the
columns or along the rows horizontally
or vertically is identical
but actually what we want to do in this
case is we want to sum across the
rows
horizontally so what we want here is p
that sum of one with keep in true
27 by one column vector
and now what we want to do is we want to
divide by that
now we have to be careful here again is
it possible to take
what's a um p dot shape you see here 27
by 27 is it possible to take a 27 by 27
array and divide it by what is a 27 by 1
array
is that an operation that you can do
and whether or not you can perform this
operation is determined by what's called
broadcasting rules so if you just search
broadcasting semantics in torch
you'll notice that there's a special
definition for
what's called broadcasting that uh for
whether or not um these two uh arrays
can be combined in a binary operation
like division
so the first condition is each tensor
has at least one dimension which is the
case for us
and then when iterating over the
dimension sizes starting at the trailing
dimension
the dimension sizes must either be equal
one of them is one or one of them does
not exist
okay
so let's do that we need to align the
two arrays and their shapes which is
very easy because both of these shapes
have two elements so they're aligned
then we iterate over from the from the
right and going to the left
each dimension must be either equal one
of them is a one or one of them does not
exist so in this case they're not equal
but one of them is a one so this is fine
and then this dimension they're both
equal
so uh this is fine
so all the dimensions are fine and
therefore the this operation is
broadcastable so that means that this
operation is allowed
and what is it that these arrays do when
you divide 27 by 27 by 27 by one
what it does is that it takes this
dimension one and it stretches it out it
copies it to match
27 here in this case
so in our case it takes this column
vector which is 27 by 1
and it copies it 27 times
to make
these both be 27 by 27 internally you
can think of it that way and so it
copies those counts
and then it does an element-wise
division
which is what we want because these
counts we want to divide by them on
every single one of these columns in
this matrix
so this actually we expect will
normalize
every single row
and we can check that this is true by
taking the first row for example and
taking its sum we expect this to be
1. because it's not normalized
and then we expect this now because if
we actually correctly normalize all the
rows we expect to get the exact same
result here so let's run this
it's the exact same result
this is correct so now i would like to
scare you a little bit
uh you actually have to like i basically
encourage you very strongly to read
through broadcasting semantics
and i encourage you to treat this with
respect and it's not something to play
fast and loose with it's something to
really respect really understand and
look up maybe some tutorials for
broadcasting and practice it and be
careful with it because you can very
quickly run into books let me show you
what i mean
you see how here we have p dot sum of
one keep them as true
the shape of this is 27 by one let me
take out this line just so we have the n
and then we can see the counts
we can see that this is a all the counts
across all the
rows
and it's a 27 by one column vector right
now suppose that i tried to do the
following
but i erase keep them just true here
what does that do if keep them is not
true it's false then remember according
to documentation it gets rid of this
dimension one it squeezes it out so
basically we just get all the same
counts the same result except the shape
of it is not 27 by 1 it is just 27 the
one disappears
but all the counts are the same
so you'd think that this divide that
would uh would work
first of all can we even uh write this
and will it is it even is it even
expected to run is it broadcastable
let's determine if this result is
broadcastable
p.summit one is shape
is 27.
this is 27 by 27. so 27 by 27
broadcasting into 27. so now
rules of broadcasting number one align
all the dimensions on the right done now
iteration over all the dimensions
starting from the right going to the
left
all the dimensions must either be equal
one of them must be one or one that does
not exist so here they are all equal
here the dimension does not exist
so internally what broadcasting will do
is it will create a one here
and then
we see that one of them is a one and
this will get copied and this will run
this will broadcast
okay so you'd expect this
to work
because we we are
this broadcast and this we can divide
this
now if i run this you'd expect it to
work but
it doesn't
uh you actually get garbage you get a
wrong dissolve because this is actually
a bug
this keep them equals true
makes it work
this is a bug
in both cases we are doing
the correct counts we are summing up
across the rows
but keep them is saving us and making it
work so in this case
i'd like to encourage you to potentially
like pause this video at this point and
try to think about why this is buggy and
why the keep dim was necessary here
okay
so the reason to do
for this is i'm trying to hint it here
when i was sort of giving you a bit of a
hint on how this works
this
27 vector
internally inside the broadcasting this
becomes a 1 by 27
and 1 by 27 is a row vector right
and now we are dividing 27 by 27 by 1 by
27
and torch will replicate this dimension
so basically
uh it will take
it will take this
row vector and it will copy it
vertically now
27 times so the 27 by 27 lies exactly
and element wise divides
and so basically what's happening here
is
we're actually normalizing the columns
instead of normalizing the rows
so you can check that what's happening
here is that p at zero which is the
first row of p dot sum
is not one it's seven
it is the first column as an example
that sums to one
so
to summarize where does the issue come
from the issue comes from the silent
adding of a dimension here because in
broadcasting rules you align on the
right and go from right to left and if
dimension doesn't exist you create it
so that's where the problem happens we
still did the counts correctly we did
the counts across the rows and we got
the the counts on the right here as a
column vector but because the keep
things was true this this uh this
dimension was discarded and now we just
have a vector of 27. and because of
broadcasting the way it works this
vector of 27 suddenly becomes a row
vector
and then this row vector gets replicated
vertically and that every single point
we are dividing by the by the count
in the opposite direction
so uh
so this thing just uh doesn't work this
needs to be keep things equal true in
this case
so then
then we have that p at zero is
normalized
and conversely the first column you'd
expect to potentially not be normalized
and this is what makes it work
so pretty subtle and uh hopefully this
helps to scare you that you should have
a respect for broadcasting be careful
check your work uh and uh understand how
it works under the hood and make sure
that it's broadcasting in the direction
that you like otherwise you're going to
introduce very subtle bugs very hard to
find bugs and uh just be careful one
more note on efficiency we don't want to
be doing this here because this creates
a completely new tensor that we store
into p
we prefer to use in place operations if
possible
so this would be an in-place operation
it has the potential to be faster it
doesn't create new memory
under the hood and then let's erase this
we don't need it
and let's
also
um just do fewer just so i'm not wasting
space
okay so we're actually in a pretty good
spot now
we trained a bigram language model and
we trained it really just by counting uh
how frequently any pairing occurs and
then normalizing so that we get a nice
property distribution
so really these elements of this array p
are really the parameters of our biogram
language model giving us and summarizing
the statistics of these bigrams
so we train the model and then we know
how to sample from a model we just
iteratively uh sample the next character
and feed it in each time and get a next
character
now what i'd like to do is i'd like to
somehow evaluate the quality of this
model we'd like to somehow summarize the
quality of this model into a single
number how good is it at predicting
the training set
and as an example so in the training set
we can evaluate now the training loss
and this training loss is telling us
about
sort of the quality of this model in a
single number just like we saw in
micrograd
so let's try to think through the
quality of the model and how we would
evaluate it
basically what we're going to do is
we're going to copy paste this code
that we previously used for counting
okay
and let me just print these diagrams
first we're gonna use f strings
and i'm gonna print character one
followed by character two these are the
diagrams and then i don't wanna do it
for all the words just do the first
three words so here we have emma olivia
and ava bigrams
now what we'd like to do is we'd like to
basically look at the probability that
the model assigns to every one of these
diagrams
so in other words we can look at the
probability which is
summarized in the matrix b
of i x 1 x 2
and then we can print it here
as probability
and because these properties are way too
large let me present
or call in 0.4 f
to like truncate it a bit
so what do we have here right we're
looking at the probabilities that the
model assigns to every one of these
bigrams in the dataset
and so we can see some of them are four
percent three percent etc
just to have a measuring stick in our
mind by the way um we have 27 possible
characters or tokens and if everything
was equally likely then you'd expect all
these probabilities
to be
four percent roughly
so anything above four percent means
that we've learned something useful from
these bigram statistics and you see that
roughly some of these are four percent
but some of them are as high as 40
percent
35 percent and so on so you see that the
model actually assigned a pretty high
probability to whatever's in the
training set and so that's a good thing
um basically if you have a very good
model you'd expect that these
probabilities should be near one because
that means that your model is correctly
predicting what's going to come next
especially on the training set where you
where you trained your model
so
now we'd like to think about how can we
summarize these probabilities into a
single number that measures the quality
of this model
now when you look at the literature into
maximum likelihood estimation and
statistical modeling and so on
you'll see that what's typically used
here is something called the likelihood
and the likelihood is the product of all
of these probabilities
and so the product of all these
probabilities is the likelihood and it's
really telling us about the probability
of the entire data set assigned uh
assigned by the model that we've trained
and that is a measure of quality
so the product of these
should be as high as possible
when you are training the model and when
you have a good model your pro your
product of these probabilities should be
very high
um
now because the product of these
probabilities is an unwieldy thing to
work with you can see that all of them
are between zero and one so your product
of these probabilities will be a very
tiny number
um
so
for convenience what people work with
usually is not the likelihood but they
work with what's called the log
likelihood
so
the product of these is the likelihood
to get the log likelihood we just have
to take the log of the probability
and so the log of the probability here i
have the log of x from zero to one
the log is a you see here monotonic
transformation of the probability
where if you pass in one
you get zero
so probability one gets your log
probability of zero
and then as you go lower and lower
probability the log will grow more and
more negative until all the way to
negative infinity at zero
so here we have a log prob which is
really just a torch.log of probability
let's print it out to get a sense of
what that looks like
log prob
also 0.4 f
okay
so as you can see when we plug in
numbers that are very close some of our
higher numbers we get closer and closer
to zero
and then if we plug in very bad
probabilities we get more and more
negative number that's bad
so
and the reason we work with this is for
a large extent convenience right
because we have mathematically that if
you have some product a times b times c
of all these probabilities right
the likelihood is the product of all
these probabilities
then the log
of these
is just log of a plus
log of b
plus log of c if you remember your logs
from your
high school or undergrad and so on
so we have that basically
the likelihood of the product
probabilities the log likelihood is just
the sum of the logs of the individual
probabilities
so
log likelihood
starts at zero
and then log likelihood here we can just
accumulate simply
and in the end we can print this
print the log likelihood
f strings
maybe you're familiar with this
so log likelihood is negative 38.
okay
now
we actually want um
so how high can log likelihood get it
can go to zero so when all the
probabilities are one log likelihood
will be zero and then when all the
probabilities are lower this will grow
more and more negative
now we don't actually like this because
what we'd like is a loss function and a
loss function has the semantics that low
is good
because we're trying to minimize the
loss so we actually need to invert this
and that's what gives us something
called the negative log likelihood
negative log likelihood is just negative
of the log likelihood
these are f strings by the way if you'd
like to look this up
negative log likelihood equals
so negative log likelihood now is just
negative of it and so the negative log
block load is a very nice loss function
because um
the lowest it can get is zero
and the higher it is the worse off the
predictions are that you're making
and then one more modification to this
that sometimes people do is that for
convenience uh they actually like to
normalize by they like to make it an
average instead of a sum
and so uh here
let's just keep some counts as well
so n plus equals one
starts at zero
and then here
um we can have sort of like a normalized
log likelihood
um
if we just normalize it by the count
then we will sort of get the average
log likelihood so this would be
usually our loss function here is what
this we would this is what we would use
uh so our loss function for the training
set assigned by the model is 2.4 that's
the quality of this model
and the lower it is the better off we
are and the higher it is the worse off
we are
and
the job of our you know training is to
find the parameters that minimize the
negative log likelihood loss
and that would be like a high quality
model okay so to summarize i actually
wrote it out here
so our goal is to maximize likelihood
which is the
product of all the probabilities
assigned by the model
and we want to maximize this likelihood
with respect to the model parameters and
in our case the model parameters here
are defined in the table these numbers
the probabilities
are
the model parameters sort of in our
program language models so far but you
have to keep in mind that here we are
storing everything in a table format the
probabilities but what's coming up as a
brief preview is that these numbers will
not be kept explicitly but these numbers
will be calculated by a neural network
so that's coming up
and we want to change and tune the
parameters of these neural networks we
want to change these parameters to
maximize the likelihood the product of
the probabilities
now maximizing the likelihood is
equivalent to maximizing the log
likelihood because log is a monotonic
function
here's the graph of log
and basically all it is doing is it's
just scaling your um you can look at it
as just a scaling of the loss function
and so the optimization problem here and
here are actually equivalent because
this is just scaling you can look at it
that way
and so these are two identical
optimization problems
um
maximizing the log-likelihood is
equivalent to minimizing the negative
log likelihood and then in practice
people actually minimize the average
negative log likelihood to get numbers
like 2.4
and then this summarizes the quality of
your model and we'd like to minimize it
and make it as small as possible
and the lowest it can get is zero
and the lower it is
the better off your model is because
it's signing it's assigning high
probabilities to your data now let's
estimate the probability over the entire
training set just to make sure that we
get something around 2.4 let's run this
over the entire oops
let's take out the print segment as well
okay 2.45 or the entire training set
now what i'd like to show you is that
you can actually evaluate the
probability for any word that you want
like for example
if we just test a single word andre and
bring back the print statement
then you see that andre is actually kind
of like an unlikely word like on average
we take
three
log probability to represent it and
roughly that's because ej apparently is
very uncommon as an example
now
think through this um
when i take andre and i append q and i
test the probability of it under q
we actually get
infinity
and that's because jq has a zero percent
probability according to our model so
the log likelihood
so the log of zero will be negative
infinity we get infinite loss
so this is kind of undesirable right
because we plugged in a string that
could be like a somewhat reasonable name
but basically what this is saying is
that this model is exactly zero percent
likely to uh to predict this
name
and our loss is infinity on this example
and really what the reason for that is
that j
is followed by q
uh zero times
uh where's q jq is zero and so jq is uh
zero percent likely
so it's actually kind of gross and
people don't like this too much to fix
this there's a very simple fix that
people like to do to sort of like smooth
out your model a little bit and it's
called model smoothing and roughly
what's happening is that we will eight
we will add some fake counts
so
imagine adding a count of one to
everything
so we add a count of one
like this
and then we recalculate the
probabilities
and that's model smoothing and you can
add as much as you like you can add five
and it will give you a smoother model
and the more you add here
the more
uniform model you're going to have and
the less you add
the more peaked model you are going to
have of course
so one is like a pretty decent count to
add
and that will ensure that there will be
no zeros in our probability matrix p
and so this will of course change the
generations a little bit in this case it
didn't but in principle it could
but what that's going to do now is that
nothing will be infinity unlikely
so now
our model will predict some other
probability and we see that jq now has a
very small probability so the model
still finds it very surprising that this
was a word or a bigram but we don't get
negative infinity so it's kind of like a
nice fix that people like to apply
sometimes and it's called model
smoothing okay so we've now trained a
respectable bi-gram character level
language model and we saw that we both
sort of trained the model by looking at
the counts of all the bigrams and
normalizing the rows to get probability
distributions
we saw that we can also then use those
parameters of this model to perform
sampling of new words
so we sample new names according to
those distributions and we also saw that
we can evaluate the quality of this
model and the quality of this model is
summarized in a single number which is
the negative log likelihood and the
lower this number is the better the
model is
because it is giving high probabilities
to the actual next characters in all the
bi-grams in our training set
so that's all well and good but we've
arrived at this model explicitly by
doing something that felt sensible we
were just performing counts and then we
were normalizing those counts
now what i would like to do is i would
like to take an alternative approach we
will end up in a very very similar
position but the approach will look very
different because i would like to cast
the problem of bi-gram character level
language modeling into the neural
network framework
in the neural network framework we're
going to approach things slightly
differently but again end up in a very
similar spot i'll go into that later now
our neural network is going to be a
still a background character level
language model so it receives a single
character as an input
then there's neural network with some
weights or some parameters w
and it's going to output the probability
distribution over the next character in
a sequence it's going to make guesses as
to what is likely to follow this
character that was input to the model
and then in addition to that we're going
to be able to evaluate any setting of
the parameters of the neural net because
we have the loss function
the negative log likelihood so we're
going to take a look at its probability
distributions and we're going to use the
labels
which are basically just the identity of
the next character in that diagram the
second character
so knowing what second character
actually comes next in the bigram allows
us to then look at what how high of
probability the model assigns to that
character
and then we of course want the
probability to be very high
and that is another way of saying that
the loss is low
so we're going to use gradient-based
optimization then to tune the parameters
of this network because we have the loss
function and we're going to minimize it
so we're going to tune the weights so
that the neural net is correctly
predicting the probabilities for the
next character
so let's get started the first thing i
want to do is i want to compile the
training set of this neural network
right so
create
the training set
of all the bigrams
okay
and
here
i'm going to copy paste this code
because this code iterates over all the
programs
so here we start with the words we
iterate over all the bygrams and
previously as you recall we did the
counts but now we're not going to do
counts we're just creating a training
set
now this training set will be made up of
two lists
we have the
inputs
and the targets
the the labels
and these bi-grams will denote x y those
are the characters right
and so we're given the first character
of the bi-gram and then we're trying to
predict the next one
both of these are going to be integers
so here we'll take x's that append is
just
x1 ystat append ix2
and then here
we actually don't want lists of integers
we will create tensors out of these so
axis is torch.tensor of axis and wise a
storage.tensor of ys
and then
we don't actually want to take all the
words just yet because i want everything
to be manageable
so let's just do the first word which is
emma
and then it's clear what these x's and
y's would be
here let me print
character 1 character 2 just so you see
what's going on here
so the bigrams of these characters is
dot e e m m m a a dot so this single
word as i mentioned has one two three
four five examples for our neural
network
there are five separate examples in emma
and those examples are summarized here
when the input to the neural network is
integer 0
the desired label is integer 5 which
corresponds to e when the input to the
neural network is 5 we want its weights
to be arranged so that 13 gets a very
high probability
when 13 is put in we want 13 to have a
high probability
when 13 is put in we also want 1 to have
a high probability
when one is input we want zero to have a
very high probability so there are five
separate input examples to a neural nut
in this data set
i wanted to add a tangent of a node of
caution to be careful with a lot of the
apis of some of these frameworks
you saw me silently use torch.tensor
with a lowercase t
and the output looked right
but you should be aware that there's
actually two ways of constructing a
tensor there's a torch.lowercase tensor
and there's also a torch.capital tensor
class which you can also construct
so you can actually call both you can
also do torch.capital tensor
and you get a nexus and wise as well
so that's not confusing at all
um
there are threads on what is the
difference between these two
and um
unfortunately the docs are just like not
clear on the difference and when you
look at the the docs of lower case
tensor construct tensor with no autograd
history by copying data
it's just like it doesn't
it doesn't make sense so the actual
difference as far as i can tell is
explained eventually in this random
thread that you can google
and really it comes down to
i believe
that um
what is this
torch.tensor in first d-type the data
type automatically while torch.tensor
just returns a float tensor
i would recommend stick to
torch.lowercase tensor
so um
indeed we see that when i
construct this with a capital t the data
type here of xs is float32
but towards that lowercase tensor
you see how it's now x dot d type is now
integer
so um
it's advised that you use lowercase t
and you can read more about it if you
like in some of these threads but
basically
um
i'm pointing out some of these things
because i want to caution you and i want
you to re get used to reading a lot of
documentation and reading through a lot
of
q and a's and threads like this
and
you know some of the stuff is
unfortunately not easy and not very well
documented and you have to be careful
out there what we want here is integers
because that's what makes uh sense
um
and so
lowercase tensor is what we are using
okay now we want to think through how
we're going to feed in these examples
into a neural network
now it's not quite as straightforward as
plugging it in because these examples
right now are integers so there's like a
0 5 or 13 it gives us the index of the
character and you can't just plug an
integer index into a neural net
these neural nets right are sort of made
up of these neurons
and
these neurons have weights and as you
saw in micrograd these weights act
multiplicatively on the inputs w x plus
b there's 10 h's and so on and so it
doesn't really make sense to make an
input neuron take on integer values that
you feed in and then multiply on with
weights
so instead
a common way of encoding integers is
what's called one hot encoding
in one hot encoding
we take an integer like 13 and we create
a vector that is all zeros except for
the 13th dimension which we turn to a
one and then that vector can feed into a
neural net
now conveniently
uh pi torch actually has something
called the one hot
function inside torching and functional
it takes a tensor made up of integers
um
long is a is a as an integer
um
and it also takes a number of classes um
which is how large you want your uh
tensor uh your vector to be
so here let's import
torch.n.functional sf this is a common
way of importing it
and then let's do f.1 hot
and we feed in the integers that we want
to encode so we can actually feed in the
entire array of x's
and we can tell it that num classes is
27.
so it doesn't have to try to guess it it
may have guessed that it's only 13 and
would give us an incorrect result
so this is the one hot let's call this x
inc for x encoded
and then we see that x encoded that
shape is 5 by 27
and uh we can also visualize it plt.i am
show of x inc
to make it a little bit more clear
because this is a little messy
so we see that we've encoded all the
five examples uh into vectors we have
five examples so we have five rows and
each row here is now an example into a
neural nut
and we see that the appropriate bit is
turned on as a one and everything else
is zero
so um
here for example the zeroth bit is
turned on the fifth bit is turned on
13th bits are turned on for both of
these examples and then the first bit
here is turned on
so that's how we can encode
integers into vectors and then these
vectors can feed in to neural nets one
more issue to be careful with here by
the way is
let's look at the data type of encoding
we always want to be careful with data
types
what would you expect x encoding's data
type to be
when we're plugging numbers into neural
nuts we don't want them to be integers
we want them to be floating point
numbers that can take on various values
but the d type here is actually 64-bit
integer
and the reason for that i suspect is
that one hot received a 64-bit integer
here and it returned the same data type
and when you look at the signature of
one hot it doesn't even take a d type a
desired data type of the output tensor
and so we can't in a lot of functions in
torch we'd be able to do something like
d type equal storage.float32
which is what we want but one heart does
not support that
so instead we're going to want to cast
this to float like this
so that these
everything is the same
everything looks the same but the d-type
is float32 and floats can feed into
neural nets so now let's construct our
first neuron
this neuron will look at these input
vectors
and as you remember from micrograd these
neurons basically perform a very simple
function w x plus b where w x is a dot
product
right
so we can achieve the same thing here
let's first define the weights of this
neuron basically what are the initial
weights at initialization for this
neuron
let's initialize them with torch.rendin
torch.rendin
is um
fills a tensor with random numbers
drawn from a normal distribution
and a normal distribution
has
a probability density function like this
and so most of the numbers drawn from
this distribution will be around 0
but some of them will be as high as
almost three and so on and very few
numbers will be above three in magnitude
so we need to take a size as an input
here
and i'm going to use size as to be 27 by
one
so
27 by one and then let's visualize w so
w is a column vector of 27 numbers
and
these weights are then multiplied by the
inputs
so now to perform this multiplication we
can take x encoding and we can multiply
it with w
this is a matrix multiplication operator
in pi torch
and the output of this operation is five
by one
the reason is five by five is the
following
we took x encoding which is five by
twenty seven and we multiplied it by
twenty seven by one
and
in matrix multiplication
you see that the output will become five
by one because these 27
will multiply and add
so basically what we're seeing here outs
out of this operation
is we are seeing the five
activations
of this neuron
on these five inputs
and we've evaluated all of them in
parallel we didn't feed in just a single
input to the single neuron we fed in
simultaneously all the five inputs into
the same neuron
and in parallel patrol has evaluated
the wx plus b but here is just the wx
there's no bias
it has value w times x for all of them
independently now instead of a single
neuron though i would like to have 27
neurons and i'll show you in a second
why i want 27 neurons
so instead of having just a 1 here which
is indicating this presence of one
single neuron
we can use 27
and then when w is 27 by 27
this will in parallel evaluate all the
27 neurons on all the 5 inputs
giving us a much better much much bigger
result so now what we've done is 5 by 27
multiplied 27 by 27
and the output of this is now 5 by 27
so we can see that the shape of this
is 5 by 27.
so what is every element here telling us
right
it's telling us for every one of 27
neurons that we created
what is the firing rate of those neurons
on every one of those five examples
so
the element for example 3 comma 13
is giving us the firing rate of the 13th
neuron
looking at the third input
and the way this was achieved is by a
dot product
between the third
input
and the 13th column
of this w matrix here
okay
so
using matrix multiplication we can very
efficiently evaluate
the dot product between lots of input
examples in a batch
and lots of neurons where all those
neurons have weights in the columns of
those w's
and in matrix multiplication we're just
doing those dot products and
in parallel just to show you that this
is the case we can take x and we can
take the third
row
and we can take the w and take its 13th
column
and then we can do
x and get three
elementwise multiply with w at 13.
and sum that up that's wx plus b
well there's no plus b it's just wx dot
product
and that's
this number
so you see that this is just being done
efficiently by the matrix multiplication
operation
for all the input examples and for all
the output neurons of this first layer
okay so we fed our 27-dimensional inputs
into a first layer of a neural net that
has 27 neurons right so we have 27
inputs and now we have 27 neurons these
neurons perform w times x they don't
have a bias and they don't have a
non-linearity like 10 h we're going to
leave them to be a linear layer
in addition to that we're not going to
have any other layers this is going to
be it it's just going to be
the dumbest smallest simplest neural net
which is just a single linear layer
and now i'd like to explain what i want
those 27 outputs to be
intuitively what we're trying to produce
here for every single input example is
we're trying to produce some kind of a
probability distribution for the next
character in a sequence
and there's 27 of them
but we have to come up with like precise
semantics for exactly how we're going to
interpret these 27 numbers that these
neurons take on
now intuitively
you see here that these numbers are
negative and some of them are positive
etc
and that's because these are coming out
of a neural net layer initialized with
these
normal distribution
parameters
but what we want is we want something
like we had here
like each row here
told us the counts and then we
normalized the counts to get
probabilities and we want something
similar to come out of the neural net
but what we just have right now is just
some negative and positive numbers
now we want those numbers to somehow
represent the probabilities for the next
character
but you see that probabilities they they
have a special structure they um
they're positive numbers and they sum to
one
and so that doesn't just come out of a
neural net
and then they can't be counts
because these counts are positive and
counts are integers
so counts are also not really a good
thing to output from a neural net
so instead what the neural net is going
to output and how we are going to
interpret the um
the 27 numbers is that these 27 numbers
are giving us log counts
basically
um
so instead of giving us counts directly
like in this table they're giving us log
counts
and to get the counts we're going to
take the log counts and we're going to
exponentiate them
now
exponentiation
takes the following form
it takes numbers
that are negative or they are positive
it takes the entire real line
and then if you plug in negative numbers
you're going to get e to the x
which is uh always below one
so you're getting numbers lower than one
and if you plug in numbers greater than
zero you're getting numbers greater than
one all the way growing to the infinity
and this here grows to zero
so basically we're going to
take these numbers
here
and
instead of them being positive and
negative and all over the place we're
going to interpret them as log counts
and then we're going to element wise
exponentiate these numbers
exponentiating them now gives us
something like this
and you see that these numbers now
because they went through an exponent
all the negative numbers turned into
numbers below 1 like 0.338 and all the
positive numbers originally turned into
even more positive numbers sort of
greater than one
so like for example
seven
is some positive number over here
that is greater than zero
but exponentiated outputs here
basically give us something that we can
use and interpret as the equivalent of
counts originally so you see these
counts here 112 7 51 1 etc
the neural net is kind of now predicting
uh
counts
and these counts are positive numbers
they can never be below zero so that
makes sense
and uh they can now take on various
values
depending on the settings of w
so let me break this down
we're going to interpret these to be the
log counts
in other words for this that is often
used is so-called logits
these are logits log counts
then these will be sort of the counts
largest exponentiated
and this is equivalent to the n matrix
sort of the n
array that we used previously remember
this was the n
this is the the array of counts
and each row here are the counts for the
for the um
next character sort of
so those are the counts and now the
probabilities are just the counts um
normalized
and so um
i'm not going to find the same but
basically i'm not going to scroll all
over the place
we've already done this we want to
counts that sum
along the first dimension and we want to
keep them as true
we've went over this and this is how we
normalize the rows of our counts matrix
to get our probabilities
props
so now these are the probabilities
and
these are the counts that we ask
currently and now when i show the
probabilities
you see that um
every row here
of course
will sum to 1
because they're normalized
and the shape of this
is 5 by 27
and so really what we've achieved is for
every one of our five examples
we now have a row that came out of a
neural net
and because of the transformations here
we made sure that this output of this
neural net now are probabilities or we
can interpret to be probabilities
so
our wx here gave us logits
and then we interpret those to be log
counts
we exponentiate to get something that
looks like counts
and then we normalize those counts to
get a probability distribution
and all of these are differentiable
operations
so what we've done now is we're taking
inputs we have differentiable operations
that we can back propagate through
and we're getting out probability
distributions
so
for example for the zeroth example that
fed in
right which was um
the zeroth example here was a one-half
vector of zero
and um
it basically corresponded to feeding in
this example here so we're feeding in a
dot into a neural net and the way we fed
the dot into a neural net is that we
first got its index
then we one hot encoded it
then it went into the neural net and out
came
this distribution of probabilities
and its shape
is 27 there's 27 numbers and we're going
to interpret this as the neural nets
assignment for how likely every one of
these characters um
the 27 characters are to come next
and as we tune the weights w
we're going to be of course getting
different probabilities out for any
character that you input
and so now the question is just can we
optimize and find a good w
such that the probabilities coming out
are pretty good and the way we measure
pretty good is by the loss function okay
so i organized everything into a single
summary so that hopefully it's a bit
more clear so it starts here
with an input data set
we have some inputs to the neural net
and we have some labels for the correct
next character in a sequence these are
integers
here i'm using uh torch generators now
so that you see the same numbers that i
see
and i'm generating um
27 neurons weights
and each neuron here receives 27 inputs
then here we're going to plug in all the
input examples x's into a neural net so
here this is a forward pass
first we have to encode all of the
inputs into one hot representations
so we have 27 classes we pass in these
integers and
x inc becomes a array that is 5 by 27
zeros except for a few ones
we then multiply this in the first layer
of a neural net to get logits
exponentiate the logits to get fake
counts sort of
and normalize these counts to get
probabilities
so we lock these last two lines by the
way here are called the softmax
which i pulled up here soft max is a
very often used layer in a neural net
that takes these z's which are logics
exponentiates them
and divides and normalizes it's a way of
taking
outputs of a neural net layer and these
these outputs can be positive or
negative
and it outputs probability distributions
it outputs something that is always
sums to one and are positive numbers
just like probabilities
um so it's kind of like a normalization
function if you want to think of it that
way and you can put it on top of any
other linear layer inside a neural net
and it basically makes a neural net
output probabilities that's very often
used and we used it as well here
so this is the forward pass and that's
how we made a neural net output
probability
now
you'll notice that
um
all of these
this entire forward pass is made up of
differentiable
layers everything here we can back
propagate through and we saw some of the
back propagation in micrograd
this is just
multiplication and addition all that's
happening here is just multiply and then
add and we know how to backpropagate
through them
exponentiation we know how to
backpropagate through
and then here we are summing
and sum is is easily backpropagable as
well
and division as well so everything here
is differentiable operation
and we can back propagate through
now we achieve these probabilities which
are 5 by 27
for every single example we have a
vector of probabilities that's into one
and then here i wrote a bunch of stuff
to sort of like break down uh the
examples
so we have five examples making up emma
right
and there are five bigrams inside emma
so bigram example a bigram example1 is
that e is the beginning character right
after dot
and the indexes for these are zero and
five
so then we feed in a zero
that's the input of the neural net
we get probabilities from the neural net
that are 27 numbers
and then the label is 5 because e
actually comes after dot
so that's the label
and then
we use this label 5 to index into the
probability distribution here
so
this
index 5 here is 0 1 2 3 4 5. it's this
number here
which is here
so that's basically the probability
assigned by the neural net to the actual
correct character
you see that the network currently
thinks that this next character that e
following dot is only one percent likely
which is of course not very good right
because this actually is a training
example and the network thinks this is
currently very very unlikely but that's
just because we didn't get very lucky in
generating a good setting of w so right
now this network things it says unlikely
and 0.01 is not a good outcome
so the log likelihood then is very
negative
and the negative log likelihood is very
positive
and so four is a very high negative log
likelihood and that means we're going to
have a high loss
because what is the loss the loss is
just the average negative log likelihood
so the second character is em
and you see here that also the network
thought that m following e is very
unlikely one percent
the for m following m i thought it was
two percent
and for a following m it actually
thought it was seven percent likely so
just by chance this one actually has a
pretty good probability and therefore
pretty low negative log likelihood
and finally here it thought this was one
percent likely
so overall our average negative log
likelihood which is the loss the total
loss that summarizes
basically the how well this network
currently works at least on this one
word not on the full data suggested one
word is 3.76 which is actually very
fairly high loss this is not a very good
setting of w's
now here's what we can do
we're currently getting 3.76
we can actually come here and we can
change our w we can resample it so let
me just add one to have a different seed
and then we get a different w
and then we can rerun this
and with this different c with this
different setting of w's we now get 3.37
so this is a much better w right and
that and it's better because the
probabilities just happen to come out
higher for the for the characters that
actually are next
and so you can imagine actually just
resampling this you know we can try two
so
okay this was not very good
let's try one more
we can try three
okay this was terrible setting because
we have a very high loss
so anyway i'm going to erase this
what i'm doing here which is just guess
and check of randomly assigning
parameters and seeing if the network is
good that is uh amateur hour that's not
how you optimize a neural net the way
you optimize your neural net is you
start with some random guess and we're
going to commit to this one even though
it's not very good
but now the big deal is we have a loss
function
so this loss
is made up only of differentiable
operations and we can minimize the loss
by tuning
ws
by computing the gradients of the loss
with respect to
these w matrices
and so then we can tune w to minimize
the loss and find a good setting of w
using gradient based optimization so
let's see how that will work now things
are actually going to look almost
identical to what we had with micrograd
so here
i pulled up the lecture from micrograd
the notebook it's from this repository
and when i scroll all the way to the end
where we left off with micrograd we had
something very very similar
we had
a number of input examples in this case
we had four input examples inside axis
and we had their targets these are
targets
just like here we have our axes now but
we have five of them and they're now
integers instead of vectors
but we're going to convert our integers
to vectors except our vectors will be 27
large instead of three large
and then here what we did is first we
did a forward pass where we ran a neural
net on all of the inputs
to get predictions
our neural net at the time this nfx was
a multi-layer perceptron
our neural net is going to look
different because our neural net is just
a single layer
single linear layer followed by a soft
max
so that's our neural net
and the loss here was the mean squared
error so we simply subtracted the
prediction from the ground truth and
squared it and summed it all up and that
was the loss and loss was the single
number that summarized the quality of
the neural net and when loss is low like
almost zero that means the neural net is
predicting correctly
so we had a single number that uh that
summarized the uh the performance of the
neural net and everything here was
differentiable and was stored in massive
compute graph
and then we iterated over all the
parameters we made sure that the
gradients are set to zero and we called
lost up backward
and lasted backward initiated back
propagation at the final output node of
loss
right so
yeah remember these expressions we had
loss all the way at the end we start
back propagation and we went all the way
back
and we made sure that we populated all
the parameters dot grad
so that graph started at zero but back
propagation filled it in
and then in the update we iterated over
all the parameters and we simply did a
parameter update where every single
element of our parameters was nudged in
the opposite direction of the gradient
and so we're going to do the exact same
thing here
so i'm going to pull this up
on the side here
so that we have it available and we're
actually going to do the exact same
thing so this was the forward pass so
where we did this
and probs is our wipe red so now we have
to evaluate the loss but we're not using
the mean squared error we're using the
negative log likelihood because we are
doing classification we're not doing
regression as it's called
so here we want to calculate loss
now the way we calculate it is it's just
this average negative log likelihood
now this probs here
has a shape of 5 by 27
and so to get all the we basically want
to pluck out the probabilities at the
correct indices here
so in particular because the labels are
stored here in array wise
basically what we're after is for the
first example we're looking at
probability of five right at index five
for the second example
at the the second row or row index one
we are interested in the probability
assigned to index 13.
at the second example we also have 13.
at the third row we want one
and then the last row which is four we
want zero so these are the probabilities
we're interested in right
and you can see that they're not amazing
as we saw above
so these are the probabilities we want
but we want like a more efficient way to
access these probabilities
not just listing them out in a tuple
like this so it turns out that the way
to do this in pytorch uh one of the ways
at least is we can basically pass in
all of these
sorry about that all of these um
integers in the vectors
so
the
these ones you see how they're just 0 1
2 3 4
we can actually create that using mp
not mp sorry torch dot range of 5
0 1 2 3 4.
so we can index here with torch.range of
5
and here we index with ys
and you see that that gives us
exactly these numbers
so that plucks out the probabilities of
that the neural network assigns to the
correct next character
now we take those probabilities and we
don't we actually look at the log
probability so we want to dot log
and then we want to just
average that up so take the mean of all
of that
and then it's the negative
average log likelihood that is the loss
so the loss here is 3.7 something and
you see that this loss 3.76 3.76 is
exactly as we've obtained before but
this is a vectorized form of that
expression
so
we get the same loss
and the same loss we can consider
service part of this forward pass
and we've achieved here now loss
okay so we made our way all the way to
loss we've defined the forward pass
we forwarded the network and the loss
now we're ready to do the backward pass
so backward pass
we want to first make sure that all the
gradients are reset so they're at zero
now in pytorch you can set the gradients
to be zero but you can also just set it
to none and setting it to none is more
efficient and pi torch will interpret
none as like a lack of a gradient and is
the same as zeros
so this is a way to set to zero the
gradient
and now we do lost it backward
before we do lost that backward we need
one more thing if you remember from
micrograd
pytorch actually requires
that we pass in requires grad is true
so that when we tell
pythorge that we are interested in
calculating gradients for this leaf
tensor by default this is false
so let me recalculate with that
and then set to none and lost that
backward
now something magical happened when
lasted backward was run
because pytorch just like micrograd when
we did the forward pass here
it keeps track of all the operations
under the hood it builds a full
computational graph just like the graphs
we've
produced in micrograd those graphs exist
inside pi torch
and so it knows all the dependencies and
all the mathematical operations of
everything
and when you then calculate the loss
we can call a dot backward on it
and that backward then fills in the
gradients of
all the intermediates
all the way back to w's which are the
parameters of our neural net so now we
can do w grad and we see that it has
structure there's stuff inside it
and these gradients
every single element here
so w dot shape is 27 by 27
w grad shape is the same 27 by 27
and every element of w that grad
is telling us
the influence of that weight on the loss
function
so for example this number all the way
here
if this element the zero zero element of
w
because the gradient is positive is
telling us that this has a positive
influence in the loss slightly nudging
w
slightly taking w 0 0
and
adding a small h to it
would increase the loss
mildly because this gradient is positive
some of these gradients are also
negative
so that's telling us about the gradient
information and we can use this gradient
information to update the weights of
this neural network so let's now do the
update it's going to be very similar to
what we had in micrograd we need no loop
over all the parameters because we only
have one parameter uh tensor and that is
w
so we simply do w dot data plus equals
uh the
we can actually copy this almost exactly
negative 0.1 times w dot grad
and that would be the update to the
tensor
so that updates
the tensor
and
because the tensor is updated we would
expect that now the loss should decrease
so
here if i print loss
that item
it was 3.76 right
so we've updated the w here so if i
recalculate forward pass
loss now should be slightly lower so
3.76 goes to
3.74
and then
we can again set to set grad to none and
backward
update
and now the parameters changed again
so if we recalculate the forward pass we
expect a lower loss again 3.72
okay and this is again doing the we're
now doing gradient descent
and when we achieve a low loss that will
mean that the network is assigning high
probabilities to the correctness
characters okay so i rearranged
everything and i put it all together
from scratch
so here is where we construct our data
set of bigrams
you see that we are still iterating only
on the first word emma
i'm going to change that in a second i
added a number that counts the number of
elements in x's so that we explicitly
see that number of examples is five
because currently we're just working
with emma and there's five backgrounds
there
and here i added a loop of exactly what
we had before so we had 10 iterations of
grainy descent of forward pass backward
pass and an update
and so running these two cells
initialization and gradient descent
gives us some improvement
on
the loss function
but now i want to use all the words
and there's not 5 but 228 000 bigrams
now
however this should require no
modification whatsoever everything
should just run because all the code we
wrote doesn't care if there's five
migrants or 228 000 bigrams and with
everything we should just work so
you see that this will just run
but now we are optimizing over the
entire training set of all the bigrams
and you see now that we are decreasing
very slightly so actually we can
probably afford a larger learning rate
and probably for even larger learning
rate
even 50 seems to work on this very very
simple example right so let me
re-initialize and let's run 100
iterations
see what happens
okay
we seem to be
coming up to some pretty good losses
here 2.47
let me run 100 more
what is the number that we expect by the
way in the loss we expect to get
something around what we had originally
actually
so all the way back if you remember in
the beginning of this video when we
optimized uh just by counting
our loss was roughly 2.47
after we had it smoothing
but before smoothing we had roughly 2.45
likelihood
sorry loss
and so that's actually roughly the
vicinity of what we expect to achieve
but before we achieved it by counting
and here we are achieving the roughly
the same result but with gradient based
optimization
so we come to about 2.4
6 2.45 etc
and that makes sense because
fundamentally we're not taking any
additional information we're still just
taking in the previous character and
trying to predict the next one but
instead of doing it explicitly by
counting and normalizing
we are doing it with gradient-based
learning and it just so happens that the
explicit approach happens to very well
optimize the loss function without any
need for a gradient based optimization
because the setup for bigram language
models are is so straightforward that's
so simple we can just afford to estimate
those probabilities directly and
maintain them
in a table
but the gradient-based approach is
significantly more flexible
so we've actually gained a lot
because
what we can do now is
we can expand this approach and
complexify the neural net so currently
we're just taking a single character and
feeding into a neural net and the neural
that's extremely simple but we're about
to iterate on this substantially we're
going to be taking multiple previous
characters and we're going to be feeding
feeding them into increasingly more
complex neural nets but fundamentally
out the output of the neural net will
always just be logics
and those logits will go through the
exact same transformation we are going
to take them through a soft max
calculate the loss function and the
negative log likelihood and do gradient
based optimization and so actually
as we complexify the neural nets and
work all the way up to transformers
none of this will really fundamentally
change none of this will fundamentally
change the only thing that will change
is
the way we do the forward pass where we
take in some previous characters and
calculate logits for the next character
in the sequence that will become more
complex
and uh but we'll use the same machinery
to optimize it
and um
it's not obvious how we would have
extended
this bigram approach
into the case where there are many more
characters at the input because
eventually these tables would get way
too large because there's way too many
combinations of what previous characters
could be
if you only have one previous character
we can just keep everything in a table
that counts but if you have the last 10
characters that are input we can't
actually keep everything in the table
anymore so this is fundamentally an
unscalable approach and the neural
network approach is significantly more
scalable and it's something that
actually we can improve on over time so
that's where we will be digging next i
wanted to point out two more things
number one
i want you to notice that
this
x ink here
this is made up of one hot vectors and
then those one hot vectors are
multiplied by this w matrix
and we think of this as multiple neurons
being forwarded in a fully connected
manner
but actually what's happening here is
that for example
if you have a one hot vector here that
has a one at say the fifth dimension
then because of the way the matrix
multiplication works
multiplying that one-half vector with w
actually ends up plucking out the fifth
row of w
log logits would become just the fifth
row of w
and that's because of the way the matrix
multiplication works
um
so
that's actually what ends up happening
so but that's actually exactly what
happened before
because remember all the way up here
we have a bigram we took the first
character and then that first character
indexed into a row of this array here
and that row gave us the probability
distribution for the next character so
the first character was used as a lookup
into a
matrix here to get the probability
distribution
well that's actually exactly what's
happening here because we're taking the
index we're encoding it as one hot and
multiplying it by w
so logics literally becomes
the
the appropriate row of w
and that gets just as before
exponentiated to create the counts
and then normalized and becomes
probability
so this w here
is literally
the same as this array here
but w remember is the log counts not the
counts so it's more precise to say that
w exponentiated
w dot x is this array
but this array was filled in by counting
and by
basically
populating the counts of bi-grams
whereas in the gradient-based framework
we initialize it randomly and then we
let the loss
guide us
to arrive at the exact same array
so this array exactly here
is
basically the array w at the end of
optimization except we arrived at it
piece by piece by following the loss
and that's why we also obtain the same
loss function at the end and the second
note is if i come here
remember the smoothing where we added
fake counts to our counts
in order to
smooth out and make more uniform the
distributions of these probabilities
and that prevented us from assigning
zero probability to
to any one bigram
now if i increase the count here
what's happening to the probability
as i increase the count probability
becomes more and more uniform
right because these counts go only up to
like 900 or whatever so if i'm adding
plus a million to every single number
here you can see how
the row and its probability then when we
divide is just going to become more and
more close to exactly even probability
uniform distribution
it turns out that the gradient based
framework has an equivalent to smoothing
in particular
think through these w's here
which we initialized randomly
we could also think about initializing
w's to be zero
if all the entries of w are zero
then you'll see that logits will become
all zero
and then exponentiating those logics
becomes all one
and then the probabilities turned out to
be exactly uniform
so basically when w's are all equal to
each other or say especially zero
then the probabilities come out
completely uniform
so
trying to incentivize w to be near zero
is basically equivalent to
label smoothing and the more you
incentivize that in the loss function
the more smooth distribution you're
going to achieve
so this brings us to something that's
called
regularization where we can actually
augment the loss function to have a
small component that we call a
regularization loss
in particular what we're going to do is
we can take w and we can for example
square all of its entries
and then we can um whoops
sorry about that
we can take all the entries of w and we
can sum them
and because we're squaring uh there will
be no signs anymore um
negatives and positives all get squashed
to be positive numbers
and then the way this works is you
achieve zero loss if w is exactly or
zero but if w has non-zero numbers you
accumulate loss
and so we can actually take this and we
can add it on here
so we can do something like loss plus
w square
dot sum
or let's actually instead of sum let's
take a mean because otherwise the sum
gets too large
so mean is like a little bit more
manageable
and then we have a regularization loss
here say 0.01 times
or something like that you can choose
the regularization strength
and then we can just optimize this and
now this optimization actually has two
components not only is it trying to make
all the probabilities work out but in
addition to that there's an additional
component that simultaneously tries to
make all w's be zero because if w's are
non-zero you feel a loss and so
minimizing this the only way to achieve
that is for w to be zero
and so you can think of this as adding
like a spring force or like a gravity
force that that pushes w to be zero so w
wants to be zero and the probabilities
want to be uniform but they also
simultaneously want to match up your
your probabilities as indicated by the
data
and so the strength of this
regularization is exactly controlling
the amount of counts
that you add here
adding a lot more counts
here
corresponds to
increasing this number
because the more you increase it the
more this part of the loss function
dominates this part and the more these
these weights will un be unable to grow
because as they grow
they uh accumulate way too much loss
and so if this is strong enough
then we are not able to overcome the
force of this loss and we will never
and basically everything will be uniform
predictions
so i thought that's kind of cool okay
and lastly before we wrap up
i wanted to show you how you would
sample from this neural net model
and i copy-pasted the sampling code from
before
where remember that we sampled five
times
and all we did we start at zero we
grabbed the current ix row of p and that
was our probability row
from which we sampled the next index and
just accumulated that and
break when zero
and running this
gave us these
results still have the
p in memory so this is fine
now
the speed doesn't come from the row of b
instead it comes from this neural net
first we take ix
and we encode it into a one hot row of x
inc
this x inc multiplies rw
which really just plucks out the row of
w corresponding to ix really that's
what's happening
and that gets our logits and then we
normalize those low jets
exponentiate to get counts and then
normalize to get uh the distribution and
then we can sample from the distribution
so if i run this
kind of anticlimactic or climatic
depending how you look at it but we get
the exact same result
um
and that's because this is in the
identical model not only does it achieve
the same loss
but
as i mentioned these are identical
models and this w is the log counts of
what we've estimated before but we came
to this answer in a very different way
and it's got a very different
interpretation but fundamentally this is
basically the same model and gives the
same samples here and so
that's kind of cool okay so we've
actually covered a lot of ground we
introduced the bigram character level
language model
we saw how we can train the model how we
can sample from the model and how we can
evaluate the quality of the model using
the negative log likelihood loss
and then we actually trained the model
in two completely different ways that
actually get the same result and the
same model
in the first way we just counted up the
frequency of all the bigrams and
normalized
in a second way we used the
negative log likelihood loss as a guide
to optimizing the counts matrix
or the counts array so that the loss is
minimized in the in a gradient-based
framework and we saw that both of them
give the same result
and
that's it
now the second one of these the
gradient-based framework is much more
flexible and right now our neural
network is super simple we're taking a
single previous character and we're
taking it through a single linear layer
to calculate the logits
this is about to complexify so in the
follow-up videos we're going to be
taking more and more of these characters
and we're going to be feeding them into
a neural net but this neural net will
still output the exact same thing the
neural net will output logits
and these logits will still be
normalized in the exact same way and all
the loss and everything else and the
gradient gradient-based framework
everything stays identical it's just
that this neural net will now complexify
all the way to transformers
so that's gonna be pretty awesome and
i'm looking forward to it for now byehi everyone
today we are continuing our
implementation of makemore
now in the last lecture we implemented
the bigram language model and we
implemented it both using counts and
also using a super simple neural network
that had a single linear layer
now this is the
jupyter notebook that we built out last
lecture
and we saw that the way we approached
this is that we looked at only the
single previous character and we
predicted the distribution for the
character that would go next in the
sequence and we did that by taking
counts and normalizing them into
probabilities
so that each row here sums to one
now this is all well and good if you
only have one character of previous
context
and this works and it's approachable the
problem with this model of course is
that
the predictions from this model are not
very good because you only take one
character of context so the model didn't
produce very name like sounding things
now the problem with this approach
though is that if we are to take more
context into account when predicting the
next character in a sequence things
quickly blow up and this table the size
of this table grows and in fact it grows
exponentially with the length of the
context
because if we only take a single
character at a time that's 27
possibilities of context
but if we take two characters in the
past and try to predict the third one
suddenly the number of rows in this
matrix you can look at it that way
is 27 times 27 so there's 729
possibilities for what could have come
in the context
if we take three characters as the
context suddenly we have
20 000 possibilities of context
and so there's just way too many rows of
this matrix it's way too few counts
for each possibility and the whole thing
just kind of explodes and doesn't work
very well
so that's why today we're going to move
on to this bullet point here and we're
going to implement a multi-layer
perceptron model to predict the next uh
character in a sequence
and this modeling approach that we're
going to adopt follows this paper
benguetal 2003
so i have the paper pulled up here
now this isn't the very first paper that
proposed the use of multiglio
perceptrons or neural networks to
predict the next character or token in a
sequence but it's definitely one that is
uh was very influential around that time
it is very often cited to stand in for
this idea and i think it's a very nice
write-up and so this is the paper that
we're going to first look at and then
implement now this paper has 19 pages so
we don't have time to go into
the full detail of this paper but i
invite you to read it
it's very readable interesting and has a
lot of interesting ideas in it as well
in the introduction they describe the
exact same problem i just described and
then to address it they propose the
following model
now keep in mind that we are building a
character level language model so we're
working on the level of characters in
this paper they have a vocabulary of 17
000 possible words and they instead
build a word level language model but
we're going to still stick with the
characters but we'll take the same
modeling approach
now what they do is basically they
propose to take every one of these words
seventeen thousand words and they're
going to associate to each word a say
thirty dimensional feature vector
so every word is now
embedded into a thirty dimensional space
you can think of it that way so we have
17 000 points or vectors in a 30
dimensional space
and that's um you might imagine that's
very crowded that's a lot of points for
a very small space
now
in the beginning these words are
initialized completely randomly so
they're spread out at random
but then we're going to tune these
embeddings of these words using back
propagation
so during the course of training of this
neural network these points or vectors
are going to basically move around in
this space and you might imagine that
for example words that have very similar
meanings or that are indeed synonyms of
each other might end up in a very
similar part of the space and conversely
words that mean very different things
would go somewhere else in a space
now their modeling approach otherwise is
identical to ours they are using a
multi-layer neural network to predict
the next word given the previous words
and to train the neural network they are
maximizing the log likelihood of the
training data just like we did
so the modeling approach itself is
identical now here they have a concrete
example of this intuition
why does it work
basically suppose that for example you
are trying to predict a dog was running
in a blank
now suppose that the exact phrase a dog
was running in a
has never occurred in a training data
and here you are at sort of test time
later when the model is deployed
somewhere
and it's trying to make a sentence and
it's saying a dog was running in a blank
and because it's never encountered this
exact phrase in the training set you're
out of distribution as we say like you
don't have fundamentally any
reason to suspect
what might come next
but this approach actually allows you to
get around that because maybe you didn't
see the exact phrase a dog was running
in a something but maybe you've seen
similar phrases maybe you've seen the
phrase the dog was running in a blank
and maybe your network has learned that
a and the
are like frequently are interchangeable
with each other and so maybe it took the
embedding for a and the embedding for
the and it actually put them like nearby
each other in the space and so you can
transfer knowledge through that
embedding and you can generalize in that
way
similarly the network could know that
cats and dogs are animals and they
co-occur in lots of very similar
contexts and so even though you haven't
seen this exact phrase
or if you haven't seen exactly walking
or running
you can through the embedding space
transfer knowledge and you can
generalize to novel scenarios
so let's now scroll down to the diagram
of the neural network
they have a nice diagram here
and in this example we are taking three
previous words
and we are trying to predict the fourth
word
in a sequence
now these three previous words as i
mentioned uh we have a vocabulary of 17
000 um possible words
so every one of these
basically basically
are the index of the incoming word
and because there are 17 000 words this
is an integer between 0 and 16999
now there's also a lookup table that
they call c
this lookup table is a matrix that is 17
000 by say 30.
and basically what we're doing here is
we're treating this as a lookup table
and so every index is
plucking out a row of this embedding
matrix
so that each index is converted to the
30 dimensional vector that corresponds
to the embedding vector for that word
so here we have the input layer of 30
neurons for three words making up 90
neurons in total
and here they're saying that this matrix
c is shared across all the words so
we're always indexing into the same
matrix c over and over um
for each one of these
words next up is the hidden layer of
this neural network the size of this
hidden neural layer of this neural net
is a hoppy parameter so we use the word
hyperparameter when it's kind of like a
design choice up to the designer of the
neural net and this can be as large as
you'd like or as small as you'd like so
for example the size could be a hundred
and we are going to go over multiple
choices of the size of this hidden layer
and we're going to evaluate how well
they work
so say there were 100 neurons here all
of them would be fully connected to the
90 words or 90 um
numbers that make up these three words
so this is a fully connected layer
then there's a 10 inch long linearity
and then there's this output layer and
because there are 17 000 possible words
that could come next
this layer has 17 000 neurons
and all of them are fully connected to
all of these neurons in the hidden layer
so there's a lot of parameters here
because there's a lot of words so most
computation is here this is the
expensive layer
now there are 17 000 logits here so on
top of there we have the softmax layer
which we've seen in our previous video
as well so every one of these logits is
exponentiated and then everything is
normalized to sum to 1 so that we have a
nice probability distribution for the
next word in the sequence
now of course during training we
actually have the label we have the
identity of the next word in a sequence
that word
or its index is used to pluck out the
probability of that word
and then we are maximizing the
probability of that word
with respect to the parameters of this
neural net
so the parameters are the weights and
biases of this output layer
the weights and biases of the hidden
layer and the embedding lookup table c
and all of that is optimized using back
propagation
and these uh dashed arrows ignore those
uh that represents a variation of a
neural nut that we are not going to
explore in this video
so that's the setup and now let's
implement it
okay so i started a brand new notebook
for this lecture
we are importing pytorch and we are
importing matplotlib so we can create
figures
then i am reading all the names into a
list of words like i did before and i'm
showing the first eight right here
keep in mind that we have a 32 000 in
total these are just the first eight
and then here i'm building out the
vocabulary of characters and all the
mappings from the characters as strings
to integers and vice versa
now the first thing we want to do is we
want to compile the data set for the
neural network
and i had to rewrite this code um i'll
show you in a second what it looks like
so this is the code that i created for
the dataset creation so let me first run
it and then i'll briefly explain how
this works
so first we're going to define something
called block size and this is basically
the context length of how many
characters do we take to predict the
next one so here in this example we're
taking three characters to predict the
fourth one so we have a block size of
three that's the size of the block that
supports the prediction
then here i'm building out the x and y
the x are the input to the neural net
and the y are the labels for each
example inside x
then i'm airing over the first five
words i'm doing first five just for
efficiency while we are developing all
the code but then later we're going to
come here and erase this so that we use
the entire training set
so here i'm printing the word
emma and here i'm basically showing the
examples that we can generate the five
examples that we can generate out of the
single
um sort of word emma
so
when we are given the context of just uh
dot dot the first character in a
sequence is e
in this context the label is m
when the context is this the label is m
and so forth and so the way i build this
out is first i start with a padded
context of just zero tokens
then i iterate over all the characters i
get the character in the sequence and i
basically build out the array y of this
current character and the array x which
stores the current running context
and then here see i print everything and
here i um crop the context and enter the
new character in a sequence so this is
kind of like a rolling window of context
now we can change the block size here to
for example four
and in that case we'll be predicting the
fifth character given the previous four
or it can be five
and then it would look like this
or it can be say ten
and then it would look something like
this we're taking ten characters to
predict the eleventh one
and we're always padding with dots
so let me bring this back to three
just so that we have what we have here
in the paper
and finally the data set right now looks
as follows
from these five words we have created a
data set of 32 examples
and each input of the neural net is
three integers and we have a label that
is also an integer
y
so x looks like this
these are the individual examples
and then y are the labels
so
given this
let's now write a neural network that
takes these axes and predicts the y's
first let's build the embedding lookup
table c
so we have 27 possible characters and
we're going to embed them in a lower
dimensional space
in the paper they have 17 000 words and
they bet them in uh spaces as small
dimensional as 30. so they cram 17 000
words into 30 dimensional space in our
case we have only 27 possible characters
so let's grab them in something as small
as to start with for example a
two-dimensional space
so this lookup table will be random
numbers
and we'll have 27 rows and we'll have
two columns
right so each 20 each one of 27
characters will have a two-dimensional
embedding
so that's our matrix c of embeddings in
the beginning initialized randomly
now before we embed all of the integers
inside the input x using this lookup
table c
let me actually just try to embed a
single individual integer like say five
so we get a sense of how this works
now
one way this works of course is we can
just take the c and we can index into
row five
and that gives us a vector the fifth row
of c
and um
this is one way to do it
the other way that i presented in the
previous lecture is actually seemingly
different but actually identical so in
the previous lecture what we did is we
took these integers and we used the one
hot encoding to first encode them so f.1
hot
we want to encode integer 5 and we want
to tell it that the number of classes is
27 so that's the 26 dimensional vector
of all zeros except the fifth bit is
turned on
now this actually doesn't work
the reason is that
this input actually must be a doorstop
tensor
and i'm making some of these errors
intentionally just so you get to see
some errors and how to fix them
so this must be a tester not an int
fairly straightforward to fix we get a
one hot vector the fifth dimension is
one and the shape of this is 27.
and now notice that just as i briefly
alluded to in the previous video if we
take this one hot vector and we multiply
it by c
then
what would you expect
well number one
first you'd expect an error
because
expected scalar type long but found
float
so a little bit confusing but
the problem here is that one hot the
data type of it
is
long it's a 64-bit integer but this is a
float tensor and so pytorch doesn't know
how to multiply an int with a float and
that's why we had to explicitly cast
this to a float so that we can multiply
now the output actually here
is identical
and that it's identical because of the
way the matrix multiplication here works
we have the one hot um vector
multiplying columns of c
and because of all the zeros they
actually end up masking out everything
in c except for the fifth row which is
plucked out
and so we actually arrive at the same
result
and that tells you that
here we can interpret this first
piece here this embedding of the integer
we can either think of it as the integer
indexing into a lookup table c but
equivalently we can also think of this
little piece here as a first layer of
this bigger neural net
this layer here has neurons that have no
non-linearity there's no 10h they're
just linear neurons and their weight
matrix is c
and then we are encoding integers into
one hot and feeding those into a neural
net and this first layer basically
embeds them
so those are two equivalent ways of
doing the same thing we're just going to
index because it's much much faster and
we're going to discard this
interpretation of one hot inputs into
neural nets and we're just going to
index integers and create and use
embedding tables now embedding a single
integer like 5 is easy enough we can
simply ask pytorch to retrieve the fifth
row of c
or the row index five of c
but how do we simultaneously embed all
of these 32 by three integers stored in
array x
luckily pytorch indexing is fairly
flexible and quite powerful
so it doesn't just work to
ask for a single element five like this
you can actually index using lists so
for example we can get the rows five six
and seven
and this will just work like this we can
index with a list
it doesn't just have to be a list it can
also be a actually a tensor of integers
and we can index with that
so this is a integer tensor 567 and this
will just work as well
in fact we can also for example repeat
row 7 and retrieve it multiple times
and
that same index will just get embedded
multiple times here
so here we are indexing with a
one-dimensional
tensor of integers but it turns out that
you can also index with
multi-dimensional tensors of integers
here we have a two-dimensional in tensor
of integers so we can simply just do c
at x and this just works
and the shape of this
is
32 by 3 which is the original shape and
now for every one of those 32 by 3
integers we've retrieved the embedding
vector
here so basically we have that as an
example
the 13th or example index 13
the second dimension is the integer 1 as
an example
and so
here if we do c of x which gives us that
array and then we index into 13 by two
of that array
then we we get the embedding
here
and you can verify that
c
at one which is the integer at that
location is indeed equal to this
you see they're equal
so basically long story short pytorch
indexing is awesome and to embed
simultaneously all of the integers in x
we can simply do c of x
and that is our embedding
and that just works
now let's construct this layer here the
hidden layer
so we have that w1 as i'll call it are
these weights which we will initialize
randomly
now the number of inputs to this layer
is going to be
three times two right because we have
two dimensional embeddings and we have
three of them
so the number of inputs is 6
and the number of neurons in this layer
is a variable up to us
let's use 100 neurons as an example
and then biases
will be also initialized randomly as an
example
and let's and we just need 100 of them
now the problem with this is we can't
simply normally we would take the input
in this case that's embedding and we'd
like to multiply it with these weights
and then we would like to add the bias
this is roughly what we want to do
but the problem here is that these
embeddings are stacked up in the
dimensions of this input tensor
so this will not work this matrix
multiplication because this is a shape
32 by 3 by 2 and i can't multiply that
by 6 by 100
so somehow we need to concatenate these
inputs here together so that we can do
something along these lines which
currently does not work
so how do we transform this 32 by 3 by 2
into a 32 by 6 so that we can actually
perform
this
multiplication over here i'd like to
show you that there are usually many
ways of
implementing what you'd like to do in
torch and some of them will be faster
better shorter etc
and that's because torch is a very large
library and it's got lots and lots of
functions so if you just go to the
documentation and click on torch you'll
see that my slider here is very tiny and
that's because there are so many
functions that you can call on these
tensors
to transform them create them multiply
them add them perform all kinds of
different operations on them
and so this is kind of like
the space of possibility if you will
now one of the things that you can do is
if we can control here ctrl f for
concatenate and we see that there's a
function
torque.cat short for concatenate
and this concatenates the given sequence
of tensors in a given dimension
and these sensors must have the same
shape etc so we can use the concatenate
operation to in a naive way concatenate
these three embeddings for each input
so in this case we have m of
amp of the shape and really what we want
to do is we want to retrieve these three
parts and concatenate them
so we want to grab all the examples
we want to grab
first the zeroth
index
and then all of
this
so this plucks out
the 32 by 2 embeddings of just the first
word here
and so basically we want this guy
we want the first dimension and we want
the second dimension
and these are the three pieces
individually
and then we want to treat this as a
sequence and we want to torch that cat
on that sequence so this is the list
tor.cat takes a
sequence
of tensors and then we have to tell it
along which dimension to concatenate
so in this case all these are 32 by 2
and we want to concatenate not across
dimension 0 by the cross dimension one
so passing in one
gives us a result
the shape of this is 32 by 6 exactly as
we'd like
so that basically took 32 and squashed
these by concatenating them into 32 by
6.
now this is kind of ugly because this
code would not generalize if we want to
later change the block size right now we
have three inputs
three words but what if we had five
then here we would have to change the
code because i'm indexing directly well
torch comes to rescue again because that
turns out to be a function called unbind
and it removes a tensor dimension
so it removes the tensor dimension
returns a tuple of all slices along a
given dimension
without it
so this is exactly what we need
and basically when we call torch dot
unbind
torch dot unbind
of m
and pass in dimension
1 index 1
this gives us a list of
a list of tensors exactly equivalent to
this
so running this
gives us a line
3
and it's exactly this list and so we can
call torch.cat on it
and along the first dimension
and this works
and this shape is the same
but now this is uh it doesn't matter if
we have block size 3 or 5 or 10 this
will just work
so this is one way to do it but it turns
out that in this case there's actually a
significantly better and more efficient
way and this gives me an opportunity to
hint at some of the internals of
torch.tensor
so let's create
an array here
of elements from 0 to 17
and the shape of this
is just 18. it's a single picture of 18
numbers
it turns out that we can very quickly
re-represent this as different sized and
dimensional tensors
we do this by calling a view
and we can say that actually this is not
a single vector of 18 this is a two by
nine tensor or alternatively this is a
nine by two tensor
or this is actually a three by three by
two tensor
as long as the total number of elements
here multiply to be the same
this will just work and
in pytorch this operation calling that
view is extremely efficient
and the reason for that is that
in each tensor there's something called
the underlying storage
and the storage is just the numbers
always as a one-dimensional vector and
this is how this tensor is represented
in the computer memory it's always a
one-dimensional vector
but when we call that view we are
manipulating some of attributes of that
tensor that dictate how this
one-dimensional sequence is interpreted
to be an n-dimensional tensor
and so what's happening here is that no
memory is being changed copied moved or
created when we call that view the
storage
is identical but when you call that view
some of the internal
attributes of the view of the sensor are
being manipulated and changed in
particular that's something there's
something called a storage offset
strides and shapes and those are
manipulated so that this one-dimensional
sequence of bytes is seen as different
and dimensional arrays
there's a blog post here from eric
called pi torch internals where he goes
into some of this with respect to tensor
and how the view of the tensor is
represented
and this is really just like a logical
construct of representing the physical
memory
and so this is a pretty good um blog
post that you can go into i might also
create an entire video on the internals
of torch tensor and how this works
for here we just note that this is an
extremely efficient operation
and if i delete this and come back to
our end
we see that the shape of our end is 32
by three by two but we can simply
ask for pytorch to view this instead as
a 32 by six
and the way this gets flattened into a
32 by six array
just happens that
these two
get stacked up
in a single row and so that's basically
the concatenation operation that we're
after
and you can verify that this actually
gives the exact same result as what we
had before
so this is an element y equals and you
can see that all the elements of these
two tensors are the same
and so we get the exact same result
so long story short we can actually just
come here
and if we just view this as a 32x6
instead then this multiplication will
work and give us the hidden states that
we're after
so if this is h
then h shape is now
the 100 dimensional activations for
every one of our 32 examples
and this gives the desired result let me
do two things here number one let's not
use 32 we can for example do something
like
m.shape at 0
so that we don't hard code these numbers
and this would work for any size of this
amp
or alternatively we can also do negative
one when we do negative one pi torch
will infer what this should be
because the number of elements must be
the same and we're saying that this is 6
by church will derive that this must be
32 or whatever else it is if m is of
different size
the other thing is here um
one more thing i'd like to point out is
here when we do the concatenation
this actually is much less efficient
because um this concatenation would
create a whole new tensor with a whole
new storage so new memory is being
created because there's no way to
concatenate tensors just by manipulating
the view attributes
so this is inefficient and creates all
kinds of new memory
uh so let me delete this now
we don't need this
and here to calculate h we want to also
dot 10h
of this
to get our
oops to get our h
so these are now numbers between
negative one and one because of the 10h
and we have
that the shape is 32 by 100
and that is basically this hidden layer
of activations here
for every one of our 32 examples
now there's one more thing i've lost
over that we have to be very careful
with and that this
and that's this plus here
in particular we want to make sure that
the broadcasting will do what we like
the shape of this is 32 by 100 and the
ones shape is 100.
so we see that the addition here will
broadcast these two and in particular we
have 32 by 100 broadcasting to 100.
so broadcasting will align on the right
create a fake dimension here so this
will become a 1 by 100 row vector and
then it will copy vertically
for every one of these rows of 32 and do
an element wise addition
so in this case the correct thing will
be happening because the same bias
vector will be added to all the rows
of
this matrix so that is correct that's
what we'd like and it's always good
practice you just make sure
so that you don't shoot yourself in the
foot and finally let's create the final
layer here
so let's create
w2 and v2
the input now is 100
and the output number of neurons will be
for us 27 because we have 27 possible
characters that come next
so the biases will be 27 as well
so therefore the logits which are the
outputs of this neural net
are going to be um
h
multiplied by w2 plus b2
logistic shape is 32 by 27
and the logits look
good now exactly as we saw in the
previous video we want to take these
logits and we want to first exponentiate
them to get our fake counts
and then we want to normalize them into
a probability
so prob is counts divide
and now
counts dot sum along the first dimension
and keep them as true exactly as in the
previous video
and so
prob that shape now is 32 by 27
and you'll see that every row of prob
sums to one so it's normalized
so that gives us the probabilities now
of course we have the actual letter that
comes next and that comes from this
array y
which we which we created during the
dataset creation so why is this last
piece here which is the identity of the
next character in the sequence that we'd
like to now predict
so what we'd like to do now is just as
in the previous video we'd like to index
into the rows of prob and in each row
we'd like to pluck out the probability
assigned to the correct character
as given here
so first we have torch.range of 32 which
is kind of like a iterator over
numbers from 0 to 31
and then we can index into prob in the
following way
prop in
torch.range of 32 which iterates the
roads and in each row we'd like to grab
this column as given by y
so this gives the current probabilities
as assigned by this neural network with
this setting of its weights
to the correct character in the sequence
and you can see here that this looks
okay for some of these characters like
this is basically 0.2
but it doesn't look very good at all for
many other characters like this is
0.0701 probability and so the network
thinks that some of these are extremely
unlikely but of course we haven't
trained the neural network yet so
this will improve and ideally all of
these numbers here of course are one
because then we are correctly predicting
the next character
now just as in the previous video we
want to take these probabilities we want
to look at the lock probability
and then we want to look at the average
probability
and the negative of it to create the
negative log likelihood loss
so the loss here is 17
and this is the loss that we'd like to
minimize to get the network to predict
the correct character in the sequence
okay so i rewrote everything here and
made it a bit more respectable so here's
our data set here's all the parameters
that we defined
i'm now using a generator to make it
reproducible
i clustered all the parameters into a
single list of parameters so that for
example it's easy to count them and see
that in total we currently have about
3400 parameters
and this is the forward pass as we
developed it
and we arrive at a single number here
the loss that is currently expressing
how well
this neural network works with the
current setting of parameters
now i would like to make it even more
respectable so in particular see these
lines here where we take the logits and
we calculate the loss
we're not actually reinventing the wheel
here this is just um
classification and many people use
classification and that's why there is a
functional.cross entropy function in
pytorch to calculate this much more
efficiently
so we can just simply call f.cross
entropy
and we can pass in the logits and we can
pass in the
array of targets y
and this calculates the exact same loss
so in fact we can simply put this here
and erase these three lines and we're
going to get the exact same result now
there are actually many good reasons to
prefer f.cross entropy over rolling your
own implementation like this i did this
for educational reasons but you'd never
use this in practice why is that
number one when you use f.cross entropy
by torch will not actually create all
these intermediate tensors because these
are all new tensors in memory and all
this is fairly inefficient to run like
this instead pytorch will cluster up all
these operations and very often create
have fused kernels that very efficiently
evaluate these expressions that are sort
of like clustered mathematical
operations
number two the backward pass can be made
much more efficient and not just because
it's a fused kernel but also
analytically and mathematically it's
much it's often a very much simpler
backward pass to implement
we actually sell this with micrograd
you see here when we implemented 10h the
forward pass of this operation to
calculate the 10h was actually a fairly
complicated mathematical expression
but because it's a clustered
mathematical expression when we did the
backward pass we didn't individually
backward through the x and the two times
and the minus one in division etc we
just said it's one minus t squared and
that's a much simpler mathematical
expression
and we were able to do this because
we're able to reuse calculations and
because we are able to mathematically
and analytically derive the derivative
and often that expression simplifies
mathematically and so there's much less
to implement
so not only can can it be made more
efficient because it runs in a fused
kernel but also because the expressions
can take a much simpler form
mathematically
so that's number one number two
under the hood f that cross entropy can
also be significantly more um
numerically well behaved let me show you
an example of how this works
suppose we have a logits of negative 2 3
negative 3 0 and 5
and then we are taking the exponent of
it and normalizing it to sum to 1.
so
when logits take on this values
everything is well and good and we get a
nice probability distribution
now consider what happens when some of
these logits take on more extreme values
and that can happen during optimization
of the neural network
suppose that some of these numbers grow
very negative like say negative 100
then actually everything will come out
fine we still get the probabilities that
um
you know are well behaved and they sum
to one and everything is great
but because of the way the x works if
you have very positive logits let's say
positive 100 in here
you actually start to run into trouble
and we get not a number here
and the reason for that is that these
counts
have an if here
so if you pass in a very negative number
to x you just get a very negative sorry
not negative but very small number very
very near zero and that's fine
but if you pass in a very positive
number suddenly we run out of range in
our floating point number that
represents these counts
so basically we're taking e and we're
raising it to the power of 100 and that
gives us if because we run out of
dynamic range on this floating point
number that is count
and so we cannot pass very large logits
through this expression
now let me reset these numbers to
something reasonable
the way pi torch solved this
is that
you see how we have a well-behaved
result here
it turns out that because of the
normalization here you can actually
offset logits by any arbitrary constant
value that you want so if i add 1 here
you actually get the exact same result
or if i add 2
or if i subtract three
any offset will produce the exact same
probabilities
so because negative numbers are okay but
positive numbers can actually overflow
this x what patrick does is it
internally calculates the maximum value
that occurs in the logits and it
subtracts it so in this case it would
subtract five
and so therefore the greatest number in
logits will become zero and all the
other numbers will become some negative
numbers
and then the result of this is always
well behaved so even if we have 100 here
previously
not good but because pytorch will
subtract 100 this will work
and so there's many good reasons to call
cross-entropy number one the forward
pass can be much more efficient the
backward pass can be much more efficient
and also things can be much more
numerically well behaved okay so let's
now set up the training of this neural
net
we have the forward pass
uh we don't need these
is that we have the losses equal to the
f.cross entropy that's the forward pass
then we need the backward pass first we
want to set the gradients to be zero so
for p in parameters
we want to make sure that p dot grad is
none which is the same as setting it to
zero in pi torch
and then lost that backward to populate
those gradients
once we have the gradients we can do the
parameter update so for p in parameters
we want to take all the
data and we want to nudge it
learning rate times p dot grad
and then we want to repeat this
a few times
and let's print the loss here as well
now this won't suffice and it will
create an error because we also have to
go for pn parameters
and we have to make sure that p dot
requires grad is set to true in pi torch
and this should just work
okay so we started off with loss of 17
and we're decreasing it
let's run longer
and you see how the loss decreases
a lot here so
if we just run for a thousand times
we get a very very low loss and that
means that we're making very good
predictions now the reason that this is
so straightforward right now
is because we're only um
overfitting 32 examples
so we only have 32 examples uh of the
first five words
and therefore it's very easy to make
this neural net fit only these two 32
examples because we have 3 400
parameters and only 32 examples so we're
doing what's called overfitting a single
batch of the data
and getting a very low loss and good
predictions
um but that's just because we have so
many parameters for so few examples so
it's easy to
uh make this be very low
now we're not able to achieve exactly
zero
and the reason for that is we can for
example look at logits which are being
predicted
and we can look at the max along the
first dimension
and in pi torch
max reports both the actual values that
take on the maximum number but also the
indices of piece
and you'll see that the indices are very
close to the labels
but in some cases they differ
for example in this very first example
the predicted index is 19 but the label
is five
and we're not able to make loss be zero
and fundamentally that's because here
the very first or the zeroth index is
the example where dot dot dot is
supposed to predict e but you see how
dot dot dot is also supposed to predict
an o and dot dot is also supposed to
predict an i and then s as well and so
basically e o a or s are all possible
outcomes in a training set for the exact
same input so we're not able to
completely over fit and um
and make the loss be exactly zero so but
we're getting very close in the cases
where
there's a unique input for a unique
output in those cases we do what's
called overfit and we basically get the
exact same and the exact correct result
so now all we have to do
is we just need to make sure that we
read in the full data set and optimize
the neural net
okay so let's swing back up
where we created the dataset
and we see that here we only use the
first five words so let me now erase
this
and let me erase the print statements
otherwise we'd be printing way too much
and so when we processed the full data
set of all the words we now had 228 000
examples instead of just 32.
so let's now scroll back down
to this is much larger reinitialize the
weights the same number of parameters
they all require gradients
and then let's push this print out
lost.item to be here
and let's just see how the optimization
goes if we run this
okay so we started with a fairly high
loss and then as we're optimizing the
loss is coming down
but you'll notice that it takes quite a
bit of time for every single iteration
so let's actually address that because
we're doing way too much work forwarding
and backwarding 220 000 examples
in practice what people usually do is
they perform forward and backward pass
and update on many batches of the data
so what we will want to do is we want to
randomly select some portion of the data
set and that's a mini batch and then
only forward backward and update on that
little mini batch and then
we iterate on those many batches
so in pytorch we can for example use
storage.randint
we can generate numbers between 0 and 5
and make 32 of them
i believe the size has to be a
tuple
in my torch
so we can have a tuple 32 of numbers
between zero and five but actually we
want x dot shape of zero here
and so this creates uh integers that
index into our data set and there's 32
of them so if our mini batch size is 32
then we can come here and we can first
do a mini batch
construct
so in the integers that we want to
optimize in this
single iteration
are in the ix
and then we want to index into
x
with ix to only grab those rows
so we're only getting 32 rows of x
and therefore embeddings will again be
32 by three by two not two hundred
thousand by three by two
and then this ix has to be used not just
to index into x
but also to index into y
and now this should be many batches and
this should be much much faster so
okay so it's instant almost
so this way we can run many many
examples
nearly instantly and decrease the loss
much much faster
now because we're only dealing with mini
batches the quality of our gradient is
lower so the direction is not as
reliable it's not the actual gradient
direction
but the gradient direction is good
enough even when it's estimating on only
32 examples that it is useful and so
it's much better to have an approximate
gradient and just make more steps than
it is to evaluate the exact gradient and
take fewer steps so that's why in
practice uh this works quite well
so let's now continue the optimization
let me take out this lost item from here
and uh
place it over here at the end
okay so we're hovering around 2.5 or so
however this is only the loss for that
mini batch so let's actually evaluate
the loss
here
for all of x
and for all of y just so we have a
full sense of exactly how all the model
is doing right now
so right now we're at about 2.7 on the
entire training set
so let's
run the optimization for a while
okay right 2.6
2.57
2.53
okay
so one issue of course is we don't know
if we're stepping too slow or too fast
so this point one i just guessed it
so one question is how do you determine
this learning rate
and how do we gain confidence that we're
stepping in the right
sort of speed so i'll show you one way
to determine a reasonable learning rate
it works as follows let's reset our
parameters
to the initial
settings
and now let's
print in every step
but let's only do 10 steps or so
or maybe maybe 100 steps
we want to find like a very reasonable
set
search range if you will so for example
if this is like very low
then
we see that the loss is barely
decreasing so that's not
that's like too low basically so let's
try
this one
okay so we're decreasing the loss but
like not very quickly so that's a pretty
good low range
now let's reset it again
and now let's try to find the place at
which the loss kind of explodes
uh so maybe at negative one
okay we see that we're minimizing the
loss but you see how uh it's kind of
unstable it goes up and down quite a bit
um so negative one is probably like a
fast learning rate let's try negative
10.
okay so this
isn't optimizing this is not working
very well so negative 10 is way too big
negative one was already kind of big
um
so therefore
negative one was like somewhat
reasonable if i reset
so i'm thinking that the right learning
rate is somewhere between
uh negative zero point zero zero one and
um
negative one
so the way we can do this here is we can
use uh torch shot lens space
and we want to basically do something
like this between zero and one but
um
those number of steps is one more
parameter that's required let's do a
thousand steps this creates 1000
numbers between 0.01 and 1
but it doesn't really make sense to step
between these linearly so instead let me
create learning rate exponent
and instead of 0.001 this will be a
negative 3 and this will be a zero
and then the actual lrs that we want to
search over are going to be 10 to the
power of lre
so now what we're doing is we're
stepping linearly between the exponents
of these learning rates this is 0.001
and this is 1 because 10 to the power of
0 is 1.
and therefore we are spaced
exponentially in this interval
so these are the candidate learning
rates
that we want to sort of like search over
roughly
so now what we're going to do is
here we are going to run the
optimization for 1000 steps
and instead of using a fixed number
we are going to use learning rate
indexing into here lrs of i
and make this i
so basically let me reset this to be
again starting from random
creating these learning rates between
negative
zero points between 0.001 and um
one but exponentially stopped
and here what we're doing is we're
iterating a thousand times
we're going to use the learning rate
um that's in the beginning very very low
in the beginning is going to be 0.001
but by the end it's going to be
1.
and then we're going to step with that
learning rate
and now what we want to do is we want to
keep track of the uh
learning rates that we used and we want
to look at the losses
that resulted
and so here let me
track stats
so lri.append lr
and um lost side that append
loss that item
okay
so again reset everything
and then run
and so basically we started with a very
low learning rate and we went all the
way up to a learning rate of negative
one
and now what we can do is we can plt
that plot
and we can plot the two so we can plot
the learning rates on the x-axis and the
losses we saw on the y-axis
and often you're going to find that your
plot looks something like this
where in the beginning
you had very low learning rates so
basically anything
barely anything happened
then we got to like a nice spot here
and then as we increase the learning
rate enough
we basically started to be kind of
unstable here
so a good learning rate turns out to be
somewhere around here
um and because we have lri here
um
we actually may want to
um
do not lr
not the learning rate but the exponent
so that would be the lre at i is maybe
what we want to log so let me reset this
and redo that calculation
but now on the x axis we have the
[Music]
exponent of the learning rate and so we
can see the exponent of the learning
rate that is good to use it would be
sort of like roughly in the valley here
because here the learning rates are just
way too low and then here where we
expect relatively good learning rates
somewhere here and then here things are
starting to explode so somewhere around
negative one x the exponent of the
learning rate is a pretty good setting
and 10 to the negative one is 0.1 so 0.1
is actually 0.1 was actually a fairly
good learning rate around here
and that's what we had in the initial
setting
but that's roughly how you would
determine it and so here now we can take
out the tracking of these
and we can just simply set lr to be 10
to the negative one or
basically otherwise 0.1 as it was before
and now we have some confidence that
this is actually a fairly good learning
rate
and so now we can do is we can crank up
the iterations
we can reset our optimization
and
we can run for a pretty long time using
this learning rate
oops and we don't want to print that's
way too much printing
so let me again reset
and run ten thousand stops
okay so we're 0.2 2.48 roughly let's run
another 10 000 steps
2.46
and now let's do one learning rate decay
what this means is we're going to take
our learning rate and we're going to 10x
lower it and so we're at the late stages
of training potentially and we may want
to go a bit slower let's do one more
actually at 0.1 just to see if
we're making a dent here
okay we're still making dent and by the
way the
bi-gram loss that we achieved last video
was 2.45 so we've already surpassed the
bi-gram model
and once i get a sense that this is
actually kind of starting to plateau off
people like to do as i mentioned this
learning rate decay so let's try to
decay the loss
the learning rate i mean
and we achieve it about 2.3 now
obviously this is janky and not exactly
how you would train it in production but
this is roughly what you're going
through you first find a decent learning
rate using the approach that i showed
you
then you start with that learning rate
and you train for a while
and then at the end people like to do a
learning rate decay where you decay the
learning rate by say a factor of 10 and
you do a few more steps and then you get
a trained network roughly speaking
so we've achieved 2.3 and dramatically
improved on the bi-gram language model
using this simple neural net as
described here
using these 3 400 parameters now there's
something we have to be careful with
i said that we have a better model
because we are achieving a lower loss
2.3 much lower than 2.45 with the
bi-gram model previously
now that's not exactly true and the
reason that's not true is that
this is actually fairly small model but
these models can get larger and larger
if you keep adding neurons and
parameters so you can imagine that we
don't potentially have a thousand
parameters we could have 10 000 or 100
000 or millions of parameters
and as the capacity of the neural
network grows
it becomes more and more capable of
overfitting your training set
what that means is that the loss on the
training set on the data that you're
training on will become very very low as
low as zero
but all that the model is doing is
memorizing your training set verbatim so
if you take that model and it looks like
it's working really well but you try to
sample from it you will basically only
get examples exactly as they are in the
training set you won't get any new data
in addition to that if you try to
evaluate the loss on some withheld names
or other words
you will actually see that the loss on
those can be very high and so basically
it's not a good model
so the standard in the field is to split
up your data set into three splits as we
call them we have the training split the
dev split or the validation split
and the test split
so
training split
test or um sorry dev or validation split
and test split and typically this would
be say eighty percent of your data set
this could be ten percent and this ten
percent roughly
so you have these three splits of the
data
now these eighty percent of your
trainings of the data set the training
set is used to optimize the parameters
of the model just like we're doing here
using gradient descent
these 10 percent of the
examples the dev or validation split
they're used for development over all
the hyper parameters of your model so
hyper parameters are for example the
size of this hidden layer
the size of the embedding so this is a
hundred or a two for us but we could try
different things
the strength of the regularization which
we aren't using yet so far
so there's lots of different hybrid
parameters and settings that go into
defining your neural net and you can try
many different variations of them and
see whichever one works best on your
validation split
so this is used to train the parameters
this is used to train the hyperprimers
and test split is used to evaluate
basically the performance of the model
at the end
so we're only evaluating the loss on the
test plate very very sparingly and very
few times because every single time you
evaluate your test loss and you learn
something from it
you are basically starting to also train
on the test split
so you are only allowed to test the loss
on a test
set
very very few times otherwise you risk
overfitting to it as well as you
experiment on your model
so let's also split up our training data
into train dev and test and then we are
going to train on train
and only evaluate on tests very very
sparingly okay so here we go
here is where we took all the words and
put them into x and y tensors
so instead let me create a new cell here
and let me just copy paste some code
here
because i don't think it's that
complex but
we're going to try to save a little bit
of time
i'm converting this to be a function now
and this function takes some list of
words and builds the arrays x and y for
those words only
and then here i am shuffling up all the
words so these are the input words that
we get
we are randomly shuffling them all up
and then um
we're going to
set n1 to be
the number of examples that there's 80
of the words and n2 to be
90
of the way of the words so basically if
len of words is 32 000 n1 is
well sorry i should probably run this
n1 is 25 000 and n2 is 28 000.
and so here we see that
i'm calling build data set to build the
training set x and y
by indexing into up to and one so we're
going to have only 25 000 training words
and then we're going to have
roughly
n2 minus n1
3 3 000 validation examples or dev
examples and we're going to have
when of words basically minus and two
or
3 204 examples
here for the test set
so
now we have x's and y's
for all those three splits
oh yeah i'm printing their size here
inside the function as well
but here we don't have words but these
are already the individual examples made
from those words
so let's now scroll down here
and the data set now for training is
more like this
and then when we reset the network
when we're training we're only going to
be training using x train
x train and y train
so that's the only thing we're training
on
let's see where we are on the
single batch
let's now train maybe a few more steps
training neural networks can take a
while usually you don't do it inline you
launch a bunch of jobs and you wait for
them to finish um can take in multiple
days and so on
luckily this is a very small network
okay so the loss is pretty good
oh we accidentally used a learning rate
that is way too low
so let me actually come back
we use the decay learning rate of 0.01
so this will train much faster
and then here when we evaluate
let's use the dep set here
xdev
and ydev to evaluate the loss
okay
and let's now decay the learning rate
and only do say 10 000 examples
and let's evaluate the dev loss
ones here
okay so we're getting about 2.3 on dev
and so the neural network when it was
training did not see these dev examples
it hasn't optimized on them and yet
when we evaluate the loss on these dev
we actually get a pretty decent loss
and so we can also look at what the
loss is on all of training set
oops
and so we see that the training and the
dev loss are about equal so we're not
over fitting
um this model is not powerful enough to
just be purely memorizing the data and
so far we are what's called underfitting
because the training loss and the dev or
test losses are roughly equal so what
that typically means is that our network
is very tiny very small and we expect to
make performance improvements by scaling
up the size of this neural net so let's
do that now so let's come over here
and let's increase the size of the
neural net the easiest way to do this is
we can come here to the hidden layer
which currently has 100 neurons and
let's just bump this up so let's do 300
neurons
and then this is also 300 biases and
here we have 300 inputs into the final
layer
so
let's initialize our neural net we now
have ten thousand ex ten thousand
parameters instead of three thousand
parameters
and then we're not using this
and then here what i'd like to do is i'd
like to actually uh keep track of uh
tap
um
okay let's just do this let's keep stats
again
and here when we're keeping track of the
loss let's just also keep track of the
steps and let's just have i here
and let's train on thirty thousand
or rather say
okay let's try thirty thousand
and we are at point one
and
we should be able to run this
and optimize the neural net
and then here basically i want to
plt.plot
the steps
against the loss
so these are the x's and y's
and this is
the loss function and how it's being
optimized
now you see that there's quite a bit of
thickness to this and that's because we
are optimizing over these mini batches
and the mini batches create a little bit
of noise
in this
uh where are we in the def set we are at
2.5 so we still haven't optimized this
neural net very well
and that's probably because we made it
bigger it might take longer for this
neural net to converge
um
and so let's continue training
um
yeah let's just continue training
one possibility is that the batch size
is so low
that uh we just have way too much noise
in the training and we may want to
increase the batch size so that we have
a bit more um correct gradient and we're
not thrashing too much and we can
actually like optimize more properly
okay
this will now become meaningless because
we've reinitialized these so
yeah this looks not
pleasing right now but there probably is
like a tiny improvement but it's so hard
to tell
let's go again
2.52
let's try to decrease the learning rate
by factor two
okay we're at 2.32
let's continue training
we basically expect to see a lower loss
than what we had before because now we
have a much much bigger model and we
were under fitting so we'd expect that
increasing the size of the model should
help the neural net
2.32 okay so that's not happening too
well
now one other concern is that even
though we've made the 10h layer here or
the hidden layer much much bigger it
could be that the bottleneck of the
network right now are these embeddings
that are two dimensional it can be that
we're just cramming way too many
characters into just two dimensions and
the neural net is not able to really use
that space effectively and that that is
sort of like the bottleneck to our
network's performance
okay 2.23 so just by decreasing the
learning rate i was able to make quite a
bit of progress let's run this one more
time
and then evaluate the training and the
dev loss
now one more thing after training that
i'd like to do is i'd like to visualize
the um
embedding vectors for these
characters before we scale up the
embedding size from two
because we'd like to make uh this
bottleneck potentially go away
but once i make this greater than two we
won't be able to visualize them
so here
okay we're at 2.23 and 2.24
so um we're not improving much more and
maybe the bottleneck now is the
character embedding size which is two
so here i have a bunch of code that will
create a figure
and then we're going to visualize
the embeddings that were trained by the
neural net
on these characters because right now
the embedding has just two so we can
visualize all the characters with the x
and the y coordinates as the two
embedding locations for each of these
characters
and so here are the x coordinates and
the y coordinates which are the columns
of c
and then for each one i also include the
text of the little character
so here what we see is actually kind of
interesting
the network has basically learned to
separate out the characters and cluster
them a little bit uh so for example you
see how the vowels
a e i o u are clustered up here
so that's telling us that is that the
neural net treats these is very similar
right because when they feed into the
neural net
the embedding uh for all these
characters is very similar and so the
neural net thinks that they're very
similar and kind of like interchangeable
if that makes sense
um
then the the points that are like really
far away are for example q q is kind of
treated as an exception and q has a very
special
embedding vector so to speak
similarly dot which is a special
character is all the way out here
and a lot of the other letters are sort
of like clustered up here and so it's
kind of interesting that there's a
little bit of structure here
after the training
and it's not definitely not random and
these embeddings make sense
so we're now going to scale up the
embedding size and won't be able to
visualize it directly but we expect that
because we're under fitting
and we made this layer much bigger and
did not sufficiently improve the loss
we're thinking that the um
constraint to better performance right
now could be these embedding pictures so
let's make them bigger okay so let's
scroll up here
and now we don't have two dimensional
embeddings we are going to have
say 10 dimensional embeddings for each
word
then
this layer will receive 3 times 10 so 30
inputs
will go into
the hidden layer
let's also make the hidden layer a bit
smaller so instead of 300 let's just do
200 neurons in that hidden layer
so now the total number of elements will
be slightly bigger at 11 000
and then here we have to be a bit
careful because um
okay the learning rate we set to 0.1
here we are hardcoded in six and
obviously if you're working in
production you don't wanna be
hard-coding magic numbers but instead of
six this should now be thirty
um
and let's run for fifty thousand
iterations and let me split out the
initialization here outside
so that when we run this cell multiple
times it's not going to wipe out
our loss
in addition to that
here
let's instead of logging lost.item let's
actually
log the
let's
do log 10
i believe that's a function of the loss
and i'll show you why in a second let's
optimize this
basically i'd like to plot the log loss
instead of the loss because when you
plot the loss many times it can have
this hockey stick appearance and log
squashes it in
uh so it just kind of like looks nicer
so the x-axis is step i
and the y-axis will be the loss i
and then here this is 30.
ideally we wouldn't be hard-coding these
okay so let's look at the loss
okay it's again very thick because the
mini batch size is very small but the
total loss over the training set is 2.3
and the the tests or the def set is 2.38
as well
so so far so good uh let's try to now
decrease the learning rate
by a factor of 10
and train for another 50 000 iterations
we'd hope that we would be able to beat
uh 2.32
but again we're just kind of like doing
this very haphazardly so i don't
actually have confidence that our
learning rate is set very well that our
learning rate decay which we just do
at random is set very well
and um so the optimization here is kind
of suspect to be honest and this is not
how you would do it typically in
production in production you would
create parameters or hyper parameters
out of all these settings and then you
would run lots of experiments and see
whichever ones are working well for you
okay
so we have 2.17 now and 2.2 okay so you
see how the training and the validation
performance are starting to slightly
slowly depart
so maybe we're getting the sense that
the neural net
is getting good enough or
that number of parameters is large
enough that we are slowly starting to
overfit
let's maybe run one more iteration of
this
and see where we get
but yeah basically you would be running
lots of experiments and then you are
slowly scrutinizing whichever ones give
you the best depth performance and then
once you find all the
hyper parameters that make your dev
performance good you take that model and
you evaluate the test set performance a
single time and that's the number that
you report in your paper or wherever
else you want to talk about and brag
about your model
so let's then rerun the plot and rerun
the train and death
and because we're getting lower loss now
it is the case that the embedding size
of these was holding us back very likely
okay so 2.162.19 is what we're roughly
getting
so there's many ways to go from many
ways to go from here we can continue
tuning the optimization
we can continue for example playing with
the sizes of the neural net or we can
increase the number of uh
words or characters in our case that we
are taking as an input so instead of
just three characters we could be taking
more characters as an input and that
could further improve the loss
okay so i changed the code slightly so
we have here 200 000 steps of the
optimization and in the first 100 000
we're using a learning rate of 0.1 and
then in the next 100 000 we're using a
learning rate of 0.01
this is the loss that i achieve
and these are the performance on the
training and validation loss
and in particular the best validation
loss i've been able to obtain in the
last 30 minutes or so is 2.17
so now i invite you to beat this number
and you have quite a few knobs available
to you to i think surpass this number
so number one you can of course change
the number of neurons in the hidden
layer of this model you can change the
dimensionality of the embedding
lookup table
you can change the number of characters
that are feeding in as an input
as the context into this model
and then of course you can change the
details of the optimization how long are
we running what is the learning rate how
does it change over time
how does it decay
you can change the batch size and you
may be able to actually achieve a much
better convergence speed
in terms of
how many seconds or minutes it takes to
train the model and get
your result in terms of really good
loss
and then of course i actually invite you
to read this paper it is 19 pages but at
this point you should actually be able
to read a good chunk of this paper and
understand
pretty good chunks of it
and this paper also has quite a few
ideas for improvements that you can play
with
so all of those are not available to you
and you should be able to beat this
number i'm leaving that as an exercise
to the reader and that's it for now and
i'll see you next time
before we wrap up i also wanted to show
how you would sample from the model
so we're going to generate 20 samples
at first we begin with all dots so
that's the context
and then until we generate
the zeroth character again
we're going to embed the current context
using the embedding table c now usually
uh here the first dimension was the size
of the training set but here we're only
working with a single example that we're
generating so this is just the mission
one just for simplicity
and so this embedding then gets
projected into the end state you get the
logits
now we calculate the probabilities for
that you can use f.softmax
of logits and that just basically
exponentiates the logits and makes them
sum to one and similar to cross entropy
it is careful that there's no overflows
once we have the probabilities we sample
from them using torture multinomial to
get our next index and then we shift the
context window to append the index and
record it and then we can just
decode all the integers to strings
and print them out
and so these are some example samples
and you can see that the model now works
much better so the words here are much
more word like or name like so we have
things like ham
joes
you know it's starting to sound a little
bit more name-like so we're definitely
making progress but we can still improve
on this model quite a lot
okay sorry there's some bonus content i
wanted to mention that i want to make
these notebooks more accessible and so i
don't want you to have to like install
jupyter notebooks and torch and
everything else so i will be sharing a
link to a google colab
and google collab will look like a
notebook in your browser and you can
just go to the url and you'll be able to
execute all of the code that you saw in
the google collab and so this is me
executing the code in this lecture and i
shortened it a little bit but basically
you're able to train the exact same
network and then plot and sample from
the model and everything is ready for
you to like tinker with the numbers
right there in your browser no
installation necessary
so i just wanted to point that out and
the link to this will be in the video
descriptionhi everyone today we are continuing our
implementation of makemore now in the
last lecture we implemented the
multi-layer perceptron along the lines
of benjiotyle 2003 for character level
language modeling so we followed this
paper took in a few characters in the
past and used an MLP to predict the next
character in a sequence
so what we'd like to do now is we'd like
to move on to more complex and larger
neural networks like recurrent neural
networks and their variations like the
Gru lstm and so on
now before we do that though we have to
stick around the level of multilia
perceptron for a bit longer and I'd like
to do this because I would like us to
have a very good intuitive understanding
of the activations in the neural net
during training and especially the
gradients that are flowing backwards and
how they behave and what they look like
and this is going to be very important
to understand the history of the
development of these architectures
because we'll see that recurrent neural
networks while they are very expressive
in that they are a universal
approximator and can in principle
Implement uh all the algorithms we'll
see that they are not very easily
optimizable with the first order
ingredient-based techniques that we have
available to us and that we use all the
time
and the key to understanding why they
are not optimizable easily is to
understand the the activations and the
gradients and how they behave during
training and we'll see that a lot of the
variants since recurring neural networks
have tried to improve that situation and
so that's the path that we have to take
and let's get started so the starting
code for this lecture is largely the
code from four but I've cleaned it up a
little bit
so you'll see that we are importing all
the torch and matplotlab utilities we're
reading in the words just like before
these are eight example words there's a
total of 32 000 of them here's a
vocabulary of all the lowercase letters
and the special dot token
here we are reading in the data set and
processing it and creating three splits
the train Dev and the test split
now in MLP this is the identical same
MLP except you see that I removed a
bunch of magic numbers that we had here
and instead we have the dimensionality
of the embedding space of the characters
and the number of hidden units in the
hidden layer and so I've pulled them
outside here so that we don't have to go
and change all these magic numbers all
the time
with the same neural net with 11 000
parameters that we optimize now over 200
000 steps with a batch size of 32 and
you'll see that I refactor I refactored
the code here a little bit but there are
no functional changes I just created a
few extra variables a few more comments
and I removed all the magic numbers and
otherwise is the exact same thing
then when we optimize we saw that our
loss looked something like this we saw
that the train and Val loss were about
2.16 and so on
here I refactored the code a little bit
for the evaluation of arbitrary splits
so you pass in a string of which split
you'd like to evaluate and then here
depending on train Val or test I index
in and I get the correct split and then
this is the forward pass of the network
and evaluation of the loss and printing
it
so just making it nicer one thing that
you'll notice here is
I'm using a decorator torch.nograd which
you can also look up and read the
documentation of basically what this
decorator does on top of a function is
that whatever happens in this function
is assumed by torch to never require any
gradients so it will not do any of the
bookkeeping that it does to keep track
of all the gradients in anticipation of
an eventual backward pass
it's almost as if all the tensors that
get created here have a requires grad of
false and so it just makes everything
much more efficient because you're
telling torch that I will not call that
backward on any of this computation and
you don't need to maintain the graph
under the hood
so that's what this does and you can
also use a context manager with
torch.nograd and you can look those up
then here we have the sampling from a
model
um just as before just a poor passive
and neural net getting the distribution
sampling from it adjusting the context
window and repeating until we get the
special and token and we see that we are
starting to get much nicer looking words
sample from the model but still not
amazing and they're still not fully
named like but it's much better than
when we had them with the byground model
so that's our starting point now the
first thing I would like to scrutinize
is the initialization
I can tell that our network is very
improperly configured at initialization
and there's multiple things wrong with
it but let's just start with the first
one
look here on the zeroth iteration the
very first iteration
we are recording a loss of 27 and this
rapidly comes down to roughly one or two
or so I can tell that the initialization
is all messed up because this is way too
high
in training of neural Nets it is almost
always the case that you will have a
rough idea for what loss to expect at
initialization and that just depends on
the loss function and the problem set up
in this case I do not expect 27. I
expect a much lower number and we can
calculate it together
basically at initialization what we'd
like is that there's 27 characters that
could come next for any one training
example at initialization we have no
reason to believe any characterist to be
much more likely than others and so we'd
expect that the probability distribution
that comes out initially is a uniform
distribution assigning about equal
probability to all the 27 characters
so basically what we like is the
probability for any character would be
roughly 1 over 27.
that is the probability we should record
and then the loss is the negative log
probability so let's wrap this in a
tensor
and then that one can take the log of it
and then the negative log probability is
the loss we would expect
which is 3.29 much much lower than 27.
and so what's happening right now is
that at initialization the neural net is
creating probability distributions that
are all messed up some characters are
very confident and some characters are
very not confident
and then basically what's happening is
that the network is very confidently
wrong
and uh that make that's what makes it um
record very high loss so here's a
smaller four-dimensional example of the
issue let's say we only have four
characters and then we have Logics that
come out of the neural land and they are
very very close to zero
then when we take the soft Max of all
zeros we get probabilities there are a
diffuse distribution
so sums to one and is exactly uniform
and then in this case if the label is
say 2 it doesn't actually matter if this
if the label is 2 or 3 or 1 or 0 because
it's a uniform distribution we're
recording the exact same loss in this
case 1.38 so this is the loss we would
expect for a four-dimensional example
and I can see of course that as we start
to manipulate these logits we're going
to be changing the loss here so it could
be that we lock out and by chance this
could be a very high number like you
know five or something like that then in
that case we'll record a very low loss
because we're assigning the correct
probability at the initialization by
chance to the correct label
much more likely it is that some other
dimension will have a high uh logit and
then what will happen is we start to
record much higher loss and what can
come what can happen is basically The
Lodges come out like something like this
you know and they take on Extreme values
and we record really high loss
um
for example if we have torch.randen of
four so these are uniform sorry these
are normally distributed numbers
uh forum
and here we can also print the logits
the probabilities that come out of it
and the loss and so because these logits
are near zero for the most part the loss
that comes out is is okay
but suppose this is like times 10 now
you see how because these are more
extreme values it's very unlikely that
you're going to be guessing the correct
bucket and then you're confidently wrong
and recording very high loss
if your lodges are coming out even more
extreme
you might get extremely insane losses
like infinity even at initialization
um
so basically this is not good and we
want the load just to be roughly zero
um when the network is initialized in
fact the logits can don't have to be
just zero they just have to be equal so
for example if all the objects are one
then because of the normalization inside
the softmax this will actually come out
okay
but by symmetry we don't want it to be
any arbitrary positive or negative
number we just want it to be all zeros
and record the loss that we expect at
initialization so let's Now quickly see
where things go wrong in our example
here we have the initialization
let me reinitialize the neural net and
here let me break after the very first
iteration so we only see the initial
loss which is 27.
so that's way too high and intuitively
now we can expect the variables involved
and we see that the logits here if we
just print some of these
if we just print the first row we see
that the load just take on quite extreme
values
and that's what's creating the fake
confidence and incorrect answers and
makes the loss
get very very high so these Lotus should
be much much closer to zero so now let's
think through how we can achieve logits
coming out of this neural net to be more
closer to zero
you see here that lodges are calculated
as the hidden States multiplied by W2
plus B2
so first of all currently we're
initializing B2 as random values of the
right size
but because we want roughly zero we
don't actually want to be adding a bias
of random numbers so in fact I'm going
to add a times a zero here to make sure
that B2 is just basically zero at the
initialization
and second this is H multiplied by W2 so
if we want logits to be very very small
then we would be multiplying W2 and
making that smaller
so for example if we scale down W2 by
0.1 all the elements then if I
do again just the very first iteration
you see that we are getting much closer
to what we expect so the rough roughly
what we want is about 3.29 this is 4.2
I can make this maybe even smaller
3.32 okay so we're getting closer and
closer now you're probably wondering can
we just set this to zero
then we get of course exactly what we're
looking for at initialization
and the reason I don't usually do this
is because I'm I'm very nervous and I'll
show you in a second why you don't want
to be setting W's or weights of a neural
net exactly to zero um you usually want
it to be small numbers instead of
exactly zero for this output layer in
this specific case I think it would be
fine but I'll show you in a second where
things go wrong very quickly if you do
that so let's just go with 0.01
in that case our loss is close enough
but has some entropy it's not exactly
zero it's got some low entropy and
that's used for symmetry breaking as
we'll see in a second
the logits are now coming out much
closer to zero and everything is well
and good
so if I just erase these and I now take
away the break statement
we can run the optimization with this
new initialization and let's just see
what losses we record okay so I let it
run and you see that we started off good
and then we came down a bit
the plot of the loss now doesn't have
this hockey shape appearance
um because basically what's happening in
the hockey stick the very first few
iterations of the loss what's happening
during the optimization is the
optimization is just squashing down the
logits and then it's rearranging the
logits so basically we took away this
easy part of the loss function where
just the the weights were just being
shrunk down
and so therefore we don't we don't get
these easy gains in the beginning and
we're just getting some of the hard
gains of training the actual neural nut
and so there's no hockey stick
appearance
so good things are happening in that
both number one lawsuit initialization
is what we expect
and the the loss doesn't look like a
hockey stick and this is true for any
neuron that you might train and
something to look out for
and second the last that came out is
actually quite a bit improved
unfortunately I erased what we had here
before I believe this was
2.12 and this is what this was 2.16 so
we get a slightly improved result and
the reason for that is uh because we're
spending more Cycles more time
optimizing the neural net actually
instead of just spending the first
several thousand iterations probably
just squashing down the weights
because they are so way too high in the
beginning in the initialization
so something to look out for and uh
that's number one now let's look at the
second problem let me re-initialize our
neural net and let me reintroduce the
break statement
so we have a reasonable initial loss so
even though everything is looking good
on the level of the loss and we get
something that we expect there's still a
deeper problem working inside this
neural net and its initialization
so the logits are now okay the problem
now is with the values of H
the activations of the Hidden States now
if we just visualize this Vector sorry
this tensor H it's kind of hard to see
but the problem here roughly speaking is
you see how many of the elements are one
or negative one
now recall that torch.nh the 10h
function is a squashing function it
takes arbitrary numbers and it squashes
them into a range of negative one and
one and it does so smoothly
so let's look at the histogram of H to
get a better idea of the distribution of
the values inside this tensor
we can do this first
well we can see that H is 32 examples
and 200 activations in each example we
can view it as negative one to stretch
it out into one large vector
and we can then call to list to convert
this into one large python list of
floats
and then we can pass this into plt.hist
for histogram and we say we want 50 bins
and a semicolon to suppress a bunch of
output we don't want
so we see this histogram and we see that
most the values by far take on the value
of negative one and one so this 10h is
very very active
and we can also look at
basically why that is
we can look at the pre-activations that
feed into the 10h
and we can see that the distribution of
the pre-activations are is very very
broad these take numbers between
negative 15 and 15 and that's why in the
torture 10h everything is being squashed
and capped to be in the range of
negative one and one and lots of numbers
here take on very extreme values
now if you are new to neural networks
you might not actually see this as an
issue but if you're well burst in the
dark arts back propagation and then have
an intuitive sense of how these
gradients flow through a neural net you
are looking at your distribution of 10h
activations here and you are sweating
so let me show you why we have to keep
in mind that during that propagation
just like we saw in micrograd we are
doing backward pass starting at the loss
and flowing through the network
backwards in particular we're going to
back propagate through this tors.10h
and this layer here is made up of 200
neurons for each one of these examples
and it implements an element twice 10 H
so let's look at what happens in 10h in
the backward pass
we can actually go back to our previous
micrograd code in the very first lecture
and see how we implement the 10h
we saw that the input here was X and
then we calculate T which is the 10h of
x
so that's T and T is between negative 1
and 1. it's the output of the 10h and
then in the backward pass how do we back
propagate through a 10 h
we take out that grad
and then we multiply it this is the
chain rule with the local gradient which
took the form of 1 minus t squared
so what happens if the outputs of your
10h are very close to negative one or
one if you plug in t equals one here
you're going to get a zero multiplying
out that grad no matter what up.grad is
we are killing the gradient and we're
stopping effectively the back
propagation through this 10h unit
similarly when T is negative one this
will again become zero and out that grad
just stops
and intuitively this makes sense because
this is a 10 H neuron
and what's happening is if its output is
very close to one then we are in the
tail of this 10 h
and so changing basically the input
is not going to impact the output of the
10h too much because it's it's so it's
in a flat region of the 10h and so
therefore there's no impact on the loss
and so indeed the the weights and the
biases along with this tan H neuron do
not impact the loss because the output
of the standard unit is in the flat
region in the 10h and there's no
influence we can we can be changing them
whatever we want however we want and the
loss is not impacted that's that's
another way to justify that indeed the
gradient would be basically zero it
vanishes
indeed when T equals zero
we get 1 times at that grad so when the
10h takes on exactly value of zero then
out.grad is just passed through
so basically what this is doing right is
if T is equal to zero then this the 10h
unit is uh sort of inactive
and uh gradient just passes through but
the more you are in the flat tails the
more the gradient is squashed
so in fact you'll see that the the
gradient flowing through 10 H can only
ever decrease in the amount that it
decreases is proportional through a
square here
depending on how far you are in the flat
tails of this 10 age
and so that's kind of what's Happening
Here and through this the concern here
is that if all of these outputs H are in
the flat regions of negative one and one
then the gradients that are flowing
through the network will just get
destroyed at this layer
now there is some redeeming quality here
and that we can actually get a sense of
the problem here as follows
I've wrote some code here and basically
what we want to do here is we want to
take a look at H take the absolute value
and see how often it is in the in a flat
region so say greater than 0.99
and what you get is the following and
this is a Boolean tensor so uh in the
Boolean tensor you get a white if this
is true and a black and this is false
and so basically what we have here is
the 32 examples and then 200 hidden
neurons and we see that a lot of this is
white and what that's telling us is that
all these 10h neurons were very very
active and uh they're in a flat tail
and so in all these cases uh the back
the backward gradient would get
destroyed
now we would be in a lot of trouble if
for ever for any one of these 200
neurons if it was the case that the
entire column is white because in that
case we have what's called a dead neuron
and this could be a tannage neuron where
the initialization of the weights and
the biases could be such that no single
example ever activates this 10h in the
sort of active part of the 10h if all
the examples land in the tail then this
neuron will never learn it is a dead
neuron
and so just scrutinizing this and
looking for Columns of completely white
we see that this is not the case so I
don't see a single neuron that is all of
uh you know white
and so therefore it is the case that for
every one of these 10h neurons
we do have some examples that activate
them in the active part of the 10h and
so some gradients will flow through and
this neuron will learn and the neuron
will change and it will move and it will
do something
but you can sometimes get yourself in
cases where you have dead neurons and
the way this manifests is that for 10
inch neuron this would be when no matter
what inputs you plug in from your data
set this 10 inch neuron always fires
completely one or completely negative
one and then it will just not learn
because all the gradients will be just
zeroed out
uh this is true not just percentage but
for a lot of other non-linearities that
people use in neural networks so we
certainly use 10h a lot but sigmoid will
have the exact same issue because it is
a squashing neuron and so the same will
be true for sigmoid but
um but um you know
um basically the same will actually
applied to sigmoid the same will also
reply to a relu so relu has a completely
flat region here below zero
so if you have a relative neuron then it
is a pass-through
um if it is positive and if it's if the
pre-activation is negative it will just
shut it off since the region here is
completely flat then during back
propagation uh this would be exactly
zeroing out the gradient
um like all of the gradient would be set
exactly to zero instead of just like a
very very small number depending on how
positive or negative T is
and so you can get for example a dead
relu neuron and a dead relinuron would
basically look like
basically what it is is if a neuron with
a relu nonlinearity never activates
so for any examples that you plug in in
the data set it never turns on it's
always in this flat region then this
reli neuron is a dead neuron it's
weights and bias will never learn they
will never get a gradient because the
neuron never activated
and this can sometimes happen at
initialization because the weights and
the biases just make it so that by
chance some neurons are just forever
dead
but it can also happen during
optimization if you have like a too high
of the learning rate for example
sometimes you have these neurons that
gets too much of a gradient and they get
knocked out off the data manifold
and what happens is that from then on no
example ever activates this neuron so
this neuron remains that forever so it's
kind of like a permanent brain damage in
a in a mind of a network
and so sometimes what can happen is if
your learning rate is very high for
example and you have a neural net with
regular neurons you train the neural net
and you get some last loss but then
actually what you do is you go through
the entire training set and you forward
your examples and you can find neurons
that never activate they are dead
neurons in your network and so those
neurons will will never turn on and
usually what happens is that during
training these relative neurons are
changing moving Etc and then because of
a high gradient somewhere by chance they
get knocked off
and then nothing ever activates them and
from then on they are just dead
uh so that's kind of like a permanent
brain damage that can happen to some of
these neurons
these other nonlinearities like leaky
relu will not suffer from this issue as
much because you can see that it doesn't
have flat tails you almost always get
gradients
and elu is also fairly frequently used
it also might suffer from this issue
because it has flat parts
so that's just something to be aware of
and something to be concerned about and
in this case we have way too many
um activations H that take on Extreme
values and because there's no column of
white I think we will be okay and indeed
the network optimizes and gives us a
pretty decent loss but it's just not
optimal and this is not something you
want especially during initialization
and so basically what's happening is
that this H pre-activation that's
flowing to 10h
it's it's too extreme it's too large
it's creating very
um it's creating a distribution that is
too saturated in both sides of the 10h
and it's not something you want because
it means that there's less training uh
for these neurons because they update
um less frequently so how do we fix this
well HP activation is
MCAT which comes from C so these are
uniform gaussian but then it's
multiplied by W1 plus B1 and H preact is
too far off from zero and that's causing
the issue so we want this reactivation
to be closer to zero very similar to
what we have with lodges
so here we want actually something very
very similar
now it's okay to set the biases to very
small number we can either multiply it
by zero zero one to get like a little
bit of entropy
um I sometimes like to do that
um just so that
there's like a little bit of variation
and diversity in the original
initialization of these 10h neurons and
I find in practice that that can help
optimization a little bit
and then the weights we can also just
like squash so let's multiply everything
by 0.1
let's rerun the first batch
and now let's look at this and well
first let's look here
you see now because we multiply doubly
by 0.1 we have a much better histogram
and that's because the pre-activations
are now between negative 1.5 and 1.5 and
this we expect much much less white
okay there's no white
so basically that's because there are no
neurons that's saturated above 0.99 in
either direction this is actually a
pretty decent place to be
um
maybe we can go up a little bit
it's very much am I changing W1 here so
maybe we can go to point two
okay so maybe something like this is is
a nice distribution so maybe this is
what our initialization should be so let
me now erase
these
and let me starting with initialization
let me run the full optimization without
the break and uh let's see what we got
okay so the optimization finished and I
Rebrand the loss and this is the result
that we get and then just as a reminder
I put down all the losses that we saw
previously in this lecture
so we see that we actually do get an
improvement here and just as a reminder
we started off with a validation loss of
2.17 when we started by fixing the
softmax being confidently wrong we came
down to 2.13 and by fixing the 10h layer
being way too saturated we came down to
2.10
and the reason this is happening of
course is because our initialization is
better and so we're spending more time
doing productive training instead of
um
not very productive training because our
gradients are set to zero and we have to
learn very simple things like the
overconfidence of the softmax in the
beginning and we're spending Cycles just
like squashing down the weight Matrix
so this is illustrating
um basically initialization and its
impacts on performance just by being
aware of the internals of these neural
Nets and their activations their
gradients now we're working with a very
small Network this is just one layer
multiplayer perceptron so because the
network is so shallow the optimization
problem is actually quite easy and very
forgiving so even though our
initialization was terrible the network
still learned eventually it just got a
bit worse result this is not the case in
general though once we actually start
working with much deeper networks that
have say 50 layers things can get much
more complicated and these problems
Stack Up
and so you can actually get into a place
where the network is basically not
training at all if your initialization
is bad enough and the deeper your
network is and the more complex it is
the less forgiving it is to some of
these errors and so
um something that we definitely be aware
of and uh something to scrutinize
something to plot and something to be
careful with and um
yeah okay so that's great that that
worked for us but what we have here now
is all these metric numbers like point
two like where do I come up with this
and how am I supposed to set these if I
have a large neural left with lots and
lots of layers
and so obviously no one does this by
hand there's actually some relatively
principled ways of setting these scales
um that I would like to introduce to you
now
so let me paste some code here that I
prepared just to motivate the discussion
of this
so what I'm doing here is we have some
random input here x that is drawn from a
gaussian and there's 1000 examples that
are 10 dimensional and then we have a
weight and layer here that is also
initialized using gaussian just like we
did here
and we these neurons in the hidden layer
look at 10 inputs and there are 200
neurons in this hidden layer and then we
have here just like here
um in this case the multiplication X
multiplied by W to get the
pre-activations of these neurons
and basically the analysis here looks at
okay suppose these are uniform gaussian
and these weights are uniform gaussian
if I do x times W and we forget for now
the bias and the non-linearity
then what is the mean and the standard
deviation of these gaussians
so in the beginning here the input is uh
just a normal gaussian distribution mean
zero and the standard deviation is one
and the standard deviation again is just
a measure of a spread of discussion
but then once we multiply here and we
look at the histogram of Y we see that
the mean of course stays the same it's
about zero because this is a symmetric
operation but we see here that the
standard deviation has expanded to three
so the input standard deviation was one
but now we've grown to three and so what
you're seeing in the histogram is that
this gaussian is expanding
and so
um we're expanding this gaussian from
the input and we don't want that we want
most of the neural Nets to have
relatively similar activations so unit
gaussian roughly throughout the neural
net and so the question is how do we
scale these wfs to preserve the um to
preserve this distribution to remain a
gaussian
and so intuitively if I multiply here uh
these elements of w by a large number
let's say by five
then this gaussian grows and grows in
standard deviation so now we're at 15.
so basically these numbers here in the
output y take on more and more extreme
values
but if we scale it down well I say 0.2
then conversely this gaussian is getting
smaller and smaller and it's shrinking
and you can see that the standard
deviation is 0.6 and so the question is
what do I multiply by here to exactly
preserve the standard deviation to be
one
and it turns out that the correct answer
mathematically when you work out through
the variance of this multiplication here
is that you are supposed to divide by
the square root of the fan in the fan in
is the basically the uh number of input
elements here 10. so we are supposed to
divide by 10 square root and this is one
way to do the square root you raise it
to a power of 0.5 that's the same as
doing a square root
so when you divide by the square root of
10 then we see that
the output gaussian it has exactly
standard deviation of one now
unsurprisingly a number of papers have
looked into how but to best initialize
neural networks and in the case of
multiplayer perceptions we can have
fairly deep networks that have these
nonlinearities in between and we want to
make sure that the activations are well
behaved and they don't expand to
infinity or Shrink all the way to zero
and the question is how do we initialize
the weights so that these activations
take on reasonable values throughout the
network
now one paper that has stuck this in
quite a bit of detail that is often
referenced is this paper by coming here
at all called the delving deep into
rectifiers now in this case they
actually study convolutional neural
networks and they studied especially the
relu nonlinearity and the p-valued
nonlinearity instead of a 10 H
nonlinearity but the analysis is very
similar and
um basically what happens here is for
them the the relation that they care
about quite a bit here is a squashing
function where all the negative numbers
are simply clamped to zero so the
positive numbers are passed through but
everything negative is just set to zero
and because uh you are basically
throwing away half of the distribution
they find in their analysis of the
forward activations in the neural net
that you have to compensate for that
with a gain
and so here
they find that basically when they
initialize their weights they have to do
it with a zero mean gaussian whose
standard deviation is square root of 2
over the fan in
what we have here is we are initializing
a concussion with the square root of
fanin
this NL here is the Fanon so what we
have is square root of one over the fan
in
because we have the division here
now they have to add this factor of 2
because of the relu which basically
discards half of the distribution and
clamps it at zero and so that's where
you get an initial Factor
now in addition to that this paper also
studies not just the uh sort of behavior
of the activations in the forward pass
of the neural net but it also studies
the back propagation and we have to make
sure that the gradients also are well
behaved and so
um because ultimately they end up
updating our parameters
and what they find here through a lot of
the analysis that I invite you to read
through but it's not exactly
approachable what they find is basically
if you properly initialize the forward
pass the backward pass is also
approximately initialized up to a
constant factor that has to do with the
size of the number of hidden neurons in
an early and uh late layer
and uh but basically they find
empirically that this is not a choice
that matters too much
now this timing initialization is also
implemented in pytorch so if you go to
torch.nn.net documentation you'll find
timing normal
and in my opinion this is probably the
most common way of initializing neural
networks now
and it takes a few keyword arguments
here so number one it wants to know the
mode would you like to normalize the
activations or would you like to
normalize the gradients to to be always
gaussian with zero mean and a unit or
one standard deviation and because they
find the paper that this doesn't matter
too much most of the people just leave
it as the default which is Fan in and
then second passing the nonlinearity
that you are using because depending on
the nonlinearity we need to calculate a
slightly different gain and so if your
nonlinearity is just linear so there's
no nonlinearity then the gain here will
be one and we have the exact same uh
kind of formula that we've got here
but if the nonlinearity is something
else we're going to get a slightly
different gain and so if we come up here
to the top
we see that for example in the case of
relu this gain is a square root of 2.
and the reason it's a square root
because in this paper
you see how the two is inside of the
square root so the gain is a square root
of 2.
in the case of linear or identity we
just get a gain of one in the case of
10h which is what we're using here the
advised gain is a 5 over 3.
and intuitively why do we need a gain on
top of the initialization is because 10h
just like relu is a contractive
transformation so what that means is
you're taking the output distribution
from this matrix multiplication and then
you are squashing it in some way now
relu squashes it by taking everything
below zero and clamping it to zero tan H
also squashes it because it's a
contractual operation it will take the
Tails and it will
squeeze them in and so in order to fight
the squeezing in we need to boost the
weights a little bit so that we
renormalize everything back to standard
unit standard deviation
so that's why there's a little bit of a
gain that comes out
now I'm skipping through this section A
little bit quickly and I'm doing that
actually intentionally and the reason
for that is because
about seven years ago when this paper
was written you had to actually be
extremely careful with the activations
and ingredients and their ranges and
their histograms and you have to be very
careful with the precise setting of
gains and the scrutinizing of the
nonlinearities used and so on and
everything was very finicky and very
fragile and very properly arranged for
the neural not to train especially if
your neural network was very deep
but there are a number of modern
innovations that have made everything
significantly more stable and more
well-behaved and has become less
important to initialize these networks
exactly right
and some of those modern Innovations for
example are residual connections which
we will cover in the future the use of a
number of normalization layers like for
example batch normalization layer
normalization group normalization we're
going to go into a lot of these as well
and number three much better optimizers
not just stochastic gradient descent the
simple Optimizer we're basically using
here but a slightly more complex
optimizers like RMS prop and especially
Adam and so all of these modern
Innovations make it less important for
you to precisely calibrate the
initialization of the neural net all
that being said in practice uh what
should we do in practice when I
initialize these neural Nets I basically
just normalize my weights by the square
root of the fan in uh so basically uh
roughly what we did here is what I do
now if we want to be exactly accurate
here we and go by init of coming normal
this is how a good implemented we want
to set the standard deviation to be
gained over the square root of fan n
right so to set the standard deviation
of our weights we will proceed as
follows
basically when we have a torch that
renin and let's say I just create a
thousand numbers we can look at the
standard deviation of this and of course
that's one that's the amount of spread
let's make this a bit bigger so it's
closer to one
so that's the spread of the gaussian of
zero mean and unit standard deviation
now basically when you take these and
you multiply by say 0.2
that basically scales down the gaussian
and that makes its standard deviation
0.2 so basically the number that you
multiply by here ends up being the
standard deviation of this caution
so here this is a standard deviation 0.2
gaussian here when we sample rw1
but we want to set the standard
deviation to gain over square root of
fan mode which is valid
so in other words we want to multiply by
gain which for 10 H is 5 over 3.
5 over 3 is the gain
and then times
um
I guess sorry divide
uh square root of the fan in and in this
example here the fan in was 10 and I
just noticed that actually here the fan
in for W1 is actually an embed times
block size which as you all recall is
actually 30 and that's because each
character is 10 dimensional but then we
have three of them and we concatenate
them so actually the fan in here was 30
and I should have used 30 here probably
but basically we want 30 square root so
this is the number this is what our
standard deviation we want to be and
this number turns out to be 0.3
whereas here just by fiddling with it
and looking at the distribution and
making sure it looks okay we came up
with 0.2
and so instead what we want to do here
is we want to make the standard
deviation B
um
5 over 3 which is our gain divide
this amount
times 0.2 square root and these brackets
here are not that necessary but I'll
just put them here for clarity this is
basically what we want this is the
chiming in it in our case for a 10h
nonlinearity and this is how we would
initialize the neural net and so we're
multiplying by 0.3 instead of
multiplying by 0.2
and so we can
we can initialize this way and then we
can train the neural net and see what we
got
okay so I trained the neural net and we
end up in roughly the same spot so
looking at the validation loss we now
get 2.10 and previously we also had 2.10
and there's a little bit of a difference
but that's just the randomness of the
process I suspect
but the big deal of course is we get to
the same spot but we did not have to
introduce any magic numbers that we got
from just looking at histograms and
guessing checking we have something that
is semi-principled and will scale us to
much bigger networks and uh something
that we can sort of use as a guide so I
mentioned that the precise setting of
these initializations is not as
important today due to some Modern
Innovations and I think now is a pretty
good time to introduce one of those
modern Innovations and that is best
normalization
so batch normalization came out in 2015
from a team at Google and it was an
extremely impactful paper because it
made it possible to train very deep
neural Nets quite reliably and uh it
basically just worked so here's what
nationalization does and what's
implemented
um
basically we have these hidden States HP
act right and we were talking about how
we don't want these uh these
pre-activation states to be way too
small because then the 10h is not doing
anything but we don't want them to be
too large because then the 10h is
saturated
in fact we want them to be roughly
roughly gaussian so zero mean and a unit
or one standard deviation at least at
initialization
so the Insight from The Bachelor
normalization paper is okay you have
these hidden States and you'd like them
to be roughly gaussian then why not take
the hidden States and just normalize
them to be gaussian and it sounds kind
of crazy but you can just do that
because uh standardizing hidden States
so that their unit caution is a
perfectly differentiable operation as
we'll soon see and so that was kind of
like the big Insight in this paper and
when I first read it my mind was blown
because you can just normalize these
hidden States and if you'd like unit
gaussian States in your network at least
initialization you can just normalize
them to be in gaussian so let's see how
that works so we're going to scroll to
our pre-activations here just before
they enter into the 10 age
now the idea again is remember we're
trying to make these roughly gaussian
and that's because if these are way too
small numbers then the 10h here is kind
of connective but if these are very
large numbers then the 10h is way too
saturated and grade is in the flow so
we'd like this to be roughly caution
so the Insight in bathroomization again
is that we can just standardize these
activations so they are exactly gaussian
so here hpact
has a shape of 32 by 200 32 examples by
200 neurons in the hidden layer
so basically what we can do is we can
take hpact and we can just calculate the
mean
and the mean we want to calculate across
the zeroth dimension
and we want to also keep them as true so
that we can easily broadcast this
so the shape of this
is 1 by 200 in other words we are doing
the mean over all the uh elements in the
batch
and similarly we can calculate the
standard deviation of these activations
and that will also be one by 200.
now in this paper they have the
uh sort of prescription here and see
here we are calculating the mean which
is just taking the average value
of any neurons activation and then the
standard deviation is basically kind of
like
this the measure of the spread that
we've been using which is the distance
of every one of these values away from
the mean and that squared and averaged
that's the that's the variance and then
if you want to take the standard
deviation you would square root the
variance to get the standard deviation
so these are the two that we're
calculating and now we're going to
normalize or standardize these X's by
subtracting the mean and
um dividing by the standard deviation
so basically we're taking Edge preact
and we subtract
the mean
and then we divide by the standard
deviation
this is exactly what these two STD and
mean are calculating
oops
sorry this is the mean and this is the
variance you see how the sigma is the
standard deviation usually so this is
Sigma Square which is variance is the
square of the standard deviation
so this is how you standardize these
values and what this will do is that
every single neuron now and its firing
rate will be exactly unit gaussian on
these 32 examples at least of this batch
that's why it's called batch
normalization we are normalizing these
batches
and then we could in principle train
this notice that calculating the mean
and the standard deviation these are
just mathematical formulas they're
perfectly differentiable all this is
perfectly differentiable and we can just
strain this
the problem is you actually won't
achieve a very good result with this and
the reason for that is
we want these to be roughly gaussian but
only at initialization but we don't want
these to be to be forced to be gaussian
always we would actually We'll add the
neural nuts to move this around to
potentially make it more diffuse to make
it more sharp to make some 10 H neurons
maybe mean more trigger more trigger
happy or less trigger happy so we'd like
this distribution to move around and
we'd like the back propagation to tell
us how that distribution should move
around and so in addition to this idea
of standardizing the activations at any
point in the network
uh we have to also introduce this
additional component in the paper
here describe the scale and shift
and so basically what we're doing is
we're taking these normalized inputs and
we are additionally scaling them by some
gain and offsetting them by some bias to
get our final output from this layer
and so what that amounts to is the
following
we are going to allow a batch
normalization gain
to be initialized at just a once
and the ones will be in the shape of 1
by n hidden
and then we also will have a b and bias
which will be torched at zeros
and it will also be of the shape n by 1
by and hidden
and then here
the B and gain will multiply this
and the BN bias will offset it here
so because this is initialized to one
and this to zero
at initialization each neuron's firing
values in this batch will be exactly
unit gaussian and we'll have nice
numbers no matter what the distribution
of the hpact is coming in coming out it
will be in gaussian for each neuron and
that's roughly what we want at least at
initialization
um and then during optimization we'll be
able to back propagate to be in game and
being biased and change them so the
network is given the full ability to do
with this whatever it wants uh
internally
here we just have to make sure that we
um include these in the parameters of
the neural nut because they will be
trained with back propagation
so let's initialize this
and then we should be able to train
and then we're going to also
copy this line
which is the best normalization layer
here on a single line of code and we're
going to swing down here and we're also
going to do the exact same thing at test
time here
so similar to training time we're going
to normalize and then scale and that's
going to give us our train and
validation loss
and we'll see in a second that we're
actually going to change this a little
bit but for now I'm going to keep it
this way
so I'm just going to wait for this to
converge okay so I'll add the neural
nuts to converge here and when we scroll
down we see that our validation loss
here is 2.10 roughly which I wrote down
here and we see that this is actually
kind of comparable to some of the
results that we've achieved previously
now I'm not actually expecting an
improvement in this case and that's
because we are dealing with a very
simple neural nut that has just a single
hidden layer so in fact in this very
simple case of just one hidden layer we
were able to actually calculate what the
scale of w should be to make these
pre-activations already have a roughly
gaussian shape so the best normalization
is not doing much here
but you might imagine that once you have
a much deeper neural nut that has lots
of different types of operations and
there's also for example residual
connections which we'll cover and so on
it will become basically very very
difficult to tune those scales of your
weight matrices such that all the
activations throughout the neural Nets
are roughly gaussian
and so that's going to become very
quickly intractable but compared to that
it's going to be much much easier to
sprinkle batch normalization layers
throughout the neural net
so in particular it's common to to look
at every single linear layer like this
one this is a linear layer multiplying
by a weight Matrix and adding the bias
or for example convolutions which we'll
cover later and also perform basically a
multiplication with the weight Matrix
but in a more spatially structured
format it's custom it's customary to
take these linear layer or convolutional
layer and append a bachelorization layer
right after it to control the scale of
these activations at every point in the
neural net so we'd be adding these
bathroom layers throughout the neural
net and then this controls the scale of
these activations throughout the neural
net it doesn't require us to do a
perfect mathematics and care about the
activation distributions for all these
different types of neural network Lego
building blocks that you might want to
introduce into your neural net and it
significantly stabilizes uh the training
and that's why these layers are quite
popular now the stability offered by
batch normalization actually comes at a
terrible cost and that cost is that if
you think about what's Happening Here
something something terribly strange and
unnatural is happening
it used to be that we have a single
example feeding into a neural net and
then we calculate this activations and
it's logits and this is a deterministic
sort of process so you arrive at some
Logics for this example and then because
of efficiency of training we suddenly
started to use batches of examples but
those batches of examples were processed
independently and it was just an
efficiency thing
but now suddenly in bash normalization
because of the normalization through the
batch we are coupling these examples
mathematically and in the forward pass
and the backward pass of the neural land
so now the hidden State activations
hpact and your logits for any one input
example are not just a function of that
example and its input but they're also a
function of all the other examples that
happen to come for a ride in that batch
and these examples are sampled randomly
and so what's happening is for example
when you look at each preact that's
going to feed into H the hidden State
activations for for example for for any
one of these input examples is going to
actually change slightly depending on
what other examples there are in a batch
and and depending on what other examples
happen to come for a ride
H is going to change subtly and it's
going to like Jitter if you imagine
sampling different examples because the
statistics of the mean and the standard
deviation are going to be impacted
and so you'll get a Jitter for H and
you'll get a Jitter for logits
and you think that this would be a bug
or something undesirable but in a very
strange way this actually turns out to
be good in neural network training and
as a side effect and the reason for that
is that you can think of this as kind of
like a regularizer because what's
happening is you have your input and you
get your age and then depending on the
other examples this is generating a bit
and so what that does is that it's
effectively padding out any one of these
input examples and it's introducing a
little bit of entropy and
um because of the padding out it's
actually kind of like a form of data
augmentation which we'll cover in the
future and it's kind of like augmenting
the input a little bit and it's
jittering it and that makes it harder
for the neural nuts to overfit to these
concrete specific examples so by
introducing all this noise it actually
like Pats out the examples and it
regularizes the neural net and that's
one of the reasons why
deceivingly as a second order effect
this is actually a regularizer and that
has made it harder for us to remove the
use of batch normalization
because basically no one likes this
property that the the examples in the
batch are coupled mathematically and in
the forward pass and at least all kinds
of like strange results uh we'll go into
some of that in a second as well
um and it leads to a lot of bugs and
um and so on and so no one likes this
property uh and so people have tried to
deprecate the use of astronomization and
move to other normalization techniques
that do not couple the examples of a
batch examples are layer normalization
instance normalization group
normalization and so on and we'll cover
we'll cover some of these later
um but basically long story short bash
formalization was the first kind of
normalization layer to be introduced it
worked extremely well it happens to have
this regularizing effect it stabilized
training
and people have been trying to remove it
and move to some of the other
normalization techniques but it's been
hard because it just works quite well
and some of the reason that it works
quite well is again because of this
regularizing effect and because of the
because it is quite effective at
controlling the activations and their
distributions
uh so that's kind of like the brief
story of nationalization and I'd like to
show you one of the other weird sort of
outcomes of this coupling
so here's one of the strange outcomes
that I only glossed over previously
when I was evaluating the loss on the
validation side
basically once we've trained a neural
net we'd like to deploy it in some kind
of a setting and we'd like to be able to
feed in a single individual example and
get a prediction out from our neural net
but how do we do that when our neural
net now in a forward pass estimates the
statistics of the mean energy standard
deviation of a batch the neural net
expects badges as an input now so how do
we feed in a single example and get
sensible results out
and so the proposal in the batch
normalization paper is the following
what we would like to do here is we
would like to basically have a step
after training that calculates and sets
the bathroom mean and standard deviation
a single time over the training set
and so I wrote this code here in
interest of time and we're going to call
what's called calibrate the Bachelor of
statistics
and basically what we do is not no grad
telling pytorch that none of this we
will call the dot backward on and it's
going to be a bit more efficient
we're going to take the training set get
the pre-activations for every single
training example and then one single
time estimate the mean and standard
deviation over the entire training set
and then we're going to get B and mean
and be in standard deviation and now
these are fixed numbers as the meaning
of the entire training set
and here instead of estimating it
dynamically
we are going to instead here use B and
mean
and here we're just going to use B and
standard deviation
and so at this time we are going to fix
these clamp them and use them during
inference and now
you see that we get basically identical
result
but the benefit that we've gained is
that we can now also forward a single
example because the mean and standard
deviation are now fixed uh sort of
tensors
that said nobody actually wants to
estimate this mean and standard
deviation as a second stage after neural
network training because everyone is
lazy and so this batch normalization
paper actually introduced one more idea
which is that we can we can estimate the
mean and standard deviation in a running
matter running manner during training of
the neural net and then we can simply
just have a single stage of training and
on the side of that training we are
estimating the running mean and standard
deviation so let's see what that would
look like
let me basically take the mean here that
we are estimating on the batch and let
me call this B and mean on the I
iteration
um and then here this is B and sdd
um bnstd at I okay
uh and the mean comes here and the STD
comes here so so far I've done nothing
I've just moved around and I created
these extra variables for the mean and
standard deviation and I've put them
here so so far nothing has changed but
what we're going to do now is we're
going to keep a running mean of both of
these values during training so let me
swing up here and let me create a BN
mean underscore running
and I'm going to initialize it at zeros
and then be an STD running
which I'll initialize at once
because
in the beginning because of the way we
initialized W1 and B1 each react will be
roughly unit gaussian so the mean will
be roughly zero and the standard
deviation roughly one so I'm going to
initialize these that way
but then here I'm going to update these
and in pytorch
these mean and standard deviation that
are running they're not actually part of
the gradient based optimization we're
never going to derive gradients with
respect to them they're they're updated
on the side of training
and so what we're going to do here is
we're going to say with torch top no
grad telling pytorch that the update
here is not supposed to be building out
a graph because there will be no doubt
backward
but this running is basically going to
be 0.99
times the current value
plus 0.001 times the this value
this new mean
and in the same way be an STD running
will be
mostly what it used to be
but it will receive a small update in
the direction of what the current
standard deviation is
and as you're seeing here this update is
outside and on the side of the gradient
based optimization and it's simply being
updated not using gradient descent it's
just being updated using a gen key like
Smooth
sort of
running mean manner
and so while the network is training and
these pre-activations are sort of
changing and shifting around during back
propagation we are keeping track of the
typical mean and standard deviation and
we're estimating them once and when I
run this
now I'm keeping track of this in a
running manner and what we're hoping for
of course is that the meat being mean
underscore running and B and mean
underscore STD are going to be very
similar to the ones that we calculated
here before
and that way we don't need a second
stage because we've sort of combined the
two stages and we've put them on the
side of each other if you want to look
at it that way
and this is how this is also implemented
in the batch normalization layer in pi
torch so during training
um the exact same thing will happen and
then later when you're using inference
it will use the estimated running mean
of both the mean estimate deviation of
those hidden States
so let's wait for the optimization to
converge and hopefully the running mean
and standard deviation are roughly equal
to these two and then we can simply use
it here and we don't need this stage of
explicit calibration at the end okay so
the optimization finished
I'll rerun the explicit estimation and
then the B and mean from the explicit
estimation is here
and B and mean from the running
estimation
during the during the optimization you
can see it's very very similar
it's not identical but it's pretty close
in the same way be an STD is this and be
an STD running is this
as you can see that once again they are
fairly similar values not identical but
pretty close
and so then here instead of being mean
we can use the B and mean running
instead of being STD we can use bnstd
running
and uh hopefully the validation loss
will not be impacted too much
okay so it's basically identical and
this way we've eliminated the need for
this explicit stage of calibration
because we are doing it in line over
here okay so we're almost done with
batch normalization there are only two
more notes that I'd like to make number
one I've skipped a discussion over what
is this plus Epsilon doing here this
Epsilon is usually like some small fixed
number for example one a negative five
by default and what it's doing is that
it's basically preventing a division by
zero in the case that the variance over
your batch
is exactly zero in that case uh here we
normally have a division by zero but
because of the plus Epsilon this is
going to become a small number in the
denominator instead and things will be
more well behaved so feel free to also
add a plus Epsilon here of a very small
number it doesn't actually substantially
change the result I'm going to skip it
in our case just because this is
unlikely to happen in our very simple
example here and the second thing I want
you to notice is that we're being
wasteful here and it's very subtle but
right here where we are adding the bias
into hpact
these biases now are actually useless
because we're adding them to the hpact
but then we are calculating the mean
for every one of these neurons and
subtracting it so whatever bias you add
here is going to get subtracted right
here
and so these biases are not doing
anything in fact they're being
subtracted out and they don't impact the
rest of the calculation so if you look
at b1.grad it's actually going to be
zero because it's being subtracted out
and doesn't actually have any effect
and so whenever you're using batch
normalization layers then if you have
any weight layers before like a linear
or a comma or something like that you're
better off coming here and just like not
using bias so you don't want to use bias
and then here you don't want to add it
because it's that spurious instead we
have this vast normalization bias here
and that bastionalization bias is now in
charge of the biasing of this
distribution instead of this B1 that we
had here originally
and so basically the rationalization
layer has its own bias and there's no
need to have a bias in the layer before
it because that bias is going to be
extracted up anyway
so that's the other small detail to be
careful with sometimes it's not going to
do anything catastrophic this B1 will
just be useless it will never get any
gradient it will not learn it will stay
constant and it's just wasteful but it
doesn't actually really impact anything
otherwise okay so I rearranged the code
a little bit with comments and I just
wanted to give a very quick summary of
the bachelorization layer
we are using batch normalization to
control the statistics of activations in
the neural net
it is common to sprinkle batch
normalization layer across the neural
net and usually we will place it after
layers that have multiplications like
for example a linear layer or a
convolutional layer which we may cover
in the future
now the batch normalization internally
has parameters for the gain and the bias
and these are trained using back
propagation
it also has two buffers the buffers are
the mean and the standard deviation the
running mean and the running mean of the
standard deviation
and these are not trained using back
propagation these are trained using this
janky update of kind of like a running
mean update
so
um
these are sort of the parameters and the
buffers of bashram layer and then really
what it's doing is it's calculating the
mean and the standard deviation of the
activations uh that are feeding into the
bathroom layer
over that batch
then it's centering that batch to be
unit gaussian and then it's offsetting
and scaling it by the Learned bias and
Gain
and then on top of that it's keeping
track of the mean and standard deviation
of the inputs
and it's maintaining this running mean
and standard deviation
and this will later be used at inference
so that we don't have to re-estimate the
meanest standard deviation all the time
and in addition that allows us to
basically forward individual examples at
test time
so that's the batch normalization layer
it's a fairly complicated layer
um but this is what it's doing
internally now I wanted to show you a
little bit of a real example
so you can search resnet which is a
residual neural network and these are
context of neural networks used for
image classification
and of course we haven't come dresnets
in detail so I'm not going to explain
all the pieces of it but for now just
note that the image feeds into a resnet
on the top here and there's many many
layers with repeating structure all the
way to predictions of what's inside that
image
this repeating structure is made up of
these blocks and these blocks are just
sequentially stacked up in this deep
neural network
now the code for this the block
basically that's used and repeated
sequentially in series is called this
bottleneck block bottleneck block
and there's a lot here this is all
pytorch and of course we haven't covered
all of it but I want to point out some
small pieces of it
here in the init is where we initialize
the neural net so this coded block here
is basically the kind of stuff we're
doing here we're initializing all the
layers
and in the forward we are specifying how
the neural lot acts once you actually
have the input so this code here is
along the lines of what we're doing here
and now these blocks are replicated and
stacked up serially and that's what a
residual Network would be
and so notice what's happening here com1
these are convolutional layers
and these convolutional layers basically
they're the same thing as a linear layer
except convolutional layers don't apply
um convolutional layers are used for
images and so they have spatial
structure and basically this linear
multiplication and bias offset are done
on patches
instead of a math instead of the full
input so because these images have
structure spatial structure convolutions
just basically do WX plus b but they do
it on overlapping patches of the input
but otherwise it's WX plus b
then we have the norm layer which by
default here is initialized to be a
batch Norm in 2D so two-dimensional
batch normalization layer
and then we have a nonlinearity like
relu so instead of uh here they use relu
we are using 10h in this case
but both both are just nonlinearities
and you can just use them relatively
interchangeably from very deep networks
relu is typically empirically work a bit
better
so see the motif that's being repeated
here we have convolution batch
normalization convolution patch
normalization early Etc and then here
this is residual connection that we
haven't covered yet
but basically that's the exact same
pattern we have here we have a weight
layer like a convolution or like a
linear layer batch normalization and
then 10h which is a nonlinearity but
basically a weight layer a normalization
layer and a nonlinearity and that's the
motif that you would be stacking up when
you create these deep neural networks
exactly as it's done here and one more
thing I'd like you to notice is that
here when they are initializing the comp
layers like comp one by one the depth
for that is right here
and so it's initializing an nn.cap2d
which is a convolutional layer in
pytorch and there's a bunch of keyword
arguments here that I'm not going to
explain yet but you see how there's bias
equals false the bicycles fall is
exactly for the same reason as bias is
not used in our case the CRI race to use
a bias and these are bias is spurious
because after this weight layer there's
a bachelorization and the bachelor
normalization subtracts that bias and
then has its own bias so there's no need
to introduce these spurious parameters
it wouldn't hurt performance it's just
useless
and so because they have this motif of
calf pasture and relu they don't need to
buy us here because there's a bias
inside here
so
by the way this example here is very
easy to find just do resnet pie torch
and uh it's this example here so this is
kind of like the stock implementation of
a residual neural network in pytorch and
you can find that here but of course I
haven't covered many of these parts yet
and I would also like to briefly descend
into the definitions of these pytorch
layers and the parameters that they take
now instead of a convolutional layer
we're going to look at a linear layer
uh because that's the one that we're
using here this is a linear layer and I
haven't covered convolutions yet but as
I mentioned convolutions are basically
linear layers except on patches
so a linear layer performs a w x plus b
except here they're calling the W A
transpose
um since is WX plus b very much
like we did here to initialize this
layer you need to know the fan in the
fan out
and that's so that they can initialize
this W this is the fan in and the fan
out so they know how how big the weight
Matrix should be
you need to also pass in whether you
whether or not you want a bias and if
you set it to false then no bias will be
inside this layer
um and you may want to do that exactly
like in our case if your layer is
followed by a normalization layer such
as Bachelor
so this allows you to basically disable
bias
now in terms of the initialization if we
swing down here this is reporting the
variables used inside this linear layer
and our linear layer here has two
parameters the weight and the bias in
the same way they have a weight and a
bias
and they're talking about how they
initialize it by default so by default
python initialize your weights by taking
the fan in
and then doing one over Fannin square
root
and then instead of a normal
distribution they are using a uniform
distribution
so it's very much the same thing but
they are using a one instead of five
over three so there's no gain being
calculated here the gain is just one but
otherwise it's exactly one over the
square root of fan in exactly as we have
here
so 1 over the square root of K is the is
the scale of the weights but when they
are drawing the numbers they're not
using a gaussian by default they're
using a uniform distribution by default
and so they draw uniformly from negative
square root of K to square root of K
but it's the exact same thing and the
same motivation from for with respect to
what we've seen in this lecture and the
reason they're doing this is if you have
a roughly gaussian input this will
ensure that out of this layer you will
have a roughly gaussian output and you
you basically achieve that by scaling
the weights by 100 square root of fan in
so that's what this is doing
and then the second thing is the battery
normalization layer so let's look at
what that looks like in pytorch
so here we have a one-dimensional mesh
normalization layer exactly as we are
using here
and there are a number of keyword
arguments going into it as well
so we need to know the number of
features uh for us that is 200 and that
is needed so that we can initialize
these parameters here the gain the bias
and the buffers for the running mean and
standard deviation
then they need to know the value of
Epsilon here and by default this is one
negative five you don't typically change
this too much then they need to know the
momentum and the momentum here as they
explain is basically used for these uh
running mean and running standard
deviation
so by default the momentum here is 0.1
the momentum we are using here in this
example is 0.001
and basically rough you may want to
change this sometimes and roughly
speaking if you have a very large batch
size
then typically what you'll see is that
when you estimate the mean and the
standard deviation
for every single batch size if it's
large enough you're going to get roughly
the same result
and so therefore you can use slightly
higher momentum like 0.1
but for a batch size as small as 32 the
mean understand deviation here might
take on slightly different numbers
because there's only 32 examples we are
using to estimate the mean of standard
deviation so the value is changing
around a lot and if your momentum is 0.1
that that might not be good enough for
this value to settle
and converge to the actual mean and
standard deviation over the entire
training set
and so basically if your batch size is
very small momentum of 0.1 is
potentially dangerous and it might make
it so that the running mean and standard
deviation is thrashing too much during
training and it's not actually
converging properly
uh Alpha and equals true determines
whether dispatch normalization layer has
these learnable affine parameters the uh
the gain and the bias and this is almost
always kept it true I'm not actually
sure why you would want to change this
to false
um
then track running stats is determining
whether or not bachelorization layer of
pytorch will be doing this
and one reason you may you may want to
skip the running stats is because you
may want to for example estimate them at
the end as a stage two like this and in
that case you don't want the batch
normalization layer to be doing all this
extra compute that you're not going to
use
and finally we need to know which device
we're going to run this batch
normalization on a CPU or a GPU and what
the data type should be uh half
Precision single Precision double
precision and so on
so that's the batch normalization layer
otherwise the link to the paper is the
same formula we've implement it and
everything is the same exactly as we've
done here
okay so that's everything that I wanted
to cover for this lecture really what I
wanted to talk about is the importance
of understanding the activations and the
gradients and their statistics in neural
networks and this becomes increasingly
important especially as you make your
neural networks bigger larger and deeper
we looked at the distributions basically
at the output layer and we saw that if
you have two confident mispredictions
because the activations are too messed
up at the last layer you can end up with
these hockey stick losses and if you fix
this you get a better loss at the end of
training because your training is not
doing wasteful work
then we also saw that we need to control
the activations we don't want them to
you know squash to zero or explode to
infinity and because that you can run
into a lot of trouble with all of these
non-linearities in these neural nuts and
basically you want everything to be
fairly homogeneous throughout the neural
net you want roughly gaussian
activations throughout the neural net
let me talk about okay if we would
roughly gaussian activations how do we
scale these weight matrices and biases
during initialization of the neural net
so that we don't get um you know so
everything is as controlled as possible
um so that gave us a large boost in
Improvement and then I talked about how
that strategy is not actually uh
possible for much much deeper neural
Nets because when you have much deeper
neural nuts with lots of different types
of layers it becomes really really hard
to precisely set the weights and the
biases in such a way that the
activations are roughly uniform
throughout the neural net
so then I introduced the notion of the
normalization layer now there are many
normalization layers that people use in
practice batch normalization layer
normalization constant normalization
group normalization we haven't covered
most of them but I've introduced the
first one and also the one that I
believe came out first and that's called
batch normalization
and we saw how batch normalization works
this is a layer that you can sprinkle
throughout your deep neural nut and the
basic idea is if you want roughly
gaussian activations well then take your
activations and take the mean understand
deviation and Center your data and you
can do that because the centering
operation is differentiable
but and on top of that we actually had
to add a lot of bells and whistles and
that gave you a sense of the
complexities of the patch normalization
layer because now we're centering the
data that's great but suddenly we need
the gain and the bias and now those are
trainable
and then because we are coupling all the
training examples now suddenly the
question is how do you do the inference
or to do to do the inference we need to
now estimate these mean and standard
deviation once or the entire training
set and then use those at inference but
then no one likes to do stage two so
instead we fold everything into the
batch normalization layer during
training and try to estimate these in
the running manner so that everything is
a bit simpler
and that gives us the batch
normalization layer
um and as I mentioned no one likes this
layer it causes a huge amount of bugs
um and intuitively it's because it is
coupling examples in the forward pass of
a neural net and I've shot myself in the
foot with this layer over and over again
in my life and I don't want you to
suffer the same
uh so basically try to avoid it as much
as possible uh some of the other
alternatives to these layers are for
example group normalization or layer
normalization and those have become more
common uh in more recent deep learning
but we haven't covered those yet but
definitely batch normalization was very
influential at the time when it came out
in roughly 2015 because it was kind of
the first time that you could train
reliably uh much deeper neural nuts and
fundamentally the reason for that is
because this layer was very effective at
controlling the statistics of the
activations in a neural net
so that's the story so far and um that's
all I wanted to cover and in the future
lectures hopefully we can start going
into recurring neural Nets and recurring
neural Nets as we'll see are just very
very deep networks because you uh you
unroll the loop and uh when you actually
optimize these neurons and that's where
a lot of this
um
analysis around the activation
statistics and all these normalization
layers will become very very important
for a good performance so we'll see that
next time
okay so I lied I would like us to do one
more summary here as a bonus and I think
it's useful as to have one more summary
of everything I've presented in this
lecture but also I would like us to
start by tortifying our code a little
bit so it looks much more like what you
would encounter in pi torch so you'll
see that I will structure our code into
these modules like a linear module and a
bachelor module and I'm putting the code
inside these modules so that we can
construct neural networks very much like
we would construct them in pytorch and I
will go through this in detail so we'll
create our neural net
then we will do the optimization loop as
we did before
and then the one more thing that I want
to do here is I want to look at the
activation statistics both in the
forward pass and in the backward pass
and then here we have the evaluation and
sampling just like before
so let me rewind all the way up here and
go a little bit slower
so here I'm creating a linear layer
you'll notice that torch.nn has lots of
different types of layers and one of
those layers is the linear layer
linear takes a number of input features
output features whether or not we should
have bias and then the device that we
want to place this layer on and the data
type so I will omit these two but
otherwise we have the exact same thing
we have the fan in which is number of
inputs fan out the number of outputs and
whether or not we want to use a bias and
internally inside this layer there's a
weight and a bias if you like it
it is typical to initialize the weight
using say random numbers drawn from a
gaussian and then here's the coming
initialization that we've discussed
already in this lecture and that's a
good default and also the default that I
believe python trees is and by default
the bias is usually initialized to zeros
now when you call this module this will
basically calculate W Times X plus b if
you have NB
and then when you also call that
parameters on this module it will return
the tensors that are the parameters of
this layer
now next we have the bachelorization
layer so I've written that here and this
is very similar to Pi torch NN dot bash
Norm 1D layer as shown here
so I'm kind of taking these three
parameters here the dimensionality the
Epsilon that we'll use in the division
and the momentum that we will use in
keeping track of these running stats the
running mean and the running variance
um now pack torch actually takes quite a
few more things but I'm assuming some of
their settings so for us I find will be
true that means that we will be using a
gamma and beta after the normalization
the track running stats will be true so
we will be keeping track of the running
mean and the running variance in the in
the pattern
our device by default is the CPU and the
data type by default is a float
float32.
so those are the defaults otherwise we
are taking all the same parameters in
this bathroom layer so first I'm just
saving them
now here's something new there's a DOT
training which by default is true in
packtorch and then modules also have
this attribute that training and that's
because many modules and batch Norm is
included in that have a different
behavior of whether you are training
your own lot and or whether you are
running it in an evaluation mode and
calculating your evaluation loss or
using it for inference on some test
examples
and masterm is an example of this
because when we are training we are
going to be using the mean and the
variance estimated from the current
batch but during inference we are using
the running mean and running variants
and so also if we are training we are
updating mean and variants but if we are
testing then these are not being updated
they're kept fixed
and so this flag is necessary and by
default true just like impact torch
now the parameters investment 1D are the
gamma and the beta here
and then the running mean and running
variants are called buffers in pytorch
nomenclature and these buffers are
trained using exponential moving average
here explicitly and they are not part of
the back propagation and stochastic
gradient descent so they are not sort of
like parameters of this layer and that's
why when we calculate when we have a
parameters here we only return gamma and
beta we do not return the mean and the
variance this is trained sort of like
internally here every forward pass using
exponential moving average
so that's the initialization
now in a forward pass if we are training
then we use the mean and the variance
estimated by the batch let me plot the
paper here
we calculate the mean and the variance
now up above I was estimating the
standard deviation and keeping track of
the standard deviation here in the
running standard deviation instead of
running variance but let's follow the
paper exactly here they calculate the
variance which is the standard deviation
squared and that's what's kept track of
in the running variance instead of a
running standard deviation
uh but those two would be very very
similar I believe
if we are not training then we use
running mean in various we normalize
and then here I'm calculating the output
of this layer and I'm also assigning it
to an attribute called dot out
now dot out is something that I'm using
in our modules here this is not what you
would find in pytorch we are slightly
deviating from it I'm creating a DOT out
because I would like to very easily
maintain all those variables so that we
can create statistics of them and plot
them but Pi torch and modules will not
have a data attribute
then finally here we are updating the
buffers using again as I mentioned
exponential moving average
uh provide given the provided momentum
and importantly you'll notice that I'm
using the torstart no grad context
manager and I'm doing this because if we
don't use this then pytorch will start
building out an entire computational
graph out of these tensors because it is
expecting that we will eventually call
it that backward but we are never going
to be calling that backward on anything
that includes running mean and running
variance so that's why we need to use
this contact manager so that we are not
sort of maintaining them using all this
additional memory so this will make it
more efficient and it's just telling
factors that will only know backward we
just have a bunch of tensors we want to
update them that's it
and then we return
okay now scrolling down we have the 10h
layer this is very very similar to
torch.10h and it doesn't do too much it
just calculates 10h as you might expect
so that's torch.nh and there's no
parameters in this layer
but because these are layers
um it now becomes very easy to sort of
like stack them up into basically just a
list
and we can do all the initializations
that we're used to so we have the
initial sort of embedding Matrix we have
our layers and we can call them
sequentially
and then again with Trump shot no grad
there's some initializations here so we
want to make the output softmax a bit
less confident like we saw and in
addition to that because we are using a
six layer multi-layer perceptron here so
you see how I'm stacking linear 10 age
linear 10h Etc
I'm going to be using the game here and
I'm going to play with this in a second
so you'll see how when we change this
what happens to the statistics
finally the primers are basically the
embedding Matrix and all the parameters
in all the layers and notice here I'm
using a double list comprehension if you
want to call it that but for every layer
in layers and for every parameter in
each of those layers we are just
stacking up all those piece all those
parameters
now in total we have 46 000 parameters
and I'm telling by George that all of
them require gradient
then here we have everything here we are
actually mostly used to we are sampling
batch we are doing forward pass the
forward pass now is just a linear
application of all the layers in order
followed by the cross entropy
and then in the backward path you'll
notice that for every single layer I now
iterate over all the outputs and I'm
telling pytorch to retain the gradient
of them
and then here we are already used to all
the all the gradients set To None do the
backward to fill in the gradients do an
update using the caskaranian scent and
then track some statistics and then I am
going to break after a single iteration
now here in this cell in this diagram
I'm visualizing the histogram the
histograms of the forward pass
activations and I'm specifically doing
it at the 10 inch layers
so iterating over all the layers except
for the very last one which is basically
just the soft Max layer
um if it is a 10 inch layer and I'm
using a 10 inch layer just because they
have a finite output negative one to one
and so it's very easy to visualize here
so you see negative one to one and it's
a finite range and easy to work with
I take the out tensor from that layer
into T and then I'm calculating the mean
the standard deviation and the percent
saturation of t
and the way I Define the percent
saturation is that t dot absolute value
is greater than 0.97 so that means we
are here at the Tails of the 10h and
remember that when we are in the Tails
of the 10h that will actually stop
gradients so we don't want this to be
too high
now
here I'm calling torch.histogram and
then I am plotting this histogram so
basically what this is doing is that
every different type of layer and they
all have a different color we are
looking at how many
um values in these tensors take on any
of the values Below on this axis here
so the first layer is fairly saturated
here at 20 so you can see that it's got
Tails here but then everything sort of
stabilizes and if we had more layers
here it would actually just stabilize at
around the standard deviation of about
0.65 and the saturation would be roughly
five percent
and the reason that this stabilizes and
gives us a nice distribution here is
because gain is set to 5 over 3.
now here this gain you see that by
default we initialize with one over
square root of fan in but then here
during initialization I come in and I
iterate all the layers and if it's a
linear layer I boost that by the gain
now we saw that one so basically if we
just do not use a gain then what happens
if I redraw this you will see that
the standard deviation is shrinking and
the saturation is coming to zero and
basically what's happening is the first
layer is you know pretty decent but then
further layers are just kind of like
shrinking down to zero and it's
happening slowly but it's shrinking to
zero and the reason for that is when you
just have a sandwich of linear layers
alone then a then initializing our
weights in this manner we saw previously
would have conserved the standard
deviation of one
but because we have this interspersed
10h layers in there
the Stanley layers are squashing
functions and so they take your
distribution and they slightly squash it
and so some gain is necessary to keep
expanding it to fight the squashing
so it just turns out that 5 over 3 is a
good value so if we have something too
small like one we saw that things will
come towards zero but if it's something
too high let's do two
then here we see that
um
well let me do something a bit more
extreme because so it's a bit more
visible let's try three
okay so we see here that the saturations
are trying to be way too large
okay so three would create way too
saturated activations
so five over three is a good setting for
a sandwich of linear layers with 10 inch
activations and it roughly stabilizes
the standard deviation at a reasonable
point
now honestly I have no idea where five
over three came from in pytorch when we
were looking at the coming
initialization I see empirically that it
stabilizes this sandwich of linear n10h
and that the saturation is in a good
range but I don't actually know this
came out of some math formula I tried
searching briefly for where this comes
from but I wasn't able to find anything
but certainly we see that empirically
these are very nice ranges our
saturation is roughly five percent which
is a pretty good number and uh this is a
good setting of The gain in this context
similarly we can do the exact same thing
with the gradients so here is a very
same Loop if it's a 10h but instead of
taking the layered that out I'm taking
the grad and then I'm also showing the
mean on the standard deviation and I'm
plotting the histogram of these values
and so you'll see that the gradient
distribution is fairly reasonable and in
particular what we're looking for is
that all the different layers in this
sandwich has roughly the same gradient
things are not shrinking or exploding
so we can for example come here and we
can take a look at what happens if this
gain was way too small so this was 0.5
then you see the
first of all the activations are
shrinking to zero but also the gradients
are doing something weird the gradient
started out here and then now they're
like expanding out
and similarly if we for example have a
too high again so like three
then we see that also the gradients have
there's some asymmetry going on where as
you go into deeper and deeper layers the
activations are also changing and so
that's not what we want and in this case
we saw that without the use of Bachelor
as we are going through right now we
have to very carefully set those gains
to get nice activations in both the
forward pass and the backward pass now
before we move on to pasture
normalization I would also like to take
a look at what happens when we have no
10h units here so erasing all the 10
inch nonlinearities
but keeping the gain at 5 over 3. we now
have just a giant linear sandwich so
let's see what happens to the
activations
as we saw before the correct gain here
is one that is the standard deviation
preserving gain so
1.667 is too high and so what's going to
happen now is the following
I have to change this to be linear so we
are because there's no more 10 inch
layers
and let me change this to linear as well
so what we're seeing is
um the activations started out on the
blue and have by layer 4 become very
diffuse so what's happening to the
activations is this
and with the gradients on the top layer
the activation the gradient statistics
are the purple and then they diminish as
you go down deeper in the layers and so
basically you have an asymmetry like in
the neural net and you might imagine
that if you have very deep neural
networks say like 50 layers or something
like that this just this is not a good
place to be so that's why before bash
normalization this was incredibly tricky
to to set in particular if this is too
large of a game this happens and if it's
too little it can gain
then this happens also the opposite of
that basically happens here we have a um
shrinking and a diffusion depending on
which direction you look at it from
and so certainly this is not what you
want and in this case the correct
setting of The gain is exactly one
just like we're doing at initialization
and then we see that
the statistics for the forward and the
backward pass are well behaved and so
the reason I want to show you this is
the basically like getting neuralness to
train before these normalization layers
and before the use of advanced
optimizers like atom which we still have
to cover and residual connections and so
on training neurons basically look like
this it's like a total Balancing Act you
have to make sure that everything is
precisely orchestrated and you have to
care about the activations and the
gradients and their statistics and then
maybe you can train something but it was
basically impossible to train very deep
networks and this is fundamentally the
reason for that you'd have to be very
very careful with your initialization
um the other point here is you might be
asking yourself by the way I'm not sure
if I covered this why do we need these
10h layers at all why do we include them
and then have to worry about the gain
and uh the reason for that of course is
that if you just have a stack of linear
layers
then certainly we're getting very easily
nice activations and so on but this is
just a massive linear sandwich and it
turns out that it collapses to a single
linear layer in terms of its
representation power so if you were to
plot the output as a function of the
input you're just getting a linear
function no matter how many linear
layers you stack up you still just end
up with a linear transformation all the
W X Plus B's just collapse into a large
WX plus b with slightly different W's as
likely different B
um but interestingly even though the
forward pass collapses to just a linear
layer because of back propagation and
the Dynamics of the backward pass the
optimization is really is not identical
you actually end up with all kinds of
interesting
um
Dynamics in the backward pass because of
the uh the way the chain rule is
calculating it and so optimizing a
linear layer by itself and optimizing a
sandwich of 10 millionaire layers in
both cases those are just a linear
transformation in the forward pass but
the training Dynamics would be different
and there's entire papers that analyze
in fact like infinitely layered linear
layers and so on and so there's a lot of
things too that you can play with there
uh but basically the technical
linearities allow us to
um
turn this sandwich from just a linear
function into a neural network that can
in principle approximate any arbitrary
function
okay so now I've reset the code to use
the linear 10h sandwich like before and
I reset everything so the gains five
over three we can run a single step of
optimization and we can look at the
activation statistics of the forward
pass and the backward pass
but I've added one more plot here that I
think is really important to look at
when you're training your neural nuts
and to consider and ultimately what
we're doing is we're updating the
parameters of the neural net so we care
about the parameters and their values
and their gradients
so here what I'm doing is I'm actually
iterating over all the parameters
available and then I'm only
um restricting it to the two-dimensional
parameters which are basically the
weights of these linear layers and I'm
skipping the biases and I'm skipping the
um Gammas and the betas in the bathroom
just for Simplicity
but you can also take a look at those as
well but what's happening with the
weights is um instructive by itself
so here we have all the different
weights their shapes so this is the
embedding layer the first linear layer
all the way to the very last linear
layer and then we have the mean the
standard deviation of all these
parameters
the histogram and you can see that it
actually doesn't look that amazing so
there's some trouble in Paradise even
though these gradients looked okay
there's something weird going on here
I'll get to that in a second and the
last thing here is the gradient to data
ratio so sometimes I like to visualize
this as well because what this gives you
a sense of is what is the scale of the
gradient compared to the scale of the
actual values and this is important
because we're going to end up taking a
step update
that is the learning rate times the
gradient onto the data and so the
gradient has two large of magnitude if
the numbers in there are too large
compared to the numbers in data then
you'd be in trouble
but in this case the gradient to data is
our loan numbers so the values inside
grad are 1000 times smaller than the
values inside data in these weights most
of them
now notably that is not true about the
last layer and so the last layer
actually here the output layer is a bit
of a troublemaker in the way that this
is currently arranged because you can
see that the um
the last layer here in pink takes on
values that are much larger than some of
the values inside
um inside the neural net so the standard
deviations are roughly 1 and negative
three throughout except for the last
last layer which actually has roughly
one e negative two a standard deviation
of gradients and so the gradients on the
last layer are currently about 100 times
greater sorry 10 times greater than all
the other weights inside the neural nut
and so that's problematic because in the
simple stochastically in the sun setup
you would be training this last layer
about 10 times faster than you would be
training the other layers at
initialization
now this actually like kind of fixes
itself a little bit if you train for a
bit longer so for example if I greater
than 1000 only then do a break
let me reinitialize and then let me do
it 1000 steps and after 1000 steps we
can look at the forward pass
okay so you see how the neurons are a
bit are saturating a bit and we can also
look at the backward pass but otherwise
they look good they're about equal and
there's no shrinking to zero or
exploding to infinities
and you can see that here in the weights
things are also stabilizing a little bit
so the Tails of the last pink layer are
actually coming down coming in during
the optimization
but certainly this is like a little bit
of troubling especially if you are using
a very simple update rule like
stochastic gradient descent instead of a
modern Optimizer like Adam
now I'd like to show you one more plot
that I usually look at when I train
neural networks and basically the
gradient to data ratio is not actually
that informative because what matters at
the end is not the gradient to date
ratio but the update to the data ratio
because that is the amount by which we
will actually change the data in these
tensors
so coming up here what I'd like to do is
I'd like to introduce a new update to
data ratio
it's going to be less than we're going
to build it out every single iteration
and here I'd like to keep track of
basically
the ratio
every single iteration
so
without any gradients I'm comparing the
update which is learning rate times the
times the gradient
that is the update that we're going to
apply to every parameter
social Mediterranean or all the
parameters and then I'm taking the
basically standard deviation of the
update we're going to apply and divide
it by the actual content the data of
that parameter and its standard
deviation
so this is the ratio of basically how
great are the updates to the values in
these tensors
then we're going to take a log of it and
actually I'd like to take a log 10.
um
just so it's a nicer visualization so
we're going to be basically looking at
the exponents of the
of this division here and then that item
to pop out the float and we're going to
be keeping track of this for all the
parameters and adding it to this UD
tensor
so now let me reinitialize and run a
thousand iterations
we can look at the activations the
gradients and the parameter gradients as
we did before but now I have one more
plot here to introduce
now what's happening here is where every
interval go to parameters and I'm
constraining it again like I did here to
just the weights
so the number of dimensions in these
sensors is two and then I'm basically
plotting all of these update ratios over
time
so when I plot this
I plot those ratios and you can see that
they evolve over time during
initialization to take on certain values
and then these updates sort of like
start stabilizing usually during
training
then the other thing that I'm plotting
here is I'm plotting here like an
approximate value that is a Rough Guide
for what it roughly should be and it
should be like roughly one in negative
three
and so that means that basically there's
some values in this tensor
um and they take on certain values and
the updates to them at every single
iteration are no more than roughly 1 000
of the actual like magnitude in those
tensors uh if this was much larger like
for example if this was
um
if the log of this was like say negative
one this is actually updating those
values quite a lot they're undergoing a
lot of change
but the reason that the final rate the
final layer here is an outlier is
because this layer was artificially
shrunk down to keep the soft max income
unconfident
so here
you see how we multiply The Weight by
point one
uh in the initialization to make the
last layer prediction less confident
that made that artificially made the
values inside that tensor way too low
and that's why we're getting temporarily
a very high ratio but you see that that
stabilizes over time once that weight
starts to learn starts to learn
but basically I like to look at the
evolution of this update ratio for all
my parameters usually and I like to make
sure that it's not too much above
wanting negative three roughly
uh so around negative three on this log
plot
if it's below negative three usually
that means that the parameters are not
training fast enough
so if our learning rate was very low
let's do that experiment
let's initialize and then let's actually
do a learning rate of say y negative
three here
so 0.001
if you're learning rate is way too low
this plot will typically reveal it
so you see how all of these updates are
way too small so the size of the update
is basically uh 10 000 times
in magnitude to the size of the numbers
in that tensor in the first place so
this is a symptom of training way too
slow
so this is another way to sometimes set
the learning rate and to get a sense of
what that learning rate should be and
ultimately this is something that you
would keep track of
if anything the learning rate here is a
little bit on the higher side because
you see that
um we're above the black line of
negative three we're somewhere around
negative 2.5 it's like okay and uh but
everything is like somewhat stabilizing
and so this looks like a pretty decent
setting of of
um learning rates and so on but this is
something to look at and when things are
miscalibrated you will you will see very
quickly so for example
everything looks pretty well behaved
right but just as a comparison when
things are not properly calibrated what
does that look like let me come up here
and let's say that for example uh what
do we do
let's say that we forgot to apply this
fan in normalization so the weights
inside the linear layers are just a
sample for my gaussian in all those
stages
what happens to our how do we notice
that something's off
well the activation plot will tell you
whoa your neurons are way too saturated
the gradients are going to be all messed
up
the histogram for these weights are
going to be all messed up as well and
there's a lot of asymmetry and then if
we look here I suspect it's all going to
be also pretty messed up so you see
there's a lot of discrepancy in how fast
these layers are learning and some of
them are learning way too fast so
negative one negative 1.5 those aren't
very large numbers in terms of this
ratio again you should be somewhere
around negative three and not much more
about that so this is how miscalibration
so if your neural nuts are going to
manifest and these kinds of plots here
are a good way of
um sort of bringing those
miscalibrations sort of uh
to your attention and so you can address
them okay so so far we've seen that when
we have this linear 10h sandwich we can
actually precisely calibrate the gains
and make the activations the gradients
and the parameters and the updates all
look pretty decent but it definitely
feels a little bit like balancing
of a pencil on your finger and that's
because this gain has to be very
precisely calibrated
so now let's introduce batch
normalization layers into the fix into
the mix and let's let's see how that
helps fix the problem
so here
I'm going to take the bachelor Monday
class
and I'm going to start placing it inside
and as I mentioned before the standard
typical place you would place it is
between the linear layer so right after
it but before the nonlinearity but
people have definitely played with that
and uh in fact you can get very similar
results even if you place it after the
nonlinearity
and the other thing that I wanted to
mention is it's totally fine to also
place it at the end after the last
linear layer and before the loss
function so this is potentially fine as
well
and in this case this would be output
would be world cup size
um now because the last layer is
mushroom we would not be changing the
weight to make the softmax less
confident we'd be changing the gamma
because gamma remember in the bathroom
is the variable that multiplicatively
interacts with the output of that
normalization
so we can initialize this sandwich now
and we can train and we can see that the
activations are going to of course look
very good and they are going to
necessarily look good because now before
every single 10 H layer there is a
normalization in The Bachelor so this is
unsurprisingly all looks pretty good
it's going to be standard deviation of
roughly 0.65 two percent and roughly
equal standard deviation throughout the
entire layers so everything looks very
homogeneous
the gradients look good the weights look
good and they're distributions
and then the updates
also look pretty reasonable we're going
above negative three a little bit but
not by too much so all the parameters
are training in roughly the same rate
here
but now what we've gained is we are
going to be slightly less
um
brittle with respect to the gain of
these so for example I can make the gain
be say 0.2 here
um
which was much much slower than what we
had with the 10h
but as we'll see the activations will
actually be exactly unaffected and
that's because of again this explicit
normalization the gradients are going to
look okay the weight gradients are going
to look okay but actually the updates
will change
and so
even though the forward and backward
pass to a very large extent look okay
because of the backward pass of the
batch form and how the scale of the
incoming activations interacts in the
basharm and its backward pass this is
actually changing the um
the scale of the updates on these
parameters so the grades and ingredients
of these weights are affected
so we still don't get a completely free
pass to pass an arbitrary weights here
but it everything else is significantly
more robust in terms of the forward
backward and the weight gradients it's
just that you may have to retune your
learning rate if you are changing
sufficiently the the scale of the
activations that are coming into the
bachelor's so here for example this we
changed the gains of these linear layers
to be greater and we're seeing that the
updates are coming out lower as a result
and then finally we can also if we are
using basharms we don't actually need to
necessarily let me reset this to one so
there's no gain we don't necessarily
even have to
um normalize by fan in sometimes so if I
take out the fan in so these are just
now uh random gaussian
we'll see that because of batch drum
this will actually be relatively well
behaved so
this this is look of course in the
forward pass look good the gradients
look good
the backward the weight updates look
okay A little bit of fat tails in some
of the layers
and this looks okay as well but as you
as you can see uh we're significantly
below negative three so we'd have to
bump up the learning rate of this
bachelor so that we are training more
properly and in particular looking at
this roughly looks like we have to 10x
the learning rate to get to about 20
negative three
so we come here and we would change this
to be update of 1.0
and if when I reinitialize
then we'll see that everything still of
course looks good
and now we are roughly here and we
expect this to be an okay training run
so long story short we are significantly
more robust to the gain of these linear
layers whether or not we have to apply
the fan in and then we can change the
gain but we actually do have to worry a
little bit about the update
um scales and making sure that the
learning rate is properly calibrated
here but thus the activations of the
forward backward pass and the updates
are all are looking significantly more
well-behaved except for the global scale
that is potentially being adjusted here
okay so now let me summarize there are
three things I was hoping to achieve
with this section number one I wanted to
introduce you to batch normalization
which is one of the first modern
innovations that we're looking into that
helped stabilize very deep neural
networks and their training and I hope
you understand how the bachelorization
works and how it would be used in a
neural network
number two I was hoping to pie tortify
some of our code and wrap it up into
these modules so like linear Bachelor 1D
10h Etc these are layers or modules and
they can be stacked up into neural Nets
like Lego building blocks and these
layers actually exist in pie torch and
if you import torch and then you can
actually the way I've constructed it you
can simply just use pytorch by
prepending and then dot to all these
different layers
and actually everything will just work
because the API that I've developed here
is identical to the API that pytorch
uses and the implementation also is
basically as far as I'm aware identical
to the one in pi torch
and number three I try to introduce you
to the diagnostic tools that you would
use to understand whether your neural
network is in a good State dynamically
so we are looking at the statistics and
histograms and activation of the forward
pass application activations the
backward pass gradients and then also
we're looking at the weights that are
going to be updated as part of
stochastic already in ascent and we're
looking at their means standard
deviations and also the ratio of
gradients to data or even better the
updates to data and we saw that
typically we don't actually look at it
as a single snapshot Frozen in time at
some particular iteration typically
people look at this as uh over time just
like I've done here and they look at
these updated data ratios and they make
sure everything looks okay and in
particular I said that
um running negative 3 or basically
negative 3 on the log scale is a good
rough heuristic for what you want this
ratio to be and if it's way too high
then probably the learning rate or the
updates are a little too too big and if
it's way too small that the learning
rate is probably too small
so that's just some of the things that
you may want to play with when you try
to get your neural network to work with
very well
now there's a number of things I did not
try to achieve I did not try to beat our
previous performance as an example by
introducing the bathroom layer actually
I did try
um and I found the new I used the
learning rate finding mechanism that
I've described before I tried to train
the bathroom layer a bachelor neural nut
and I actually ended up with results
that are very very similar to what we've
obtained before
and that's because our performance now
is not bottlenecked by the optimization
which is what bass Norm is helping with
the performance at this stage is
bottlenecked by what I suspect is the
context length of our context
So currently we are taking three
characters to predict the fourth one and
I think we need to go beyond that and we
need to look at more powerful
architectures that are like recurrent
neural networks and Transformers in
order to further push
um the log probabilities that we're
achieving on this data set
and I also did not try to have a full
explanation of all of these activations
the gradients and the backward paths and
the statistics of all these gradients
and so you may have found some of the
parts here unintuitive and maybe you're
slightly confused about okay if I change
the gain here how come that we need a
different learning rate and I didn't go
into the full detail because you'd have
to actually look at the backward pass of
all these different layers and get an
intuitive understanding of how that
works and I did not go into that in this
lecture the purpose really was just to
introduce you to the diagnostic tools
and what they look like but there's
still a lot of work remaining on the
intuitive level to understand the
initialization the backward pass and how
all of that interacts but you shouldn't
feel too bad because honestly we are
getting to The Cutting Edge of where the
field is we certainly haven't I would
say solved initialization and we haven't
solved back propagation and these are
still very much an active area of
research people are still trying to
figure out what's the best way to
initialize these networks what is the
best update rule to use
um and so on so none of this is really
solved and we don't really have all the
answers to all the uh to you know all
these cases but at least you know we're
making progress at least we have some
tools to tell us whether or not things
are on the right track for now
so
I think we've made positive progress in
this lecture and I hope you enjoyed that
and I will see you next timehi everyone so today we are once again
continuing our implementation of make
more now so far we've come up to here
montalia perceptrons and our neural net
looked like this and we were
implementing this over the last few
lectures
now I'm sure everyone is very excited to
go into recurring neural networks and
all of their variants and how they work
and the diagrams look cool and it's very
exciting and interesting and we're going
to get a better result but unfortunately
I think we have to remain here for one
more lecture and the reason for that is
we've already trained this multilio
perceptron right and we are getting
pretty good loss and I think we have a
pretty decent understanding of the
architecture and how it works but the
line of code here that I take an issue
with is here lost up backward that is we
are taking a pytorch auto grad and using
it to calculate all of our gradients
along the way and I would like to remove
the use of lost at backward and I would
like us to write our backward pass
manually on the level of tensors and I
think that this is a very useful
exercise for the following reasons
I actually have an entire blog post on
this topic but I'd like to call back
propagation a leaky abstraction
and what I mean by that is back
propagation does doesn't just make your
neural networks just work magically it's
not the case they can just Stack Up
arbitrary Lego blocks of differentiable
functions and just cross your fingers
and back propagate and everything is
great things don't just work
automatically it is a leaky abstraction
in the sense that you can shoot yourself
in the foot if you do not understanding
its internals it will magically not work
or not work optimally and you will need
to understand how it works under the
hood if you're hoping to debug it and if
you are hoping to address it in your
neural nut
um so this blog post here from a while
ago goes into some of those examples so
for example we've already covered them
some of them already for example the
flat tails of these functions and how
you do not want to saturate them too
much because your gradients will die the
case of dead neurons which I've already
covered as well
the case of exploding or Vanishing
gradients in the case of repair neural
networks which we are about to cover
and then also you will often come across
some examples in the wild
this is a snippet that I found uh in a
random code base on the internet where
they actually have like a very subtle
but pretty major bug in their
implementation and the bug points at the
fact that the author of this code does
not actually understand by propagation
so they're trying to do here is they're
trying to clip the loss at a certain
maximum value but actually what they're
trying to do is they're trying to
collect the gradients to have a maximum
value instead of trying to clip the loss
at a maximum value and
um indirectly they're basically causing
some of the outliers to be actually
ignored because when you clip a loss of
an outlier you are setting its gradient
to zero and so have a look through this
and read through it but there's
basically a bunch of subtle issues that
you're going to avoid if you actually
know what you're doing and that's why I
don't think it's the case that because
pytorch or other Frameworks offer
autograd it is okay for us to ignore how
it works
now we've actually already covered
covered autograd and we wrote micrograd
but micrograd was an autograd engine
only on the level of individual scalars
so the atoms were single individual
numbers and uh you know I don't think
it's enough and I'd like us to basically
think about back propagation on level of
tensors as well and so in a summary I
think it's a good exercise I think it is
very very valuable you're going to
become better at debugging neural
networks and making sure that you
understand what you're doing it is going
to make everything fully explicit so
you're not going to be nervous about
what is hidden away from you and
basically in general we're going to
emerge stronger and so let's get into it
a bit of a fun historical note here is
that today writing your backward pass by
hand and manually is not recommended and
no one does it except for the purposes
of exercise but about 10 years ago in
deep learning this was fairly standard
and in fact pervasive so at the time
everyone used to write their own
backward pass by hand manually including
myself and it's just what you would do
so we used to ride backward pass by hand
and now everyone just calls lost that
backward uh we've lost something I want
to give you a few examples of this so
here's a 2006 paper from Jeff Hinton and
Russell selectinov in science that was
influential at the time and this was
training some architectures called
restricted bolstery machines and
basically it's an auto encoder trained
here and this is from roughly 2010 I had
a library for training researchable
machines and this was at the time
written in Matlab so python was not used
for deep learning pervasively it was all
Matlab and Matlab was this a scientific
Computing package that everyone would
use so we would write Matlab which is
barely a programming language as well
but I've had a very convenient tensor
class and was this a Computing
environment and you would run here it
would all run on a CPU of course but you
would have very nice plots to go with it
and a built-in debugger and it was
pretty nice now the code in this package
in 2010 that I wrote for fitting
research multiple machines to a large
extent is recognizable but I wanted to
show you how you would well I'm creating
the data in the XY batches I'm
initializing the neural nut so it's got
weights and biases just like we're used
to and then this is the training Loop
where we actually do the forward pass
and then here at this time they didn't
even necessarily use back propagation to
train neural networks so this in
particular implements contrastive
Divergence which estimates a gradient
and then here we take that gradient and
use it for a parameter update along the
lines that we're used to
um yeah here
but you can see that basically people
are meddling with these gradients uh
directly and inline and themselves uh it
wasn't that common to use an auto grad
engine here's one more example from a
paper of mine from 2014
um called the fragmented embeddings
and here what I was doing is I was
aligning images and text
um and so it's kind of like a clip if
you're familiar with it but instead of
working on the level of entire images
and entire sentences it was working on
the level of individual objects and
little pieces of sentences and I was
embedding them and then calculating very
much like a clip-like loss and I dig up
the code from 2014 of how I implemented
this and it was already in numpy and
python
and here I'm planting the cost function
and it was standard to implement not
just the cost but also the backward pass
manually so here I'm calculating the
image embeddings sentence embeddings the
loss function I calculate this course
this is the loss function and then once
I have the loss function I do the
backward pass right here so I backward
through the loss function and through
the neural nut and I append
regularization so everything was done by
hand manually and you were just right
out the backward pass and then you would
use a gradient Checker to make sure that
your numerical estimate of the gradient
agrees with the one you calculated
during back propagation so this was very
standard for a long time but today of
course it is standard to use an auto
grad engine
um but it was definitely useful and I
think people sort of understood how
these neural networks work on a very
intuitive level and so I think it's a
good exercise again and this is where we
want to be okay so just as a reminder
from our previous lecture this is The
jupyter Notebook that we implemented at
the time and
we're going to keep everything the same
so we're still going to have a two layer
multiplayer perceptron with a batch
normalization layer so the forward pass
will be basically identical to this
lecture but here we're going to get rid
of lost and backward and instead we're
going to write the backward pass
manually
now here's the starter code for this
lecture we are becoming a back prop
ninja in this notebook
and the first few cells here are
identical to what we are used to so we
are doing some imports loading the data
set and processing the data set none of
this changed
now here I'm introducing a utility
function that we're going to use later
to compare the gradients so in
particular we are going to have the
gradients that we estimate manually
ourselves and we're going to have
gradients that Pi torch calculates and
we're going to be checking for
correctness assuming of course that
pytorch is correct
um then here we have the initialization
that we are quite used to so we have our
embedding table for the characters the
first layer second layer and the batch
normalization in between
and here's where we create all the
parameters now you will note that I
changed the initialization a little bit
uh to be small numbers so normally you
would set the biases to be all zero here
I am setting them to be small random
numbers and I'm doing this because
if your variables are initialized to
exactly zero sometimes what can happen
is that can mask an incorrect
implementation of a gradient
um because uh when everything is zero it
sort of like simplifies and gives you a
much simpler expression of the gradient
than you would otherwise get and so by
making it small numbers I'm trying to
unmask those potential errors in these
calculations
you also notice that I'm using uh B1 in
the first layer I'm using a bias despite
batch normalization right afterwards
um so this would typically not be what
you do because we talked about the fact
that you don't need the bias but I'm
doing this here just for fun
um because we're going to have a
gradient with respect to it and we can
check that we are still calculating it
correctly even though this bias is
asparious
so here I'm calculating a single batch
and then here I'm doing a forward pass
now you'll notice that the forward pass
is significantly expanded from what we
are used to here the forward pass was
just
um here
now the reason that the forward pass is
longer is for two reasons number one
here we just had an F dot cross entropy
but here I am bringing back a explicit
implementation of the loss function
and number two
I've broken up the implementation into
manageable chunks so we have a lot a lot
more intermediate tensors along the way
in the forward pass and that's because
we are about to go backwards and
calculate the gradients in this back
propagation from the bottom to the top
so we're going to go upwards and just
like we have for example the lock props
tensor in a forward pass in the backward
pass we're going to have a d-lock probes
which is going to store the derivative
of the loss with respect to the lock
props tensor and so we're going to be
prepending D to every one of these
tensors and calculating it along the way
of this back propagation
so as an example we have a b and raw
here we're going to be calculating a DB
in raw so here I'm telling pytorch that
we want to retain the grad of all these
intermediate values because here in
exercise one we're going to calculate
the backward pass so we're going to
calculate all these D values D variables
and use the CNP function I've introduced
above to check our correctness with
respect to what pi torch is telling us
this is going to be exercise one uh
where we sort of back propagate through
this entire graph
now just to give you a very quick
preview of what's going to happen in
exercise two and below here we have
fully broken up the loss and back
propagated through it manually in all
the little Atomic pieces that make it up
but here we're going to collapse the
laws into a single cross-entropy call
and instead we're going to analytically
derive using math and paper and pencil
the gradient of the loss with respect to
the logits and instead of back
propagating through all of its little
chunks one at a time we're just going to
analytically derive what that gradient
is and we're going to implement that
which is much more efficient as we'll
see in the in a bit
then we're going to do the exact same
thing for patch normalization so instead
of breaking up bass drum into all the
old tiny components we're going to use
uh pen and paper and Mathematics and
calculus to derive the gradient through
the bachelor Bachelor layer so we're
going to calculate the backward
passthrough bathroom layer in a much
more efficient expression instead of
backward propagating through all of its
little pieces independently
so there's going to be exercise three
and then in exercise four we're going to
put it all together and this is the full
code of training this two layer MLP and
we're going to basically insert our
manual back prop and we're going to take
out lost it backward and you will
basically see that you can get all the
same results using fully your own code
and the only thing we're using from
pytorch is the torch.tensor to make the
calculations efficient but otherwise you
will understand fully what it means to
forward and backward and neural net and
train it and I think that'll be awesome
so let's get to it
okay so I read all the cells of this
notebook all the way up to here and I'm
going to erase this and I'm going to
start implementing backward pass
starting with d lock problems so we want
to understand what should go here to
calculate the gradient of the loss with
respect to all the elements of the log
props tensor
now I'm going to give away the answer
here but I wanted to put a quick note
here that I think would be most
pedagogically useful for you is to
actually go into the description of this
video and find the link to this Jupiter
notebook you can find it both on GitHub
but you can also find Google collab with
it so you don't have to install anything
you'll just go to a website on Google
collab and you can try to implement
these derivatives or gradients yourself
and then if you are not able to come to
my video and see me do it and so work in
Tandem and try it first yourself and
then see me give away the answer and I
think that'll be most valuable to you
and that's how I recommend you go
through this lecture
so we are starting here with d-log props
now d-lock props will hold the
derivative of the loss with respect to
all the elements of log props
what is inside log blobs the shape of
this is 32 by 27. so it's not going to
surprise you that D log props should
also be an array of size 32 by 27
because we want the derivative loss with
respect to all of its elements so the
sizes of those are always going to be
equal
now how how does log props influence the
loss okay loss is negative block probes
indexed with range of N and YB and then
the mean of that now just as a reminder
YB is just a basically an array of all
the correct indices
um so what we're doing here is we're
taking the lock props array of size 32
by 27.
right
and then we are going in every single
row and in each row we are plugging
plucking out the index eight and then 14
and 15 and so on so we're going down the
rows that's the iterator range of N and
then we are always plucking out the
index of the column specified by this
tensor YB so in the zeroth row we are
taking the eighth column in the first
row we're taking the 14th column Etc and
so log props at this plugs out
all those
log probabilities of the correct next
character in a sequence
so that's what that does and the shape
of this or the size of it is of course
32 because our batch size is 32.
so these elements get plugged out and
then their mean and the negative of that
becomes loss
so I always like to work with simpler
examples to understand the numerical
form of derivative what's going on here
is once we've plucked out these examples
um we're taking the mean and then the
negative so the loss basically
I can write it this way is the negative
of say a plus b plus c
and the mean of those three numbers
would be say negative would divide three
that would be how we achieve the mean of
three numbers ABC although we actually
have 32 numbers here
and so what is basically the loss by say
like d a right
well if we simplify this expression
mathematically this is negative one over
three of A and negative plus negative
one over three of B
plus negative 1 over 3 of c and so what
is D loss by D A it's just negative one
over three
and so you can see that if we don't just
have a b and c but we have 32 numbers
then D loss by D
um you know every one of those numbers
is going to be one over N More generally
because n is the um the size of the
batch 32 in this case
so D loss by
um D Lock probs is negative 1 over n
in all these places
now what about the other elements inside
lock problems because lock props is
large array you see that lock problems
at shape is 32 by 27. but only 32 of
them participate in the loss calculation
so what's the derivative of all the
other most of the elements that do not
get plucked out here
while their loss intuitively is zero
sorry they're gradient intuitively is
zero and that's because they did not
participate in the loss
so most of these numbers inside this
tensor does not feed into the loss and
so if we were to change these numbers
then the loss doesn't change which is
the equivalent of way of saying that the
derivative of the loss with respect to
them is zero they don't impact it
so here's a way to implement this
derivative then we start out with
torch.zeros of shape 32 by 27 or let's
just say instead of doing this because
we don't want to hard code numbers let's
do torch.zeros like
block probs so basically this is going
to create an array of zeros exactly in
the shape of log probs
and then we need to set the derivative
of negative 1 over n inside exactly
these locations so here's what we can do
the lock props indexed in The Identical
way
will be just set to negative one over
zero divide n
right just like we derived here
so now let me erase all this reasoning
and then this is the candidate
derivative for D log props let's
uncomment the first line and check that
this is correct
okay so CMP ran and let's go back to CMP
and you see that what it's doing is it's
calculating if
the calculated value by us which is DT
is exactly equal to T dot grad as
calculated by pi torch and then this is
making sure that all the elements are
exactly equal and then converting this
to a single Boolean value because we
don't want the Boolean tensor we just
want to Boolean value
and then here we are making sure that
okay if they're not exactly equal maybe
they are approximately equal because of
some floating Point issues but they're
very very close
so here we are using torch.allclose
which has a little bit of a wiggle
available because sometimes you can get
very very close but if you use a
slightly different calculation because a
floating Point arithmetic you can get a
slightly different result so this is
checking if you get an approximately
close result
and then here we are checking the
maximum uh basically the value that has
the highest difference and what is the
difference in the absolute value
difference between those two and so we
are printing whether we have an exact
equality an approximate equality and
what is the largest difference
and so here
we see that we actually have exact
equality and so therefore of course we
also have an approximate equality and
the maximum difference is exactly zero
so basically our d-log props is exactly
equal to what pytors calculated to be
lockprops.grad in its back propagation
so so far we're working pretty well okay
so let's now continue our back
propagation
we have that lock props depends on
probes through a log
so all the elements of probes are being
element wise applied log to
now if we want deep props then then
remember your micrograph training
we have like a log node it takes in
probs and creates log probs and the
props will be the local derivative of
that individual Operation Log times the
derivative loss with respect to its
output which in this case is D log props
so what is the local derivative of this
operation well we are taking log element
wise and we can come here and we can see
well from alpha is your friend that d by
DX of log of x is just simply one of our
X
so therefore in this case X is problems
so we have d by DX is one over X which
is one of our probes and then this is
the local derivative and then times we
want to chain it
so this is chain rule
times do log props
let me uncomment this and let me run the
cell in place and we see that the
derivative of props as we calculated
here is exactly correct
and so notice here how this works probes
that are props is going to be inverted
and then element was multiplied here
so if your probes is very very close to
one that means you are your network is
currently predicting the character
correctly then this will become one over
one and D log probes just gets passed
through
but if your probabilities are
incorrectly assigned so if the correct
character here is getting a very low
probability then 1.0 dividing by it will
boost this
and then multiply by the log props so
basically what this line is doing
intuitively is it's taking the examples
that have a very low probability
currently assigned and it's boosting
their gradient uh you can you can look
at it that way next up is Count some imp
so we want the river of this now let me
just pause here and kind of introduce
What's Happening Here in general because
I know it's a little bit confusing we
have the locusts that come out of the
neural nut here what I'm doing is I'm
finding the maximum in each row and I'm
subtracting it for the purposes of
numerical stability and we talked about
how if you do not do this you run
numerical issues if some of the logits
take on two large values because we end
up exponentiating them
so this is done just for safety
numerically then here's the
exponentiation of all the sort of like
logits to create our accounts and then
we want to take the some of these counts
and normalize so that all of the probes
sum to one
now here instead of using one over count
sum I use uh raised to the power of
negative one mathematically they are
identical I just found that there's
something wrong with the pytorch
implementation of the backward pass of
division
um and it gives like a real result but
that doesn't happen for star star native
one that's why I'm using this formula
instead but basically all that's
happening here is we got the logits
we're going to exponentiate all of them
and want to normalize the counts to
create our probabilities it's just that
it's happening across multiple lines
so now
here
we want to First Take the derivative we
want to back propagate into account
sumiv and then into counts as well
so what should be the count sum M now we
actually have to be careful here because
we have to scrutinize and be careful
with the shapes so counts that shape and
then count some inverse shape
are different
so in particular counts as 32 by 27 but
this count sum m is 32 by 1. and so in
this multiplication here we also have an
implicit broadcasting that pytorch will
do because it needs to take this column
tensor of 32 numbers and replicate it
horizontally 27 times to align these two
tensors so it can do an element twice
multiply
so really what this looks like is the
following using a toy example again
what we really have here is just props
is counts times conservative so it's a C
equals a times B
but a is 3 by 3 and b is just three by
one a column tensor and so pytorch
internally replicated this elements of B
and it did that across all the columns
so for example B1 which is the first
element of B would be replicated here
across all the columns in this
multiplication
and now we're trying to back propagate
through this operation to count some m
so when we're calculating this
derivative
it's important to realize that these two
this looks like a single operation but
actually is two operations applied
sequentially the first operation that
pytorch did is it took this column
tensor and replicated it across all the
um across all the columns basically 27
times so that's the first operation it's
a replication and then the second
operation is the multiplication so let's
first background through the
multiplication
if these two arrays are of the same size
and we just have a and b of both of them
three by three then how do we mult how
do we back propagate through a
multiplication so if we just have
scalars and not tensors then if you have
C equals a times B then what is uh the
order of the of C with respect to B well
it's just a and so that's the local
derivative
so here in our case undoing the
multiplication and back propagating
through just the multiplication itself
which is element wise is going to be the
local derivative which in this case is
simply counts because counts is the a
so this is the local derivative and then
times because the chain rule D props
so this here is the derivative or the
gradient but with respect to replicated
B
but we don't have a replicated B we just
have a single B column so how do we now
back propagate through the replication
and intuitively this B1 is the same
variable and it's just reused multiple
times
and so you can look at it
as being equivalent to a case we've
encountered in micrograd
and so here I'm just pulling out a
random graph we used in micrograd we had
an example where a single node
has its output feeding into two branches
of basically the graph until the last
function and we're talking about how the
correct thing to do in the backward pass
is we need to sum all the gradients that
arrive at any one node so across these
different branches the gradients would
sum
so if a node is used multiple times the
gradients for all of its uses sum during
back propagation
so here B1 is used multiple times in all
these columns and therefore the right
thing to do here is to sum
horizontally across all the rows so I'm
going to sum in
Dimension one but we want to retain this
Dimension so that the uh so that counts
some end and its gradient are going to
be exactly the same shape so we want to
make sure that we keep them as true so
we don't lose this dimension and this
will make the count sum M be exactly
shape 32 by 1.
so revealing this comparison as well and
running this we see that we get an exact
match
so this derivative is exactly correct
and let me erase
this now let's also back propagate into
counts which is the other variable here
to create probes so from props to count
some INF we just did that let's go into
counts as well
so decounts will be
the chances are a so DC by d a is just B
so therefore it's count summative
um and then times chain rule the props
now councilman is three two by One D
probs is 32 by 27.
so
um those will broadcast fine and will
give us decounts there's no additional
summation required here
um there will be a broadcasting that
happens in this multiply here because
count some M needs to be replicated
again to correctly multiply D props but
that's going to give the correct result
so as far as the single operation is
concerned so we back probably go from
props to counts but we can't actually
check the derivative counts uh I have it
much later on and the reason for that is
because count sum in depends on counts
and so there's a second Branch here that
we have to finish because can't summon
back propagates into account sum and
count sum will buy properly into counts
and so counts is a node that is being
used twice it's used right here in two
props and it goes through this other
Branch through count summative
so even though we've calculated the
first contribution of it we still have
to calculate the second contribution of
it later
okay so we're continuing with this
Branch we have the derivative for count
sum if now we want the derivative of
count sum so D count sum equals what is
the local derivative of this operation
so this is basically an element wise one
over counts sum
so count sum raised to the power of
negative one is the same as one over
count sum if we go to all from alpha we
see that x to the negative one D by D by
D by DX of it is basically Negative X to
the negative 2. right one negative one
over squared is the same as Negative X
to the negative two
so D count sum here will be local
derivative is going to be negative
um
counts sum to the negative two that's
the local derivative times chain rule
which is D count sum in
so that's D count sum
let's uncomment this and check that I am
correct okay so we have perfect equality
and there's no sketchiness going on here
with any shapes because these are of the
same shape okay next up we want to back
propagate through this line we have that
count sum it's count.sum along the rows
so I wrote out
um some help here we have to keep in
mind that counts of course is 32 by 27
and count sum is 32 by 1. so in this
back propagation we need to take this
column of derivatives and transform it
into a array of derivatives
two-dimensional array
so what is this operation doing we're
taking in some kind of an input like say
a three by three Matrix a and we are
summing up the rows into a column tells
her B1 b2b3 that is basically this
so now we have the derivatives of the
loss with respect to B all the elements
of B
and now we want to derivative loss with
respect to all these little A's
so how do the B's depend on the ace is
basically what we're after what is the
local derivative of this operation
well we can see here that B1 only
depends on these elements here the
derivative of B1 with respect to all of
these elements down here is zero but for
these elements here like a11 a12 Etc the
local derivative is one right so DB 1 by
D A 1 1 for example is one so it's one
one and one
so when we have the derivative of loss
with respect to B1
did a local derivative of B1 with
respect to these inputs is zeros here
but it's one on these guys
so in the chain rule
we have the local derivative uh times
sort of the derivative of B1 and so
because the local derivative is one on
these three elements the look of them
are multiplying the derivative of B1
will just be the derivative of B1 and so
you can look at it as a router basically
an addition is a router of gradient
whatever gradient comes from above it
just gets routed equally to all the
elements that participate in that
addition
so in this case the derivative of B1
will just flow equally to the derivative
of a11 a12 and a13
. so if we have a derivative of all the
elements of B and in this column tensor
which is D counts sum that we've
calculated just now
we basically see that what that amounts
to is all of these are now flowing to
all these elements of a and they're
doing that horizontally
so basically what we want is we want to
take the decount sum of size 30 by 1 and
we just want to replicate it 27 times
horizontally to create 32 by 27 array
so there's many ways to implement this
operation you could of course just
replicate the tensor but I think maybe
one clean one is that the counts is
simply torch dot once like
so just an two-dimensional arrays of
ones in the shape of counts so 32 by 27
times D counts sum so this way we're
letting the broadcasting here basically
implement the replication you can look
at it that way
but then we have to also be careful
because decounts was already calculated
we calculated earlier here and that was
just the first branch and we're now
finishing the second Branch so we need
to make sure that these gradients add so
plus equals
and then here
um let's comment out the comparison and
let's make sure crossing fingers that we
have the correct result so pytorch
agrees with us on this gradient as well
okay hopefully we're getting a hang of
this now counts as an element-wise X of
Norm legits so now we want D Norm logits
and because it's an element price
operation everything is very simple what
is the local derivative of e to the X
it's famously just e to the x so this is
the local derivative
that is the local derivative now we
already calculated it and it's inside
counts so we may as well potentially
just reuse counts that is the local
derivative
times uh D counts
funny as that looks constant decount is
derivative on the normal objects and now
let's erase this and let's verify and it
looks good
so that's uh normal agents
okay so we are here on this line now the
normal objects
we have that and we're trying to
calculate the logits and deloget Maxes
so back propagating through this line
now we have to be careful here because
the shapes again are not the same and so
there's an implicit broadcasting
Happening Here
so normal jits has this shape 32 by 27
logist does as well but logit Maxis is
only 32 by one so there's a broadcasting
here in the minus
now here I try to sort of write out a
two example again we basically have that
this is our C equals a minus B
and we see that because of the shape
these are three by three but this one is
just a column
and so for example every element of C we
have to look at how it uh came to be and
every element of C is just the
corresponding element of a minus uh
basically that associated b
so it's very clear now that the
derivatives of every one of these c's
with respect to their inputs are one for
the corresponding a
and it's a negative one for the
corresponding B
and so therefore
um
the derivatives on the C will flow
equally to the corresponding Ace and
then also to the corresponding base but
then in addition to that the B's are
broadcast so we'll have to do the
additional sum just like we did before
and of course the derivatives for B's
will undergo a minus because the local
derivative here is uh negative one
so DC three two by D B3 is negative one
so let's just Implement that basically
delugits will be uh exactly copying the
derivative on normal objects
so
delugits equals the norm logits and I'll
do a DOT clone for safety so we're just
making a copy
and then we have that the loaded Maxis
will be the negative of the non-legits
because of the negative sign
and then we have to be careful because
logic Maxis is a column
and so just like we saw before because
we keep replicating the same elements
across all the columns
then in the backward pass because we
keep reusing this these are all just
like separate branches of use of that
one variable and so therefore we have to
do a Sum along one would keep them
equals true so that we don't destroy
this dimension
and then the logic Maxes will be the
same shape now we have to be careful
because this deloaches is not the final
deloaches and that's because not only do
we get gradient signal into logits
through here but the logic Maxes as a
function of logits and that's a second
Branch into logits so this is not yet
our final derivative for logits we will
come back later for the second branch
for now the logic Maxis is the final
derivative so let me uncomment this CMP
here and let's just run this
and logit Maxes hit by torch agrees with
us
so that was the derivative into through
this line
now before we move on I want to pause
here briefly and I want to look at these
logic Maxes and especially their
gradients
we've talked previously in the previous
lecture that the only reason we're doing
this is for the numerical stability of
the softmax that we are implementing
here and we talked about how if you take
these logents for any one of these
examples so one row of this logit's
tensor if you add or subtract any value
equally to all the elements then the
value of the probes will be unchanged
you're not changing soft Max the only
thing that this is doing is it's making
sure that X doesn't overflow and the
reason we're using a Max is because then
we are guaranteed that each row of
logits the highest number is zero and so
this will be safe
and so
um
basically what that has repercussions
if it is the case that changing logit
Maxis does not change the props and
therefore there's not change the loss
then the gradient on logic masses should
be zero right because saying those two
things is the same
so indeed we hope that this is very very
small numbers so indeed we hope this is
zero now because of floating Point uh
sort of wonkiness
um this doesn't come out exactly zero
only in some of the rows it does but we
get extremely small values like one e
negative nine or ten and so this is
telling us that the values of loaded
Maxes are not impacting the loss as they
shouldn't
it feels kind of weird to back propagate
through this branch honestly because
if you have any implementation of like f
dot cross entropy and pytorch and you
you block together all these elements
and you're not doing the back
propagation piece by piece then you
would probably assume that the
derivative through here is exactly zero
uh so you would be sort of
um skipping this branch because it's
only done for numerical stability but
it's interesting to see that even if you
break up everything into the full atoms
and you still do the computation as
you'd like with respect to numerical
stability uh the correct thing happens
and you still get a very very small
gradients here
um basically reflecting the fact that
the values of these do not matter with
respect to the final loss
okay so let's now continue back
propagation through this line here we've
just calculated the logit Maxis and now
we want to back prop into logits through
this second branch
now here of course we took legits and we
took the max along all the rows and then
we looked at its values here now the way
this works is that in pytorch
this thing here
the max returns both the values and it
Returns the indices at which those
values to count the maximum value
now in the forward pass we only used
values because that's all we needed but
in the backward pass it's extremely
useful to know about where those maximum
values occurred and we have the indices
at which they occurred and this will of
course helps us to help us do the back
propagation because what should the
backward pass be here in this case we
have the largest tensor which is 32 by
27 and in each row we find the maximum
value and then that value gets plucked
out into loaded Maxis and so intuitively
um basically the derivative flowing
through here then should be one
times the look of derivatives is 1 for
the appropriate entry that was plucked
out
and then times the global derivative of
the logic axis
so really what we're doing here if you
think through it is we need to take the
deloachet Maxis and we need to scatter
it to the correct positions in these
logits from where the maximum values
came
and so
um
I came up with one line of code sort of
that does that let me just erase a bunch
of stuff here so the line of uh you
could do it kind of very similar to what
we've done here where we create a zeros
and then we populate uh the correct
elements uh so we use the indices here
and we would set them to be one but you
can also use one hot
so F dot one hot and then I'm taking the
lowest of Max over the First Dimension
dot indices and I'm telling uh pytorch
that the dimension of every one of these
tensors should be
um
27 and so what this is going to do
is okay I apologize this is crazy filthy
that I am sure of this
it's really just a an array of where the
Maxes came from in each row and that
element is one and the all the other
elements are zero so it's a one-half
Vector in each row and these indices are
now populating a single one in the
proper place
and then what I'm doing here is I'm
multiplying by the logit Maxis and keep
in mind that this is a column
of 32 by 1. and so when I'm doing this
times the logic Maxis the logic Maxes
will broadcast and that column will you
know get replicated and in an element
wise multiply will ensure that each of
these just gets routed to whichever one
of these bits is turned on
and so that's another way to implement
uh this kind of a this kind of a
operation and both of these can be used
I just thought I would show an
equivalent way to do it and I'm using
plus equals because we already
calculated the logits here and this is
not the second branch
so let's
look at logits and make sure that this
is correct
and we see that we have exactly the
correct answer
next up we want to continue with logits
here that is an outcome of a matrix
multiplication and a bias offset in this
linear layer
so I've printed out the shapes of all
these intermediate tensors we see that
logits is of course 32 by 27 as we've
just seen
then the H here is 32 by 64. so these
are 64 dimensional hidden States and
then this W Matrix projects those 64
dimensional vectors into 27 dimensions
and then there's a 27 dimensional offset
which is a one-dimensional vector
now we should note that this plus here
actually broadcasts because H multiplied
by by W2 will give us a 32 by 27. and so
then this plus B2 is a 27 dimensional
lecture here
now in the rules of broadcasting what's
going to happen with this bias Vector is
that this one-dimensional Vector of 27
will get aligned with a padded dimension
of one on the left and it will basically
become a row vector and then it will get
replicated vertically 32 times to make
it 32 by 27 and then there's an
element-wise multiply
now
the question is how do we back propagate
from logits to the hidden States the
weight Matrix W2 and the bias B2
and you might think that we need to go
to some Matrix calculus and then we have
to look up the derivative for a matrix
multiplication but actually you don't
have to do any of that and you can go
back to First principles and derive this
yourself on a piece of paper and
specifically what I like to do and I
what I find works well for me is you
find a specific small example that you
then fully write out and then in the
process of analyzing how that individual
small example works you will understand
the broader pattern and you'll be able
to generalize and write out the full
general formula for what how these
derivatives flow in an expression like
this so let's try that out
so pardon the low budget production here
but what I've done here is I'm writing
it out on a piece of paper really what
we are interested in is we have a
multiply B plus C and that creates a d
and we have the derivative of the loss
with respect to D and we'd like to know
what the derivative of the losses with
respect to a b and c
now these here are little
two-dimensional examples of a matrix
multiplication Two by Two Times a two by
two
plus a 2 a vector of just two elements
C1 and C2 gives me a two by two
now notice here that I have a bias
Vector here called C and the bisex
vector is C1 and C2 but as I described
over here that bias Vector will become a
row Vector in the broadcasting and will
replicate vertically so that's what's
happening here as well C1 C2 is
replicated vertically and we see how we
have two rows of C1 C2 as a result
so now when I say write it out I just
mean like this basically break up this
matrix multiplication into the actual
thing that that's going on under the
hood so as a result of matrix
multiplication and how it works d11 is
the result of a DOT product between the
first row of a and the First Column of B
so a11 b11 plus a12 B21 plus C1
and so on so forth for all the other
elements of D and once you actually
write it out it becomes obvious this is
just a bunch of multipliers and
um adds and we know from micrograd how
to differentiate multiplies and adds and
so this is not scary anymore it's not
just matrix multiplication it's just uh
tedious unfortunately but this is
completely tractable we have DL by D for
all of these and we want DL by uh all
these little other variables so how do
we achieve that and how do we actually
get the gradients okay so the low budget
production continues here
so let's for example derive the
derivative of the loss with respect to
a11
we see here that a11 occurs twice in our
simple expression right here right here
and influences d11 and D12
. so this is so what is DL by d a one
one well it's DL by d11 times the local
derivative of d11 which in this case is
just b11 because that's what's
multiplying a11 here
so uh and likewise here the local
derivative of D12 with respect to a11 is
just B12 and so B12 well in the chain
rule therefore multiply the L by d 1 2.
and then because a11 is used both to
produce d11 and D12 we need to add up
the contributions of both of those sort
of chains that are running in parallel
and that's why we get a plus just adding
up those two
um those two contributions and that
gives us DL by d a one one we can do the
exact same analysis for the other one
for all the other elements of a and when
you simply write it out it's just super
simple
um taking of gradients on you know
expressions like this
you find that
this Matrix DL by D A that we're after
right if we just arrange all the all of
them in the same shape as a takes so a
is just too much Matrix so d l by D A
here will be also just the same shape
tester with the derivatives now so deal
by D a11 Etc
and we see that actually we can express
what we've written out here as a matrix
multiplied
and so it just so happens that D all by
that all of these formulas that we've
derived here by taking gradients can
actually be expressed as a matrix
multiplication and in particular we see
that it is the matrix multiplication of
these two array matrices
so it is the um DL by D and then Matrix
multiplying B but B transpose actually
so you see that B21 and b12 have changed
place
whereas before we had of course b11 B12
B2 on B22 so you see that this other
Matrix B is transposed
and so basically what we have long story
short just by doing very simple
reasoning here by breaking up the
expression in the case of a very simple
example is that DL by d a is which is
this is simply equal to DL by DD Matrix
multiplied with B transpose
so that is what we have so far now we
also want the derivative with respect to
um B and C now
for B I'm not actually doing the full
derivation because honestly it's um it's
not deep it's just uh annoying it's
exhausting you can actually do this
analysis yourself you'll also find that
if you take this these expressions and
you differentiate with respect to b
instead of a you will find that DL by DB
is also a matrix multiplication in this
case you have to take the Matrix a and
transpose it and Matrix multiply that
with bl by DD
and that's what gives you a deal by DB
and then here for the offsets C1 and C2
if you again just differentiate with
respect to C1 you will find an
expression like this
and C2 an expression like this
and basically you'll find the DL by DC
is simply because they're just
offsetting these Expressions you just
have to take the deal by DD Matrix
of the derivatives of D and you just
have to sum across the columns and that
gives you the derivatives for C
so long story short
the backward Paths of a matrix multiply
is a matrix multiply
and instead of just like we had D equals
a times B plus C in the scalar case uh
we sort of like arrive at something very
very similar but now uh with a matrix
multiplication instead of a scalar
multiplication
so the derivative of D with respect to a
is
DL by DD Matrix multiplied B trespose
and here it's a transpose multiply deal
by DD but in both cases it's a matrix
multiplication with the derivative and
the other term in the multiplication
and for C it is a sum
now I'll tell you a secret I can never
remember the formulas that we just
arrived for back proper gain information
multiplication and I can back propagate
through these Expressions just fine and
the reason this works is because the
dimensions have to work out
uh so let me give you an example say I
want to create DH
then what should the H be number one I
have to know that the shape of DH must
be the same as the shape of H
and the shape of H is 32 by 64. and then
the other piece of information I know is
that DH must be some kind of matrix
multiplication of the logits with W2
and delojits is 32 by 27 and W2 is a 64
by 27. there is only a single way to
make the shape work out in this case and
it is indeed the correct result in
particular here H needs to be 32 by 64.
the only way to achieve that is to take
a deluges
and Matrix multiply it with you see how
I have to take W2 but I have to
transpose it to make the dimensions work
out
so w to transpose and it's the only way
to make these to Matrix multiply those
two pieces to make the shapes work out
and that turns out to be the correct
formula so if we come here we want DH
which is d a and we see that d a is DL
by DD Matrix multiply B transpose
so that's Delo just multiply and B is W2
so W2 transpose which is exactly what we
have here so there's no need to remember
these formulas similarly now if I want
dw2 well I know that it must be a matrix
multiplication of D logits and H
and maybe there's a few transpose like
there's one transpose in there as well
and I don't know which way it is so I
have to come to W2 and I see that its
shape is 64 by 27
and that has to come from some interest
multiplication of these two
and so to get a 64 by 27 I need to take
um
H I need to transpose it
and then I need to Matrix multiply it
um so that will become 64 by 32 and then
I need to make sure to multiply with the
32 by 27 and that's going to give me a
64 by 27. so I need to make sure it's
multiplied this with the logist that
shape just like that that's the only way
to make the dimensions work out and just
use matrix multiplication and if we come
here we see that that's exactly what's
here so a transpose a for us is H
multiplied with deloaches
so that's W2 and then db2
is just the um
vertical sum and actually in the same
way there's only one way to make the
shapes work out I don't have to remember
that it's a vertical Sum along the zero
axis because that's the only way that
this makes sense because B2 shape is 27
so in order to get a um delugits
here is 30 by 27 so knowing that it's
just sum over deloaches in some
Direction
that direction must be zero because I
need to eliminate this Dimension so it's
this
so this is so let's kind of like the
hacky way let me copy paste and delete
that and let me swing over here and this
is our backward pass for the linear
layer uh hopefully
so now let's uncomment
these three and we're checking that we
got all the three derivatives correct
and run
and we see that h wh and B2 are all
exactly correct so we back propagated
through a linear layer
now next up we have derivative for the h
already and we need to back propagate
through 10h into h preact
so we want to derive DH preact
and here we have to back propagate
through a 10 H and we've already done
this in micrograd and we remember that
10h has a very simple backward formula
now unfortunately if I just put in D by
DX of 10 h of X into both from alpha it
lets us down it tells us that it's a
hyperbolic secant function squared of X
it's not exactly helpful but luckily
Google image search does not let us down
and it gives us the simpler formula and
in particular if you have that a is
equal to 10 h of Z then d a by DZ by
propagating through 10 H is just one
minus a square and take note that 1
minus a square a here is the output of
the 10h not the input to the 10h Z so
the D A by DZ is here formulated in
terms of the output of that 10h
and here also in Google image search we
have the full derivation if you want to
actually take the actual definition of
10h and work through the math to figure
out 1 minus standard square of Z
so 1 minus a square is the local
derivative in our case that is 1 minus
uh the output of 10 H squared which here
is H
so it's h squared and that is the local
derivative and then times the chain rule
DH
so that is going to be our candidate
implementation so if we come here
and then uncomment this let's hope for
the best
and we have the right answer
okay next up we have DH preact and we
want to back propagate into the gain the
B and raw and the B and bias
so here this is the bathroom parameters
being gained in bias inside the bash
term that take the B and raw that is
exact unit caution and then scale it and
shift it
and these are the parameters of The
Bachelor now here we have a
multiplication but it's worth noting
that this multiply is very very
different from this Matrix multiply here
Matrix multiply are DOT products between
rows and Columns of these matrices
involved this is an element twice
multiply so things are quite a bit
simpler
now we do have to be careful with some
of the broadcasting happening in this
line of code though so you see how BN
gain and B and bias are 1 by 64. but H
preact and B and raw are 32 by 64.
so we have to be careful with that and
make sure that all the shapes work out
fine and that the broadcasting is
correctly back propagated
so in particular let's start with the B
and Gain so DB and gain should be
and here this is again elementorized
multiply and whenever we have a times b
equals c we saw that the local
derivative here is just if this is a the
local derivative is just the B the other
one so the local derivative is just B
and raw and then times chain rule
so DH preact
so this is the candidate gradient now
again we have to be careful because B
and Gain Is of size 1 by 64. but this
here would be 32 by 64.
and so
um the correct thing to do in this case
of course is that b and gain here is a
rule Vector of 64 numbers it gets
replicated vertically in this operation
and so therefore the correct thing to do
is to sum because it's being replicated
and therefore all the gradients in each
of the rows that are now flowing
backwards need to sum up to that same
tensor DB and Gain so we have to sum
across all the zero all the examples
basically
which is the direction in which this
gets replicated
and now we have to be also careful
because we
um being gain is of shape 1 by 64. so in
fact I need to keep them as true
otherwise I would just get 64.
now I don't actually really remember why
the being gain and the BN bias I made
them be 1 by 64.
um
but the biases B1 and B2 I just made
them be one-dimensional vectors they're
not two-dimensional tensors so I can't
recall exactly why I left the gain and
the bias as two-dimensional but it
doesn't really matter as long as you are
consistent and you're keeping it the
same
so in this case we want to keep the
dimension so that the tensor shapes work
next up we have B and raw so DB and raw
will be BN gain
multiplying
dhreact that's our chain rule now what
about the
um
dimensions of this we have to be careful
right so DH preact is 32 by 64. B and
gain is 1 by 64. so it will just get
replicated and to create this
multiplication which is the correct
thing because in a forward pass it also
gets replicated in just the same way
so in fact we don't need the brackets
here we're done
and the shapes are already correct
and finally for the bias
very similar this bias here is very very
similar to the bias we saw when you
layer in the linear layer and we see
that the gradients from each preact will
simply flow into the biases and add up
because these are just these are just
offsets
and so basically we want this to be DH
preact but it needs to Sum along the
right Dimension and in this case similar
to the gain we need to sum across the
zeroth dimension the examples because of
the way that the bias gets replicated
vertically
and we also want to have keep them as
true
and so this will basically take this and
sum it up and give us a 1 by 64.
so this is the candidate implementation
it makes all the shapes work
let me bring it up down here and then
let me uncomment these three lines
to check that we are getting the correct
result for all the three tensors and
indeed we see that all of that got back
propagated correctly so now we get to
the batch Norm layer we see how here
being gay and being bias are the
parameters so the back propagation ends
but B and raw now is the output of the
standardization
so here what I'm doing of course is I'm
breaking up the batch form into
manageable pieces so we can back
propagate through each line individually
but basically what's happening is BN
mean I is the sum
so this is the B and mean I I apologize
for the variable naming B and diff is x
minus mu
B and div 2 is x minus mu squared here
inside the variance
B and VAR is the variance so uh Sigma
Square this is B and bar and it's
basically the sum of squares
so this is the x minus mu squared and
then the sum now you'll notice one
departure here
here it is normalized as 1 over m
uh which is number of examples here I'm
normalizing as one over n minus 1
instead of N and this is deliberate and
I'll come back to that in a bit when we
are at this line it is something called
the bezels correction
but this is how I want it in our case
bienvar inv then becomes basically
bienvar plus Epsilon Epsilon is one
negative five and then it's one over
square root
is the same as raising to the power of
negative 0.5 right because 0.5 is square
root and then negative makes it one over
square root
so BM Bar M is a one over this uh
denominator here and then we can see
that b and raw which is the X hat here
is equal to the BN diff the numerator
multiplied by the
um BN bar in
and this line here that creates pre-h
pre-act was the last piece we've already
back propagated through it
so now what we want to do is we are here
and we have B and raw and we have to
first back propagate into B and diff and
B and Bar M
so now we're here and we have DB and raw
and we need to back propagate through
this line
now I've written out the shapes here and
indeed bien VAR m is a shape 1 by 64. so
there is a broadcasting happening here
that we have to be careful with but it
is just an element-wise simple
multiplication by now we should be
pretty comfortable with that to get DB
and diff we know that this is just B and
varm
multiplied with
DP and raw
and conversely to get dbmring
we need to take the end if
and multiply that by DB and raw
so this is the candidate but of course
we need to make sure that broadcasting
is obeyed so in particular B and VAR M
multiplying with DB and raw
will be okay and give us 32 by 64 as we
expect
but dbm VAR inv would be taking a 32 by
64.
multiplying it by 32 by 64. so this is a
32 by 64. but of course DB this uh B and
VAR in is only 1 by 64. so the second
line here needs a sum across the
examples and because there's this
Dimension here we need to make sure that
keep them is true
so this is the candidate
let's erase this and let's swing down
here
and implement it and then let's comment
out dbm barif and DB and diff
now we'll actually notice that DB and
diff by the way is going to be incorrect
so when I run this
BMR m is correct B and diff is not
correct and this is actually expected
because we're not done with b and diff
so in particular when we slide here we
see here that b and raw as a function of
B and diff but actually B and far of is
a function of B of R which is a function
of B and df2 which is a function of B
and diff
so it comes here so bdn diff
um these variable names are crazy I'm
sorry it branches out into two branches
and we've only done one branch of it we
have to continue our back propagation
and eventually come back to B and diff
and then we'll be able to do a plus
equals and get the actual card gradient
for now it is good to verify that CMP
also works it doesn't just lie to us and
tell us that everything is always
correct it can in fact detect when your
gradient is not correct so it's that's
good to see as well okay so now we have
the derivative here and we're trying to
back propagate through this line
and because we're raising to a power of
negative 0.5 I brought up the power rule
and we see that basically we have that
the BM bar will now be we bring down the
exponent so negative 0.5 times
uh X which is this
and now raised to the power of negative
0.5 minus 1 which is negative 1.5
now we would have to also apply a small
chain rule here in our head because we
need to take further the derivative of B
and VAR with respect to this expression
here inside the bracket but because this
is an elementalized operation and
everything is fairly simple that's just
one and so there's nothing to do there
so this is the local derivative and then
times the global derivative to create
the chain rule this is just times the BM
bar have
so this is our candidate let me bring
this down
and uncommon to the check
and we see that we have the correct
result
now before we propagate through the next
line I want to briefly talk about the
note here where I'm using the bezels
correction dividing by n minus 1 instead
of dividing by n when I normalize here
the sum of squares
now you'll notice that this is departure
from the paper which uses one over n
instead not one over n minus one their m
is RN
and
um so it turns out that there are two
ways of estimating variance of an array
one is the biased estimate which is one
over n and the other one is the unbiased
estimate which is one over n minus one
now confusingly in the paper this is uh
not very clearly described and also it's
a detail that kind of matters I think
um they are using the biased version
training time but later when they are
talking about the inference they are
mentioning that when they do the
inference they are using the unbiased
estimate which is the n minus one
version in
um
basically for inference
and to calibrate the running mean and
the running variance basically and so
they they actually introduce a trained
test mismatch where in training they use
the biased version and in the in test
time they use the unbiased version I
find this extremely confusing you can
read more about the bezels correction
and why uh dividing by n minus one gives
you a better estimate of the variance in
a case where you have population size or
samples for the population
that are very small and that is indeed
the case for us because we are dealing
with many patches and these mini matches
are a small sample of a larger
population which is the entire training
set and so it just turns out that if you
just estimate it using one over n that
actually almost always underestimates
the variance and it is a biased
estimator and it is advised that you use
the unbiased version and divide by n
minus one and you can go through this
article here that I liked that actually
describes the full reasoning and I'll
link it in the video description
now when you calculate the torture
variance
you'll notice that they take the
unbiased flag whether or not you want to
divide by n or n minus one confusingly
they do not mention what the default is
for unbiased but I believe unbiased by
default is true I'm not sure why the
docs here don't cite that
now in The Bachelor
1D the documentation again is kind of
wrong and confusing it says that the
standard deviation is calculated via the
biased estimator
but this is actually not exactly right
and people have pointed out that it is
not right in a number of issues since
then because actually the rabbit hole is
deeper and they follow the paper exactly
and they use the biased version for
training but when they're estimating the
running standard deviation we are using
the unbiased version so again there's
the train test mismatch so long story
short I'm not a fan of trained test
discrepancies I basically kind of
consider
the fact that we use the bias version
the training time and the unbiased test
time I basically consider this to be a
bug and I don't think that there's a
good reason for that it's not really
they don't really go into the detail of
the reasoning behind it in this paper so
that's why I basically prefer to use the
bestless correction in my own work
unfortunately Bastion does not take a
keyword argument that tells you whether
or not you want to use the unbiased
version of the bias version in both
train and test and so therefore anyone
using batch normalization basically in
my view has a bit of a bug in the code
um
and this turns out to be much less of a
problem if your batch mini batch sizes
are a bit larger but still I just might
kind of uh unpardable so maybe someone
can explain why this is okay but for now
I prefer to use the unbiased version
consistently both during training and at
this time and that's why I'm using one
over n minus one here
okay so let's now actually back
propagate through this line
so
the first thing that I always like to do
is I like to scrutinize the shapes first
so in particular here looking at the
shapes of what's involved I see that b
and VAR shape is 1 by 64. so it's a row
vector and BND if two dot shape is 32 by
64.
so clearly here we're doing a sum over
the zeroth axis to squash the first
dimension of of the shapes here using a
sum so that right away actually hints to
me that there will be some kind of a
replication or broadcasting in the
backward pass and maybe you're noticing
the pattern here but basically anytime
you have a sum in the forward pass that
turns into a replication or broadcasting
in the backward pass along the same
Dimension and conversely when we have a
replication or a broadcasting in the
forward pass that indicates a variable
reuse and so in the backward pass that
turns into a sum over the exact same
dimension
and so hopefully you're noticing that
Duality that those two are kind of like
the opposite of each other in the
forward and backward pass
now once we understand the shapes the
next thing I like to do always is I like
to look at a toy example in my head to
sort of just like understand roughly how
uh the variable the variable
dependencies go in the mathematical
formula
so here we have a two-dimensional array
of the end of two which we are scaling
by a constant and then we are summing uh
vertically over the columns so if we
have a two by two Matrix a and then we
sum over the columns and scale we would
get a row Vector B1 B2 and B1 depends on
a in this way whereas just sum they're
scaled of a and B2 in this way where
it's the second column sump and scale
and so looking at this basically
what we want to do now is we have the
derivatives on B1 and B2 and we want to
back propagate them into Ace and so it's
clear that just differentiating in your
head the local derivative here is one
over n minus 1 times uh one
uh for each one of these A's and um
basically the derivative of B1 has to
flow through The Columns of a
scaled by one over n minus one
and that's roughly What's Happening Here
so intuitively the derivative flow tells
us that DB and diff2
will be the local derivative of this
operation and there are many ways to do
this by the way but I like to do
something like this torch dot once like
of bndf2 so I'll create a large array
two-dimensional of ones
and then I will scale it so 1.0 divided
by n minus 1.
so this is a array of
um one over n minus one and that's sort
of like the local derivative
and now for the chain rule I will simply
just multiply it by dbm bar
and notice here what's going to happen
this is 32 by 64 and this is just 1 by
64. so I'm letting the broadcasting do
the replication because internally in
pytorch basically dbnbar which is 1 by
64 row vector
well in this multiplication get
um copied vertically until the two are
of the same shape and then there will be
an element wise multiply and so that uh
so that the broadcasting is basically
doing the replication
and I will end up with the derivatives
of DB and diff2 here
so this is the candidate solution let's
bring it down here
let's uncomment this line where we check
it and let's hope for the best
and indeed we see that this is the
correct formula next up let's
differentiate here and to be in this
so here we have that b and diff is
element y squared to create B and F2
so this is a relatively simple
derivative because it's a simple element
wise operation so it's kind of like the
scalar case and we have that DB and div
should be if this is x squared then the
derivative of this is 2x right so it's
simply 2 times B and if that's the local
derivative
and then times chain Rule and the shape
of these is the same they are of the
same shape so times this
so that's the backward pass for this
variable let me bring that down here
and now we have to be careful because we
already calculated dbm depth right so
this is just the end of the other uh you
know other Branch coming back to B and
diff
because B and diff was already back
propagated to way over here
from being raw so we now completed the
second branch and so that's why I have
to do plus equals and if you recall we
had an incorrect derivative for being
diff before and I'm hoping that once we
append this last missing piece we have
the exact correctness so let's run
ambient to be in div now actually shows
the exact correct derivative
um so that's comforting okay so let's
now back propagate through this line
here
um the first thing we do of course is we
check the shapes and I wrote them out
here and basically the shape of this is
32 by 64. hpbn is the same shape
but B and mean I is a row Vector 1 by
64. so this minus here will actually do
broadcasting and so we have to be
careful with that and as a hint to us
again because of The Duality a
broadcasting and the forward pass means
a variable reuse and therefore there
will be a sum in the backward pass
so let's write out the backward pass
here now
um
back propagate into the hpbn
because this is these are the same shape
then the local derivative for each one
of the elements here is just one for the
corresponding element in here
so basically what this means is that the
gradient just simply copies it's just a
variable assignment it's quality so I'm
just going to clone this tensor just for
safety to create an exact copy of DB and
div
and then here to back propagate into
this one what I'm inclined to do here is
will basically be
uh what is the local derivative well
it's negative torch.1's like
of the shape of uh B and diff
right
and then times
the um
the derivative here dbf
and this here is the back propagation
for the replicated B and mean I
so I still have to back propagate
through the uh replication in the
broadcasting and I do that by doing a
sum so I'm going to take this whole
thing and I'm going to do a sum over the
zeroth dimension which was the
replication
so if you scrutinize this by the way
you'll notice that this is the same
shape as that and so what I'm doing uh
what I'm doing here doesn't actually
make that much sense because it's just a
array of ones multiplying DP and diff so
in fact I can just do this
um and that is equivalent
so this is the candidate backward pass
let me copy it here and then let me
comment out this one and this one
enter
and it's wrong
damn
actually sorry this is supposed to be
wrong and it's supposed to be wrong
because
we are back propagating from a b and
diff into hpbn and but we're not done
because B and mean I depends on hpbn and
there will be a second portion of that
derivative coming from this second
Branch so we're not done yet and we
expect it to be incorrect so there you
go
uh so let's now back propagate from uh B
and mean I into hpbn
um
and so here again we have to be careful
because there's a broadcasting along
um or there's a Sum along the zeroth
dimension so this will turn into
broadcasting in the backward pass now
and I'm going to go a little bit faster
on this line because it is very similar
to the line that we had before and
multiplies in the past in fact
so the hpbn
will be
the gradient will be scaled by 1 over n
and then basically this gradient here on
dbn mean I
is going to be scaled by 1 over n and
then it's going to flow across all the
columns and deposit itself into the hpvn
so what we want is this thing scaled by
1 over n
only put the constant up front here
um
so scale down the gradient and now we
need to replicate it across all the um
across all the rows here so we I like to
do that by torch.lunslike of basically
um hpbn
and I will let the broadcasting do the
work of replication
so
like that
so this is uh the hppn and hopefully
we can plus equals that
so this here is broadcasting
um and then this is the scaling so this
should be current
okay
so that completes the back propagation
of the bathroom layer and we are now
here let's back propagate through the
linear layer one here now because
everything is getting a little
vertically crazy I copy pasted the line
here and let's just back properly
through this one line
so first of course we inspect the shapes
and we see that this is 32 by 64. MCAT
is 32 by 30.
W1 is 30 30 by 64 and B1 is just 64. so
as I mentioned back propagating through
linear layers is fairly easy just by
matching the shapes so let's do that we
have that dmcat
should be
um some matrix multiplication of dhbn
with uh W1 and one transpose thrown in
there so to make uh MCAT be 32 by 30
I need to take dhpn
32 by 64 and multiply it by w1.
transpose
to get the only one I need to end up
with 30 by 64.
so to get that I need to take uh MCAT
transpose
and multiply that by
uh dhpion
and finally to get DB1
this is a addition and we saw that
basically I need to just sum the
elements in dhpbn along some Dimension
and to make the dimensions work out I
need to Sum along the zeroth axis here
to eliminate this Dimension and we do
not keep dims
uh so that we want to just get a single
one-dimensional lecture of 64.
so these are the claimed derivatives
let me put that here and let me
uncomment three lines and cross our
fingers
everything is great okay so we now
continue almost there we have the
derivative of MCAT and we want to
derivative we want to back propagate
into m
so I again copied this line over here
so this is the forward pass and then
this is the shapes so remember that the
shape here was 32 by 30 and the original
shape of M plus 32 by 3 by 10. so this
layer in the forward pass as you recall
did the concatenation of these three
10-dimensional character vectors
and so now we just want to undo that
so this is actually relatively
straightforward operation because uh the
backward pass of the what is the view
view is just a representation of the
array it's just a logical form of how
you interpret the array so let's just
reinterpret it to be what it was before
so in other words the end is not uh 32
by 30. it is basically dmcat
but if you view it as the original shape
so just m dot shape
uh you can you can pass in tuples into
view
and so this should just be okay
we just re-represent that view and then
we uncomment this line here and
hopefully
yeah so the derivative of M is correct
so in this case we just have to
re-represent the shape of those
derivatives into the original View
so now we are at the final line and the
only thing that's left to back propagate
through is this indexing operation here
MSC at xB so as I did before I copy
pasted this line here and let's look at
the shapes of everything that's involved
and remind ourselves how this worked
so m.shape was 32 by 3 by 10.
it says 32 examples and then we have
three characters each one of them has a
10 dimensional embedding
and this was achieved by taking the
lookup table C which have 27 possible
characters
each of them 10 dimensional and we
looked up
at the rows that were specified inside
this tensor xB
so XB is 32 by 3 and it's basically
giving us for each example the Identity
or the index of which character is part
of that example
and so here I'm showing the first five
rows of three of this tensor xB
and so we can see that for example here
it was the first example in this batch
is that the first character and the
first character and the fourth character
comes into the neural net
and then we want to predict the next
character in a sequence after the
character is one one four
so basically What's Happening Here is
there are integers inside XB and each
one of these integers is specifying
which row of C we want to pluck out
right and then we arrange those rows
that we've plucked out into 32 by 3 by
10 tensor and we just package them in we
just package them into the sensor
and now what's happening is that we have
D amp
so for every one of these uh basically
plucked out rows we have their gradients
now
but they're arranged inside this 32 by 3
by 10 tensor so all we have to do now is
we just need to Route this gradient
backwards through this assignment so we
need to find which row of C that every
one of these
um 10 dimensional embeddings come from
and then we need to deposit them into DC
so we just need to undo the indexing and
of course if any of these rows of C was
used multiple times which almost
certainly is the case like the row one
and one was used multiple times then we
have to remember that the gradients that
arrive there have to add
so for each occurrence we have to have
an addition
so let's now write this out and I don't
actually know if like a much better way
to do this than a for Loop unfortunately
in Python
um so maybe someone can come up with a
vectorized efficient operation but for
now let's just use for loops so let me
create a torch.zeros like
C to initialize uh just uh 27 by 10
tensor of all zeros
and then honestly 4K in range XB dot
shape at zero
maybe someone has a better way to do
this but for J and range
be that shape at one
this is going to iterate over all the
um all the elements of XB all these
integers
and then let's get the index at this
position
so the index is basically x b at KJ
so that an example of that like is 11 or
14 and so on
and now in the forward pass we took
and we basically took um
the row of C at index and we deposited
it into M at K of J
that's what happened that's where they
are packaged so now we need to go
backwards and we just need to route
DM at the position KJ
we now have these derivatives
for each position and it's 10
dimensional
and you just need to go into the correct
row of C
so DC rather at IX is this but plus
equals
because there could be multiple
occurrences uh like the same row could
have been used many many times and so
all of those derivatives will just go
backwards through the indexing and they
will add
so this is my candidate solution
let's copy it here
let's uncomment this and cross our
fingers
hey
so that's it we've back propagated
through
this entire Beast
so there we go totally makes sense
so now we come to exercise two it
basically turns out that in this first
exercise we were doing way too much work
uh we were back propagating way too much
and it was all good practice and so on
but it's not what you would do in
practice and the reason for that is for
example here I separated out this loss
calculation over multiple lines and I
broke it up all all to like its smallest
atomic pieces and we back propagated
through all of those individually
but it turns out that if you just look
at the mathematical expression for the
loss
um then actually you can do the
differentiation on pen and paper and a
lot of terms cancel and simplify and the
mathematical expression you end up with
can be significantly shorter and easier
to implement than back propagating
through all the little pieces of
everything you've done
so before we had this complicated
forward paths going from logits to the
loss
but in pytorch everything can just be
glued together into a single call at
that cross entropy you just pass in
logits and the labels and you get the
exact same loss as I verify here so our
previous loss and the fast loss coming
from the chunk of operations as a single
mathematical expression is the same but
it's much much faster in a forward pass
it's also much much faster in backward
pass and the reason for that is if you
just look at the mathematical form of
this and differentiate again you will
end up with a very small and short
expression so that's what we want to do
here we want to in a single operation or
in a single go or like very quickly go
directly to delojits
and we need to implement the logits as a
function of logits and yb's
but it will be significantly shorter
than whatever we did here where to get
to deluggets we had to go all the way
here
so all of this work can be skipped in a
much much simpler mathematical
expression that you can Implement here
so you can give it a shot yourself
basically look at what exactly is the
mathematical expression of loss and
differentiate with respect to the logits
so let me show you a hint you can of
course try it fully yourself but if not
I can give you some hint of how to get
started mathematically
so basically What's Happening Here is we
have logits then there's a softmax that
takes the logits and gives you
probabilities then we are using the
identity of the correct next character
to pluck out a row of probabilities take
the negative log of it to get our
negative block probability and then we
average up all the log probabilities or
negative block probabilities to get our
loss
so basically what we have is for a
single individual example rather we have
that loss is equal to negative log
probability uh where P here is kind of
like thought of as a vector of all the
probabilities so at the Y position where
Y is the label
and we have that P here of course is the
softmax so the ith component of P of
this probability Vector is just the
softmax function so raising all the
logits uh basically to the power of E
and normalizing so everything comes to
1.
now if you write out P of Y here you can
just write out the soft Max and then
basically what we're interested in is
we're interested in the derivative of
the loss with respect to the I logit
and so basically it's a d by DLI of this
expression here
where we have L indexed with the
specific label Y and on the bottom we
have a sum over J of e to the L J and
the negative block of all that so
potentially give it a shot pen and paper
and see if you can actually derive the
expression for the loss by DLI and then
we're going to implement it here okay so
I'm going to give away the result here
so this is some of the math I did to
derive the gradients analytically and so
we see here that I'm just applying the
rules of calculus from your first or
second year of bachelor's degree if you
took it and we see that the expression
is actually simplify quite a bit you
have to separate out the analysis in the
case where the ith index that you're
interested in inside logits is either
equal to the label or it's not equal to
the label and then the expression
simplify and cancel in a slightly
different way and what we end up with is
something very very simple
and we either end up with basically
pirai where p is again this Vector of
probabilities after a soft Max or P at I
minus 1 where we just simply subtract a
one but in any case we just need to
calculate the soft Max p e and then in
the correct Dimension we need to
subtract one and that's the gradient the
form that it takes analytically so let's
implement this basically and we have to
keep in mind that this is only done for
a single example but here we are working
with batches of examples
so we have to be careful of that and
then the loss for a batch is the average
loss over all the examples so in other
words is the example for all the
individual examples is the loss for each
individual example summed up and then
divided by n and we have to back
propagate through that as well and be
careful with it
so deluggets is going to be of that soft
Max
uh pytorch has a softmax function that
you can call and we want to apply the
softmax on the logits and we want to go
in the dimension that is one so
basically we want to do the softmax
along the rows of these logits
then at the correct positions we need to
subtract a 1. so delugits at iterating
over all the rows
and indexing into the columns
provided by the correct labels inside YB
we need to subtract one
and then finally it's the average loss
that is the loss and in the average
there's a one over n of all the losses
added up and so we need to also
propagate through that division
so the gradient has to be scaled down by
by n as well because of the mean
but this otherwise should be the result
so now if we verify this
we see that we don't get an exact match
but at the same time the maximum
difference from logits from pytorch and
RD logits here is uh on the order of 5e
negative 9. so it's a tiny tiny number
so because of floating point wantiness
we don't get the exact bitwise result
but we basically get the correct answer
approximately
now I'd like to pause here briefly
before we move on to the next exercise
because I'd like us to get an intuitive
sense of what the logits is because it
has a beautiful and very simple
explanation honestly
um so here I'm taking the logits and I'm
visualizing it and we can see that we
have a batch of 32 examples of 27
characters
and what is the logits intuitively right
the logits is the probabilities that the
properties Matrix in the forward pass
but then here these black squares are
the positions of the correct indices
where we subtracted a one
and so uh what is this doing right these
are the derivatives on the logits and so
let's look at just the first row here
so that's what I'm doing here I'm
clocking the probabilities of these
logits and then I'm taking just the
first row and this is the probability
row and then the logits of the first row
and multiplying by n just for us so that
we don't have the scaling by n in here
and everything is more interpretable we
see that it's exactly equal to the
probability of course but then the
position of the correct index has a
minus equals one so minus one on that
position
and so notice that
um if you take Delo Jets at zero and you
sum it
it actually sums to zero and so you
should think of these uh gradients here
at each cell as like a force
um we are going to be basically pulling
down on the probabilities of the
incorrect characters and we're going to
be pulling up on the probability at the
correct index and that's what's
basically happening in each row and thus
the amount of push and pull is exactly
equalized because the sum is zero so the
amount to which we pull down in the
probabilities and the demand that we
push up on the probability of the
correct character is equal
so sort of the the repulsion and the
attraction are equal and think of the
neural app now as a like a massive uh
pulley system or something like that
we're up here on top of the logits and
we're pulling up we're pulling down the
properties of Incorrect and pulling up
the property of the correct and in this
complicated pulley system because
everything is mathematically uh just
determined just think of it as sort of
like this tension translating to this
complicating pulling mechanism and then
eventually we get a tug on the weights
and the biases and basically in each
update we just kind of like tug in the
direction that we like for each of these
elements and the parameters are slowly
given in to the tug and that's what
training in neural net kind of like
looks like on a high level
and so I think the the forces of push
and pull in these gradients are actually
uh very intuitive here we're pushing and
pulling on the correct answer and the
incorrect answers and the amount of
force that we're applying is actually
proportional to uh the probabilities
that came out in the forward pass
and so for example if our probabilities
came out exactly correct so they would
have had zero everywhere except for one
at the correct uh position then the the
logits would be all a row of zeros for
that example there would be no push and
pull so the amount to which your
prediction is incorrect is exactly the
amount by which you're going to get a
pull or a push in that dimension
so if you have for example a very
confidently mispredicted element here
then
um what's going to happen is that
element is going to be pulled down very
heavily and the correct answer is going
to be pulled up to the same amount
and the other characters are not going
to be influenced too much
so the amounts to which you mispredict
is then proportional to the strength of
the pole and that's happening
independently in all the dimensions of
this of this tensor and it's sort of
very intuitive and varies to think
through and that's basically the magic
of the cross-entropy loss and what it's
doing dynamically in the backward pass
of the neural net so now we get to
exercise number three which is a very
fun exercise
um depending on your definition of fun
and we are going to do for batch
normalization exactly what we did for
cross entropy loss in exercise number
two that is we are going to consider it
as a glued single mathematical
expression and back propagate through it
in a very efficient manner because we
are going to derive a much simpler
formula for the backward path of batch
normalization
and we're going to do that using pen and
paper
so previously we've broken up
bastionalization into all of the little
intermediate pieces and all the atomic
operations inside it and then we back
propagate it through it one by one
now we just have a single sort of
forward pass of a batch form and it's
all glued together
and we see that we get the exact same
result as before
now for the backward pass we'd like to
also Implement a single formula
basically for back propagating through
this entire operation that is the
bachelorization
so in the forward pass previously we
took hpvn the hidden states of the
pre-batch realization and created H
preact which is the hidden States just
before the activation
in the bachelorization paper each pbn is
X and each preact is y
so in the backward pass what we'd like
to do now is we have DH preact and we'd
like to produce d h previous
and we'd like to do that in a very
efficient manner so that's the name of
the game calculate the H previan given
DH preact and for the purposes of this
exercise we're going to ignore gamma and
beta and their derivatives because they
take on a very simple form in a very
similar way to what we did up above
so let's calculate this given that right
here
so to help you a little bit like I did
before I started off the implementation
here on pen and paper and I took two
sheets of paper to derive the
mathematical formulas for the backward
pass
and basically to set up the problem uh
just write out the MU Sigma Square
variance x i hat and Y I exactly as in
the paper except for the bezel
correction
and then
in a backward pass we have the
derivative of the loss with respect to
all the elements of Y and remember that
Y is a vector there's there's multiple
numbers here
so we have all the derivatives with
respect to all the Y's
and then there's a demo and a beta and
this is kind of like the compute graph
the gamma and the beta there's the X hat
and then the MU and the sigma squared
and the X so we have DL by DYI and we
won't DL by d x i for all the I's in
these vectors
so this is the compute graph and you
have to be careful because I'm trying to
note here that these are vectors so
there's many nodes here inside x x hat
and Y but mu and sigma sorry Sigma
Square are just individual scalars
single numbers so you have to be careful
with that you have to imagine there's
multiple nodes here or you're going to
get your math wrong
um so as an example I would suggest that
you go in the following order one two
three four in terms of the back
propagation so back propagating to X hat
then into Sigma Square then into mu and
then into X
um just like in a topological sort in
micrograd we would go from right to left
you're doing the exact same thing except
you're doing it with symbols and on a
piece of paper
so for number one uh I'm not giving away
too much if you want DL of d x i hat
then we just take DL by DYI and multiply
it by gamma because of this expression
here where any individual Yi is just
gamma times x i hat plus beta so it
doesn't help you too much there but this
gives you basically the derivatives for
all the X hats and so now try to go
through this computational graph and
derive what is DL by D Sigma Square
and then what is DL by B mu and then one
is D L by DX
eventually so give it a go and I'm going
to be revealing the answer one piece at
a time okay so to get DL by D Sigma
Square we have to remember again like I
mentioned that there are many excess X
hats here
and remember that Sigma square is just a
single individual number here
so when we look at the expression
for the L by D Sigma Square
we have that we have to actually
consider all the possible paths that um
we basically have that there's many X
hats and they all feed off from they all
depend on Sigma Square so Sigma square
has a large fan out there's lots of
arrows coming out from Sigma square into
all the X hats
and then there's a back propagating
signal from each X hat into Sigma square
and that's why we actually need to sum
over all those I's from I equal to 1 to
m
of the DL by d x i hat which is the
global gradient
times the x i Hat by D Sigma Square
which is the local gradient
of this operation here
and then mathematically I'm just working
it out here and I'm simplifying and you
get a certain expression for DL by D
Sigma square and we're going to be using
this expression when we back propagate
into mu and then eventually into X so
now let's continue our back propagation
into mu so what is D L by D mu now again
be careful that mu influences X hat and
X hat is actually lots of values so for
example if our mini batch size is 32 as
it is in our example that we were
working on then this is 32 numbers and
32 arrows going back to mu and then mu
going to Sigma square is just a single
Arrow because Sigma square is a scalar
so in total there are 33 arrows
emanating from you and then all of them
have gradients coming into mu and they
all need to be summed up
and so that's why when we look at the
expression for DL by D mu I am summing
up over all the gradients of DL by d x i
hat times the x i Hat by being mu
uh so that's the that's this arrow and
that's 32 arrows here and then plus the
one Arrow from here which is the L by
the sigma Square Times the sigma squared
by D mu
so now we have to work out that
expression and let me just reveal the
rest of it
uh simplifying here is not complicated
the first term and you just get an
expression here
for the second term though there's
something really interesting that
happens
when we look at the sigma squared by D
mu and we simplify
at one point if we assume that in a
special case where mu is actually the
average of X I's as it is in this case
then if we plug that in then actually
the gradient vanishes and becomes
exactly zero and that makes the entire
second term cancel
and so these uh if you just have a
mathematical expression like this and
you look at D Sigma Square by D mu you
would get some mathematical formula for
how mu impacts Sigma Square
but if it is the special case that Nu is
actually equal to the average as it is
in the case of pastoralization that
gradient will actually vanish and become
zero so the whole term cancels and we
just get a fairly straightforward
expression here for DL by D mu okay and
now we get to the craziest part which is
uh deriving DL by dxi which is
ultimately what we're after
now let's count
first of all how many numbers are there
inside X as I mentioned there are 32
numbers there are 32 Little X I's and
let's count the number of arrows
emanating from each x i
there's an arrow going to Mu an arrow
going to Sigma Square
and then there's an arrow going to X hat
but this Arrow here let's scrutinize
that a little bit
each x i hat is just a function of x i
and all the other scalars so x i hat
only depends on x i and none of the
other X's
and so therefore there are actually in
this single Arrow there are 32 arrows
but those 32 arrows are going exactly
parallel they don't interfere and
they're just going parallel between x
and x hat you can look at it that way
and so how many arrows are emanating
from each x i there are three arrows mu
Sigma squared and the associated X hat
and so in back propagation we now need
to apply the chain rule and we need to
add up those three contributions
so here's what that looks like if I just
write that out
we have uh we're going through we're
chaining through mu Sigma square and
through X hat and those three terms are
just here
now we already have three of these we
have d l by d x i hat
we have DL by D mu which we derived here
and we have DL by D Sigma Square which
we derived here but we need three other
terms here
the this one this one and this one so I
invite you to try to derive them it's
not that complicated you're just looking
at these Expressions here and
differentiating with respect to x i
so give it a shot but here's the result
or at least what I got
um
yeah I'm just I'm just differentiating
with respect to x i for all these
expressions and honestly I don't think
there's anything too tricky here it's
basic calculus
now it gets a little bit more tricky is
we are now going to plug everything
together so all of these terms
multiplied with all of these terms and
add it up according to this formula and
that gets a little bit hairy so what
ends up happening is
uh
you get a large expression and the thing
to be very careful with here of course
is we are working with a DL by dxi for
specific I here but when we are plugging
in some of these terms
like say
um
this term here deal by D signal squared
you see how the L by D Sigma squared I
end up with an expression and I'm
iterating over little I's here but I
can't use I as the variable when I plug
in here because this is a different I
from this eye
this I here is just a place or like a
local variable for for a for Loop in
here so here when I plug that in you
notice that I rename the I to a j
because I need to make sure that this J
is not that this J is not this I this J
is like like a little local iterator
over 32 terms and so you have to be
careful with that when you're plugging
in the expressions from here to here you
may have to rename eyes into J's and you
have to be very careful what is actually
an I with respect to the L by t x i
so some of these are J's some of these
are I's
and then we simplify this expression
and I guess like the big thing to notice
here is a bunch of terms just kind of
come out to the front and you can
refactor them there's a sigma squared
plus Epsilon raised to the power of
negative three over two uh this Sigma
squared plus Epsilon can be actually
separated out into three terms each of
them are Sigma squared plus Epsilon to
the negative one over two so the three
of them multiplied is equal to this and
then those three terms can go different
places because of the multiplication so
one of them actually comes out to the
front and will end up here outside one
of them joins up with this term and one
of them joins up with this other term
and then when you simplify the
expression you'll notice that some of
these terms that are coming out are just
the x i hats
so you can simplify just by rewriting
that
and what we end up with at the end is a
fairly simple mathematical expression
over here that I cannot simplify further
but basically you'll notice that it only
uses the stuff we have and it derives
the thing we need so we have the L by d
y for all the I's and those are used
plenty of times here and also in
addition what we're using is these x i
hats and XJ hats and they just come from
the forward pass
and otherwise this is a simple
expression and it gives us DL by d x i
for all the I's and that's ultimately
what we're interested in
so that's the end of Bachelor backward
pass analytically let's now implement
this final result
okay so I implemented the expression
into a single line of code here and you
can see that the max diff is Tiny so
this is the correct implementation of
this formula now I'll just uh
basically tell you that getting this
formula here from this mathematical
expression was not trivial and there's a
lot going on packed into this one
formula and this is a whole exercise by
itself because you have to consider the
fact that this formula here is just for
a single neuron and a batch of 32
examples but what I'm doing here is I'm
actually we actually have 64 neurons and
so this expression has to in parallel
evaluate the bathroom backward pass for
all of those 64 neurons in parallel
independently so this has to happen
basically in every single
um
column of the inputs here
and in addition to that you see how
there are a bunch of sums here and we
need to make sure that when I do those
sums that they broadcast correctly onto
everything else that's here
and so getting this expression is just
like highly non-trivial and I invite you
to basically look through it and step
through it and it's a whole exercise to
make sure that this this checks out but
once all the shapes are green and once
you convince yourself that it's correct
you can also verify that Patrick's gets
the exact same answer as well and so
that gives you a lot of peace of mind
that this mathematical formula is
correctly implemented here and
broadcasted correctly and replicated in
parallel for all of the 64 neurons
inside this bastrum layer okay and
finally exercise number four asks you to
put it all together and uh here we have
a redefinition of the entire problem so
you see that we reinitialize the neural
nut from scratch and everything and then
here instead of calling loss that
backward we want to have the manual back
propagation here as we derived It Up
Above so go up copy paste all the chunks
of code that we've already derived put
them here and drive your own gradients
and then optimize this neural nut
basically using your own gradients all
the way to the calibration of The
Bachelor and the evaluation of the loss
and I was able to achieve quite a good
loss basically the same loss you would
achieve before and that shouldn't be
surprising because all we've done is
we've really gotten to Lost That
backward and we've pulled out all the
code
and inserted it here but those gradients
are identical and everything is
identical and the results are identical
it's just that we have full visibility
on exactly what goes on under the hood
I'll plot that backward in this specific
case and this is all of our code this is
the full backward pass using basically
the simplified backward pass for the
cross entropy loss and the mass
generalization so back propagating
through cross entropy the second layer
the 10 H nonlinearity the batch
normalization
uh through the first layer and through
the embedding and so you see that this
is only maybe what is this 20 lines of
code or something like that and that's
what gives us gradients and now we can
potentially erase losses backward so the
way I have the code set up is you should
be able to run this entire cell once you
fill this in and this will run for only
100 iterations and then break
and it breaks because it gives you an
opportunity to check your gradients
against pytorch
so here our gradients we see are not
exactly equal they are approximately
equal and the differences are tiny
wanting negative 9 or so and I don't
exactly know where they're coming from
to be honest
um so once we have some confidence that
the gradients are basically correct we
can take out the gradient tracking
we can disable this breaking statement
and then we can
basically disable lost of backward we
don't need it anymore it feels amazing
to say that
and then here when we are doing the
update we're not going to use P dot grad
this is the old way of pytorch we don't
have that anymore because we're not
doing backward we are going to use this
update where we you see that I'm
iterating over
I've arranged the grads to be in the
same order as the parameters and I'm
zipping them up the gradients and the
parameters into p and grad and then here
I'm going to step with just the grad
that we derived manually
so the last piece
um is that none of this now requires
gradients from pytorch and so one thing
you can do here
um
is you can do with no grad and offset
this whole code block
and really what you're saying is you're
telling Pat George that hey I'm not
going to call backward on any of this
and this allows pytorch to be a bit more
efficient with all of it
and then we should be able to just uh
run this
and
it's running
and you see that losses backward is
commented out
and we're optimizing
so we're going to leave this run and uh
hopefully we get a good result
okay so I allowed the neural net to
finish optimization
then here I calibrate the bachelor
parameters because I did not keep track
of the running mean and very variants in
their training Loop
then here I ran the loss and you see
that we actually obtained a pretty good
loss very similar to what we've achieved
before
and then here I'm sampling from the
model and we see some of the name like
gibberish that we're sort of used to so
basically the model worked and samples
uh pretty decent results compared to
what we were used to so everything is
the same but of course the big deal is
that we did not use lots of backward we
did not use package Auto grad and we
estimated our gradients ourselves by
hand
and so hopefully you're looking at this
the backward pass of this neural net and
you're thinking to yourself actually
that's not too complicated
um
each one of these layers is like three
lines of code or something like that and
most of it is fairly straightforward
potentially with the notable exception
of the batch normalization backward pass
otherwise it's pretty good okay and
that's everything I wanted to cover for
this lecture so hopefully you found this
interesting and what I liked about it
honestly is that it gave us a very nice
diversity of layers to back propagate
through and
um I think it gives a pretty nice and
comprehensive sense of how these
backward passes are implemented and how
they work and you'd be able to derive
them yourself but of course in practice
you probably don't want to and you want
to use the pythonograd but hopefully you
have some intuition about how gradients
flow backwards through the neural net
starting at the loss and how they flow
through all the variables and all the
intermediate results
and if you understood a good chunk of it
and if you have a sense of that then you
can count yourself as one of these buff
doji's on the left instead of the uh
those on the right here now in the next
lecture we're actually going to go to
recurrent neural nuts lstms and all the
other variants of RNs and we're going to
start to complexify the architecture and
start to achieve better uh log
likelihoods and so I'm really looking
forward to that and I'll see you thenhi everyone today we are continuing our
implementation of make more our favorite
character level language model
now you'll notice that the background
behind me is different that's because I
am in Kyoto and it is awesome so I'm in
a hotel room here
now over the last few lectures we've
built up to this architecture that is a
multi-layer perceptron character level
language model so we see that it
receives three previous characters and
tries to predict the fourth character in
a sequence using a very simple multi
perceptron using one hidden layer of
neurons with 10ational neuralities
so we'd like to do now in this lecture
is I'd like to complexify this
architecture in particular we would like
to take more characters in a sequence as
an input not just three and in addition
to that we don't just want to feed them
all into a single hidden layer because
that squashes too much information too
quickly instead we would like to make a
deeper model that progressively fuses
this information to make its guess about
the next character in a sequence
and so we'll see that as we make this
architecture more complex we're actually
going to arrive at something that looks
very much like a wavenet
the witness is this paper published by
the point in 2016 and it is also a
language model basically but it tries to
predict audio sequences instead of
character level sequences or Word level
sequences but fundamentally the modeling
setup is identical it is an auto
aggressive model and it tries to predict
next character in a sequence and the
architecture actually takes this
interesting hierarchical sort of
approach to predicting the next
character in a sequence uh with the
street-like structure and this is the
architecture and we're going to
implement it in the course of this video
so let's get started so the starter code
for part five is very similar to where
we ended up in in part three recall that
part four was the manual black
replication exercise that is kind of an
aside so we are coming back to part
three copy pasting chunks out of it and
that is our starter code for part five
I've changed very few things otherwise
so a lot of this should look familiar to
if you've gone through part three so in
particular very briefly we are doing
Imports we are reading our our data set
of words and we are processing their set
of words into individual examples and
none of this data generation code has
changed and basically we have lots and
lots of examples in particular we have
182 000 examples of three characters try
to predict the fourth one and we've
broken up every one of these words into
little problems of given three
characters predict the fourth one so
this is our data set and this is what
we're trying to get the neural lot to do
now in part three we started to develop
our code around these layer modules
um that are for example like class
linear and we're doing this because we
want to think of these modules as
building blocks and like a Lego building
block bricks that we can sort of like
stack up into neural networks and we can
feed data between these layers and stack
them up into a sort of graphs
now we also developed these layers to
have apis and signatures very similar to
those that are found in pytorch so we
have torch.nn and it's got all these
layer building blocks that you would use
in practice and we were developing all
of these to mimic the apis of these so
for example we have linear so there will
also be a torch.nn.linear and its
signature will be very similar to our
signature and the functionality will be
also quite identical as far as I'm aware
so we have the linear layer with the
Bass from 1D layer and the 10h layer
that we developed previously
and linear just as a matrix multiply in
the forward pass of this module batch
number of course is this crazy layer
that we developed in the previous
lecture and what's crazy about it is
well there's many things number one it
has these running mean and variances
that are trained outside of back
propagation they are trained using
exponential moving average inside this
layer when we call the forward pass
in addition to that
there's this training plug because the
behavior of bathroom is different during
train time and evaluation time and so
suddenly we have to be very careful that
bash form is in its correct state that
it's in the evaluation state or training
state so that's something to now keep
track of something that sometimes
introduces bugs
uh because you forget to put it into the
right mode and finally we saw that
Bachelor couples the statistics or the
the activations across the examples in
the batch so normally we thought of the
bat as just an efficiency thing but now
we are coupling the computation across
batch elements and it's done for the
purposes of controlling the automation
statistics as we saw in the previous
video
so it's a very weird layer at least a
lot of bugs
partly for example because you have to
modulate the training in eval phase and
so on
um in addition for example you have to
wait for uh the mean and the variance to
settle and to actually reach a steady
state and so um you have to make sure
that you basically there's state in this
layer and state is harmful uh usually
now I brought out the generator object
previously we had a generator equals g
and so on inside these layers I've
discarded that in favor of just
initializing the torch RNG outside here
use it just once globally just for
Simplicity
and then here we are starting to build
out some of the neural network elements
this should look very familiar we are we
have our embedding table C and then we
have a list of players and uh it's a
linear feeds to Bachelor feeds to 10h
and then a linear output layer and its
weights are scaled down so we are not
confidently wrong at the initialization
we see that this is about 12 000
parameters we're telling pytorch that
the parameters require gradients
the optimization is as far as I'm aware
identical and should look very very
familiar
nothing changed here
uh loss function looks very crazy we
should probably fix this and that's
because 32 batch elements are too few
and so you can get very lucky lucky or
unlucky in any one of these batches and
it creates a very thick loss function
um so we're going to fix that soon
now once we want to evaluate the trained
neural network we need to remember
because of the bathroom layers to set
all the layers to be training equals
false so this only matters for the
bathroom layer so far
and then we evaluate
we see that currently we have validation
loss of 2.10 which is fairly good but
there's still ways to go but even at
2.10 we see that when we sample from the
model we actually get relatively
name-like results that do not exist in a
training set so for example Yvonne kilo
Pros
Alaia Etc so certainly not
reasonable not unreasonable I would say
but not amazing and we can still push
this validation loss even lower and get
much better samples that are even more
name-like
so let's improve this model
okay first let's fix this graph because
it is daggers in my eyes and I just
can't take it anymore
um so last I if you recall is a python
list of floats so for example the first
10 elements
now what we'd like to do basically is we
need to average up
um some of these values to get a more
sort of Representative uh value along
the way so one way to do this is the
following
in part torch if I create for example
a tensor of the first 10 numbers
then this is currently a one-dimensional
array but recall that I can view this
array as two-dimensional so for example
I can use it as a two by five array and
this is a 2d tensor now two by five and
you see what petroch has done is that
the first row of this tensor is the
first five elements and the second row
is the second five elements
I can also view it as a five by two as
an example
and then recall that I can also
use negative one in place of one of
these numbers
and pytorch will calculate what that
number must be in order to make the
number of elements work out so this can
be
this or like that but it will work of
course this would not work
okay so this allows it to spread out
some of the consecutive values into rows
so that's very helpful because what we
can do now is first of all we're going
to create a torshot tensor out of the a
list of floats
and then we're going to view it as
whatever it is but we're going to
stretch it out into rows of 1000
consecutive elements so the shape of
this now becomes 200 by 1000. and each
row is one thousand um consecutive
elements in this list
so that's very helpful because now we
can do a mean along the rows
and the shape of this will just be 200.
and so we've taken basically the mean on
every row so plt.plot of that should be
something nicer
much better
so we see that we basically made a lot
of progress and then here this is the
learning rate Decay so here we see that
the learning rate Decay subtracted a ton
of energy out of the system and allowed
us to settle into sort of the local
minimum in this optimization
so this is a much nicer plot let me come
up and delete the monster and we're
going to be using this going forward now
next up what I'm bothered by is that you
see our forward pass is a little bit
gnarly and takes way too many lines of
code
so in particular we see that we've
organized some of the layers inside the
layers list but not all of them uh for
no reason so in particular we see that
we still have the embedding table a
special case outside of the layers and
in addition to that the viewing
operation here is also outside of our
layers so let's create layers for these
and then we can add those layers to just
our list
so in particular the two things that we
need is here we have this embedding
table and we are indexing at the
integers inside uh the batch XB uh
inside the tensor xB
so that's an embedding table lookup just
done with indexing and then here we see
that we have this view operation which
if you recall from the previous video
Simply rearranges the character
embeddings and stretches them out into a
row and effectively what print that does
is the concatenation operation basically
except it's free because viewing is very
cheap in pytorch no no memory is being
copied we're just re-representing how we
view that tensor so let's create
um
modules for both of these operations the
embedding operation and flattening
operation
so I actually wrote the code in just to
save some time
so we have a module embedding and a
module pattern and both of them simply
do the indexing operation in the forward
pass and the flattening operation here
and this C now will just become a salt
dot weight inside an embedding module
and I'm calling these layers
specifically embedding a platinum
because it turns out that both of them
actually exist in pi torch so in
phytorch we have n and Dot embedding and
it also takes the number of embeddings
and the dimensionality of the bedding
just like we have here but in addition
python takes in a lot of other keyword
arguments that we are not using for our
purposes yet
and for flatten that also exists in
pytorch and it also takes additional
keyword arguments that we are not using
so we have a very simple platform
but both of them exist in pytorch
they're just a bit more simpler and now
that we have these we can simply take
out some of these special cased
um things so instead of C we're just
going to have an embedding
and of a cup size and N embed
and then after the embedding we are
going to flatten
so let's construct those modules and now
I can take out this the
and here I don't have to special case
anymore because now C is the embeddings
weight and it's inside layers
so this should just work
and then here our forward pass
simplifies substantially because we
don't need to do these now outside of
these layer outside and explicitly
they're now inside layers
so we can delete those
but now to to kick things off we want
this little X which in the beginning is
just XB uh the tensor of integers
specifying the identities of these
characters at the input
and so these characters can now directly
feed into the first layer and this
should just work
so let me come here and insert a break
because I just want to make sure that
the first iteration of this runs and
then there's no mistake so that ran
properly and basically we substantially
simplified the forward pass here okay
I'm sorry I changed my microphone so
hopefully the audio is a little bit
better
now one more thing that I would like to
do in order to pytortify our code even
further is that right now we are
maintaining all of our modules in a
naked list of layers and we can also
simplify this uh because we can
introduce the concept of Pi torch
containers so in tors.nn which we are
basically rebuilding from scratch here
there's a concept of containers
and these containers are basically a way
of organizing layers into
lists or dicts and so on so in
particular there's a sequential which
maintains a list of layers and is a
module class in pytorch and it basically
just passes a given input through all
the layers sequentially exactly as we
are doing here
so let's write our own sequential
I've written a code here and basically
the code for sequential is quite
straightforward we pass in a list of
layers which we keep here and then given
any input in a forward pass we just call
all the layers sequentially and return
the result in terms of the parameters
it's just all the parameters of the
child modules
so we can run this and we can again
simplify this substantially because we
don't maintain this naked list of layers
we now have a notion of a model which is
a module and in particular is a
sequential of all these layers
and now parameters are simply just a
model about parameters
and so that list comprehension now lives
here
and then here we are press here we are
doing all the things we used to do
now here the code again simplifies
substantially because we don't have to
do this forwarding here instead of just
call the model on the input data and the
input data here are the integers inside
xB so we can simply do logits which are
the outputs of our model are simply the
model called on xB
and then the cross entropy here takes
the logits and the targets
so this simplifies substantially
and then this looks good so let's just
make sure this runs that looks good
now here we actually have some work to
do still here but I'm going to come back
later for now there's no more layers
there's a model that layers but it's not
a to access attributes of these classes
directly so we'll come back and fix this
later
and then here of course this simplifies
substantially as well because logits are
the model called on x
and then these low Jets come here
so we can evaluate the train and
validation loss which currently is
terrible because we just initialized the
neural net and then we can also sample
from the model and this simplifies
dramatically as well
because we just want to call the model
onto the context and outcome logits
and these logits go into softmax and get
the probabilities Etc so we can sample
from this model
what did I screw up
okay so I fixed the issue and we now get
the result that we expect which is
gibberish because the model is not
trained because we re-initialize it from
scratch
the problem was that when I fixed this
cell to be modeled out layers instead of
just layers I did not actually run the
cell and so our neural net was in a
training mode and what caused the issue
here is the bathroom layer as bathroom
layer of the likes to do because
Bachelor was in a training mode and here
we are passing in an input which is a
batch of just a single example made up
of the context
and so if you are trying to pass in a
single example into a bash Norm that is
in the training mode you're going to end
up estimating the variance using the
input and the variance of a single
number is is not a number because it is
a measure of a spread so for example the
variance of just the single number five
you can see is not a number and so
that's what happened in the master
basically caused an issue and then that
polluted all of the further processing
so all that we have to do was make sure
that this runs and we basically made the
issue of
again we didn't actually see the issue
with the loss we could have evaluated
the loss but we got the wrong result
because basharm was in the training mode
and uh and so we still get a result it's
just the wrong result because it's using
the uh sample statistics of the batch
whereas we want to use the running mean
and running variants inside the bachelor
and so
again an example of introducing a bug
inline because we did not properly
maintain the state of what is training
or not okay so I Rewritten everything
and here's where we are as a reminder we
have the training loss of 2.05 and
validation 2.10
now because these losses are very
similar to each other we have a sense
that we are not overfitting too much on
this task and we can make additional
progress in our performance by scaling
up the size of the neural network and
making everything bigger and deeper
now currently we are using this
architecture here where we are taking in
some number of characters going into a
single hidden layer and then going to
the prediction of the next character
the problem here is we don't have a
naive way of making this bigger in a
productive way we could of course use
our layers sort of building blocks and
materials to introduce additional layers
here and make the network deeper but it
is still the case that we are crushing
all of the characters into a single
layer all the way at the beginning
and even if we make this a bigger layer
and add neurons it's still kind of like
silly to squash all that information so
fast in a single step
so we'd like to do instead is we'd like
our Network to look a lot more like this
in the wavenet case so you see in the
wavenet when we are trying to make the
prediction for the next character in the
sequence it is a function of the
previous characters that are feeding
that feed in but not all of these
different characters are not just
crushed to a single layer and then you
have a sandwich they are crushed slowly
so in particular we take two characters
and we fuse them into sort of like a
diagram representation and we do that
for all these characters consecutively
and then we take the bigrams and we fuse
those into four character level chunks
and then we fuse that again and so we do
that in this like tree-like hierarchical
manner so we fuse the information from
the previous context slowly into the
network as it gets deeper and so this is
the kind of architecture that we want to
implement
now in the wave Nets case this is a
visualization of a stack of dilated
causal convolution layers and this makes
it sound very scary but actually the
idea is very simple and the fact that
it's a dilated causal convolution layer
is really just an implementation detail
to make everything fast we're going to
see that later but for now let's just
keep the basic idea of it which is this
Progressive Fusion so we want to make
the network deeper and at each level we
want to fuse only two consecutive
elements two characters then two bigrams
then two four grams and so on so let's
unplant this okay so first up let me
scroll to where we built the data set
and let's change the block size from 3
to 8. so we're going to be taking eight
characters of context to predict the
ninth character so the data set now
looks like this we have a lot more
context feeding in to predict any next
character in a sequence and these eight
characters are going to be processed in
this tree like structure
now if we scroll here everything here
should just be able to work so we should
be able to redefine the network
you see the number of parameters has
increased by 10 000 and that's because
the block size has grown so this first
linear layer is much much bigger our
linear layer now takes eight characters
into this middle layer so there's a lot
more parameters there but this should
just run let me just break right after
the very first iteration so you see that
this runs just fine it's just that this
network doesn't make too much sense
we're crushing way too much information
way too fast
so let's now come in and see how we
could try to implement the hierarchical
scheme now before we dive into the
detail of the re-implementation here I
was just curious to actually run it and
see where we are in terms of the
Baseline performance of just lazily
scaling up the context length so I'll
let it run we get a nice loss curve and
then evaluating the loss we actually see
quite a bit of improvement just from
increasing the context line length so I
started a little bit of a performance
log here and previously where we were is
we were getting a performance of 2.10 on
the validation loss and now simply
scaling up the contact length from 3 to
8 gives us a performance of 2.02 so
quite a bit of an improvement here and
also when you sample from the model you
see that the names are definitely
improving qualitatively as well
so we could of course spend a lot of
time here tuning
um uh tuning things and making it even
bigger and scaling up the network
further even with the simple
um sort of setup here but let's continue
and let's Implement here model and treat
this as just a rough Baseline
performance but there's a lot of
optimization like left on the table in
terms of some of the hyper parameters
that you're hopefully getting a sense of
now okay so let's scroll up now
and come back up and what I've done here
is I've created a bit of a scratch space
for us to just like look at the forward
pass of the neural net and inspect the
shape of the tensor along the way as the
neural net uh forwards so here I'm just
temporarily for debugging creating a
batch of just say four examples so four
random integers then I'm plucking out
those rows from our training set
and then I'm passing into the model the
input xB
now the shape of XB here because we have
only four examples is four by eight and
this eight is now the current block size
so uh inspecting XP we just see that we
have four examples each one of them is a
row of xB
and we have eight characters here and
this integer tensor just contains the
identities of those characters
so the first layer of our neural net is
the embedding layer so passing XB this
integer tensor through the embedding
layer creates an output that is four by
eight by ten
so our embedding table has for each
character a 10-dimensional vector that
we are trying to learn
and so what the embedding layer does
here is it plucks out the embedding
Vector for each one of these integers
and organizes it all in a four by eight
by ten tensor now
so all of these integers are translated
into 10 dimensional vectors inside this
three-dimensional tensor now
passing that through the flattened layer
as you recall what this does is it views
this tensor as just a 4 by 80 tensor and
what that effectively does is that all
these 10 dimensional embeddings for all
these eight characters just end up being
stretched out into a long row
and that looks kind of like a
concatenation operation basically so by
viewing the tensor differently we now
have a four by eighty and inside this 80
it's all the 10 dimensional uh
vectors just uh concatenate next to each
other
and then the linear layer of course
takes uh 80 and creates 200 channels
just via matrix multiplication
so so far so good now I'd like to show
you something surprising
let's look at the insides of the linear
layer and remind ourselves how it works
the linear layer here in the forward
pass takes the input X multiplies it
with a weight and then optionally adds
bias and the weight here is
two-dimensional as defined here and the
bias is one dimensional here
so effectively in terms of the shapes
involved what's happening inside this
linear layer looks like this right now
and I'm using random numbers here but
I'm just illustrating the shapes and
what happens
basically a 4 by 80 input comes into the
linear layer that's multiplied by this
80 by 200 weight Matrix inside and
there's a plus 200 bias and the shape of
the whole thing that comes out of the
linear layer is four by two hundred as
we see here
now notice here by the way that this
here will create a 4x200 tensor and then
plus 200 there's a broadcasting
happening here about 4 by 200 broadcasts
with 200 uh so everything works here
so now the surprising thing that I'd
like to show you that you may not expect
is that this input here that is being
multiplied uh doesn't actually have to
be two-dimensional this Matrix multiply
operator in pytorch is quite powerful
and in fact you can actually pass in
higher dimensional arrays or tensors and
everything works fine so for example
this could be four by five by eighty and
the result in that case will become four
by five by two hundred
you can add as many dimensions as you
like on the left here
and so effectively what's happening is
that the matrix multiplication only
works on the last Dimension and the
dimensions before it in the input tensor
are left unchanged
so that is basically these um these
dimensions on the left are all treated
as just a batch Dimension so we can have
multiple batch dimensions and then in
parallel over all those Dimensions we
are doing the matrix multiplication on
the last dimension
so this is quite convenient because we
can use that in our Network now
because remember that we have these
eight characters coming in
and we don't want to now uh flatten all
of it out into a large eight-dimensional
vector
because we don't want to Matrix multiply
80.
into a weight Matrix multiply
immediately instead we want to group
these
like this
so every consecutive two elements
one two and three and four and five and
six and seven and eight all of these
should be now
basically flattened out and multiplied
by weight Matrix but all of these four
groups here we'd like to process in
parallel so it's kind of like a batch
Dimension that we can introduce
and then we can in parallel basically
process all of these uh bigram groups in
the four batch dimensions of an
individual example and also over the
actual batch dimension of the you know
four examples in our example here so
let's see how that works effectively
what we want is right now we take a 4 by
80
and multiply it by 80 by 200
to in the linear layer this is what
happens
but instead what we want is we don't
want 80 characters or 80 numbers to come
in we only want two characters to come
in on the very first layer and those two
characters should be fused
so in other words we just want 20 to
come in right 20 numbers would come in
and here we don't want a 4 by 80 to feed
into the linear layer we actually want
these groups of two to feed in so
instead of four by eighty we want this
to be a 4 by 4 by 20.
so these are the four groups of two and
each one of them is ten dimensional
vector
so what we want is now is we need to
change the flattened layer so it doesn't
output a four by eighty but it outputs a
four by four by Twenty where basically
these um
every two consecutive characters are uh
packed in on the very last Dimension and
then these four is the first batch
Dimension and this four is the second
batch Dimension referring to the four
groups inside every one of these
examples
and then this will just multiply like
this so this is what we want to get to
so we're going to have to change the
linear layer in terms of how many inputs
it expects it shouldn't expect 80 it
should just expect 20 numbers and we
have to change our flattened layer so it
doesn't just fully flatten out this
entire example it needs to create a 4x4
by 20 instead of four by eighty so let's
see how this could be implemented
basically right now we have an input
that is a four by eight by ten that
feeds into the flattened layer and
currently the flattened layer just
stretches it out so if you remember the
implementation of flatten
it takes RX and it just views it as
whatever the batch Dimension is and then
negative one
so effectively what it does right now is
it does e dot view of 4 negative one and
the shape of this of course is 4 by 80.
so that's what currently happens and we
instead want this to be a four by four
by Twenty where these consecutive
ten-dimensional vectors get concatenated
so you know how in Python you can take a
list of range of 10
so we have numbers from zero to nine and
we can index like this to get all the
even parts
and we can also index like starting at
one and going in steps up two to get all
the odd parts
so one way to implement this it would be
as follows we can take e and we can
index into it for all the batch elements
and then just even elements in this
Dimension so at indexes 0 2 4 and 8.
and then all the parts here from this
last dimension
and this gives us the even characters
and then here
this gives us all the odd characters and
basically what we want to do is we make
sure we want to make sure that these get
concatenated in pi torch and then we
want to concatenate these two tensors
along the second dimension
so this and the shape of it would be
four by four by Twenty this is
definitely the result we want we are
explicitly grabbing the even parts and
the odd parts and we're arranging those
four by four by ten right next to each
other and concatenate
so this works but it turns out that what
also works is you can simply use a view
again and just request the right shape
and it just so happens that in this case
those vectors will again end up being
arranged in exactly the way we want so
in particular if we take e and we just
view it as a four by four by Twenty
which is what we want
we can check that this is exactly equal
to but let me call this this is the
explicit concatenation I suppose
um
so explosives dot shape is 4x4 by 20. if
you just view it as 4x4 by 20 you can
check that when you compare to explicit
uh you got a big this is element wise
operation so making sure that all of
them are true that is the truth so
basically long story short we don't need
to make an explicit call to concatenate
Etc we can simply take this input tensor
to flatten and we can just view it in
whatever way we want
and in particular you don't want to
stretch things out with negative one we
want to actually create a
three-dimensional array and depending on
how many vectors that are consecutive we
want to
um fuse like for example two then we can
just simply ask for this Dimension to be
20. and um
use a negative 1 here and python will
figure out how many groups it needs to
pack into this additional batch
dimension
so let's now go into flatten and
implement this okay so I scroll up here
to flatten and what we'd like to do is
we'd like to change it now so let me
create a Constructor and take the number
of elements that are consecutive that we
would like to concatenate now in the
last dimension of the output
so here we're just going to remember
solve.n equals n
and then I want to be careful here
because pipe pytorch actually has a
torch to flatten and its keyword
arguments are different and they kind of
like function differently so R flatten
is going to start to depart from patreon
flatten so let me call it flat flatten
consecutive or something like that just
to make sure that our apis are about
equal
so this uh basically flattens only some
n consecutive elements and puts them
into the last dimension
now here the shape of X is B by T by C
so let me
pop those out into variables and recall
that in our example down below B was 4 T
was 8 and C was 10.
now instead of doing x dot view of B by
negative one
right this is what we had before
we want this to be B by
um negative 1 by
and basically here we want c times n
that's how many consecutive elements we
want
and here instead of negative one I don't
super love the use of negative one
because I like to be very explicit so
that you get error messages when things
don't go according to your expectation
so what do we expect here we expect this
to become t
divide n using integer division here
so that's what I expect to happen
and then one more thing I want to do
here is remember previously all the way
in the beginning n was three and uh
basically we're concatenating
um all the three characters that existed
there
so we basically are concatenated
everything
and so sometimes I can create a spurious
dimension of one here so if it is the
case that x dot shape at one is one then
it's kind of like a spurious dimension
um so we don't want to return a
three-dimensional tensor with a one here
we just want to return a two-dimensional
tensor exactly as we did before
so in this case basically we will just
say x equals x dot squeeze that is a
pytorch function
and squeeze takes a dimension that it
either squeezes out all the dimensions
of a tensor that are one or you can
specify the exact Dimension that you
want to be squeezed and again I like to
be as explicit as possible always so I
expect to squeeze out the First
Dimension only
of this tensor
this three-dimensional tensor and if
this Dimension here is one then I just
want to return B by c times n
and so self dot out will be X and then
we return salt dot out
so that's the candidate implementation
and of course this should be self.n
instead of just n
so let's run
and let's come here now
and take it for a spin so flatten
consecutive
and in the beginning let's just use
eight so this should recover the
previous Behavior so flagging
consecutive of eight uh which is the
current block size
we can do this uh that should recover
the previous Behavior
so we should be able to run the model
and here we can inspect I have a little
code snippet here where I iterate over
all the layers I print the name of this
class and the shape
and so we see the shapes as we expect
them after every single layer in the top
bit so now let's try to restructure it
using our flattened consecutive and do
it hierarchically so in particular
we want to flatten consecutive not just
not block size but just two
and then we want to process this with
linear now then the number of inputs to
this linear will not be an embed times
block size it will now only be n embed
times two
20.
this goes through the first layer and
now we can in principle just copy paste
this
now the next linear layer should expect
and hidden times two
and the last piece of it should expect
and it enters 2 again
so this is sort of like the naive
version of it
um
so running this we now have a much much
bigger model
and we should be able to basically just
forward the model
and now we can inspect uh the numbers in
between
so four byte by 20
was Platinum consecutively into four by
four by Twenty
this was projected into four by four by
two hundred
and then bash storm just worked out of
the box we have to verify that bastron
does the correct thing even though it
takes a three-dimensional impedance that
are two dimensional input
then we have 10h which is element wise
then we crushed it again so if we
flatten consecutively and ended up with
a four by two by 400 now
then linear brought it back down to 200
batch room 10h and lastly we get a 4 by
400 and we see that the flattened
consecutive for the last flatten here uh
it squeezed out that dimension of one so
we only ended up with four by four
hundred and then linear Bachelor on 10h
and uh the last linear layer to get our
logents and so The Lodges end up in the
same shape as they were before but now
we actually have a nice three layer
neural nut and it basically corresponds
to whoops sorry it basically corresponds
exactly to this network now except only
this piece here because we only have
three layers whereas here in this
example there's uh four layers with the
total receptive field size of 16
characters instead of just eight
characters so the block size here is 16.
so this piece of it's basically
implemented here
um now we just have to kind of figure
out some good Channel numbers to use
here now in particular I changed the
number of hidden units to be 68 in this
architecture because when I use 68 the
number of parameters comes out to be 22
000 so that's exactly the same that we
had before and we have the same amount
of capacity at this neural net in terms
of the number of parameters but the
question is whether we are utilizing
those parameters in a more efficient
architecture so what I did then is I got
rid of a lot of the debugging cells here
and I rerun the optimization and
scrolling down to the result we see that
we get the identical performance roughly
so our validation loss now is 2.029 and
previously it was 2.027 so controlling
for the number of parameters changing
from the flat to hierarchical is not
giving us anything yet
that said there are two things
um to point out number one we didn't
really torture the um architecture here
very much this is just my first guess
and there's a bunch of hyper parameters
search that we could do in order in
terms of how we allocate uh our budget
of parameters to what layers number two
we still may have a bug inside the
bachelor 1D layer so let's take a look
at
um uh that because it runs but does it
do the right thing
so I pulled up the layer inspector sort
of that we have here and printed out the
shape along the way and currently it
looks like the batch form is receiving
an input that is 32 by 4 by 68 right and
here on the right I have the current
implementation of Bachelor that we have
right now
now this bachelor assumed in the way we
wrote it and at the time that X is
two-dimensional so it was n by D where n
was the batch size so that's why we only
reduced uh the mean and the variance
over the zeroth dimension but now X will
basically become three-dimensional so
what's happening inside the bachelor
right now and how come it's working at
all and not giving any errors the reason
for that is basically because everything
broadcasts properly but the bachelor is
not doing what we need what we wanted to
do
so in particular let's basically think
through what's happening inside the
bathroom uh looking at what's what's do
What's Happening Here
I have the code here
so we're receiving an input of 32 by 4
by 68 and then we are doing uh here x
dot mean here I have e instead of X but
we're doing the mean over zero and
that's actually giving us 1 by 4 by 68.
so we're doing the mean only over the
very first Dimension and it's giving us
a mean and a variance that still
maintain this Dimension here
so these means are only taking over 32
numbers in the First Dimension and then
when we perform this everything
broadcasts correctly still
but basically what ends up happening is
when we also look at the running mean
the shape of it so I'm looking at the
model that layers at three which is the
first bathroom layer and they're looking
at whatever the running mean became and
its shape
the shape of this running mean now is 1
by 4 by 68.
right instead of it being
um you know just a size of dimension
because we have 68 channels we expect to
have 68 means and variances that we're
maintaining but actually we have an
array of 4 by 68 and so basically what
this is telling us is this bash Norm is
only
this bachelor is currently working in
parallel
over
4 times 68 instead of just 68 channels
so basically we are maintaining
statistics for every one of these four
positions individually and independently
and instead what we want to do is we
want to treat this four as a batch
Dimension just like the zeroth dimension
so as far as the bachelor is concerned
it doesn't want to average we don't want
to average over 32 numbers we want to
now average over 32 times four numbers
for every single one of these 68
channels
and uh so let me now
remove this
it turns out that when you look at the
documentation of torch.mean
so let's go to torch.me
in one of its signatures when we specify
the dimension
we see that the dimension here is not
just it can be in or it can also be a
tuple of ins so we can reduce over
multiple integers at the same time over
multiple Dimensions at the same time so
instead of just reducing over zero we
can pass in a tuple 0 1.
and here zero one as well and then
what's going to happen is the output of
course is going to be the same
but now what's going to happen is
because we reduce over 0 and 1 if we
look at immin.shape
we see that now we've reduced we took
the mean over both the zeroth and the
First Dimension
so we're just getting 68 numbers and a
bunch of spurious Dimensions here
so now this becomes 1 by 1 by 68 and the
running mean and the running variance
analogously will become one by one by
68. so even though there are the
spurious Dimensions uh the current the
current the correct thing will happen in
that we are only maintaining means and
variances for 64 sorry for 68 channels
and we're not calculating the mean
variance across 32 times 4 dimensions so
that's exactly what we want and let's
change the implementation of bash term
1D that we have so that it can take in
two-dimensional or three-dimensional
inputs and perform accordingly so at the
end of the day the fix is relatively
straightforward basically the dimension
we want to reduce over is either 0 or
the Tuple zero and one depending on the
dimensionality of X so if x dot and dim
is two so it's a two dimensional tensor
then Dimension we want to reduce over is
just the integer zero
L if x dot ending is three so it's a
three-dimensional tensor then the dims
we're going to assume are zero and one
that we want to reduce over and then
here we just pass in dim
and if the dimensionality of X is
anything else we'll now get an error
which is good
um so that should be the fix now I want
to point out one more thing we're
actually departing from the API of Pi
torch here a little bit because when you
come to batch room 1D and pytorch you
can scroll down and you can see that the
input to this layer can either be n by C
where n is the batch size and C is the
number of features or channels or it
actually does accept three-dimensional
inputs but it expects it to be n by C by
L
where LSA like the sequence length or
something like that
so um
this is problem because you see how C is
nested here in the middle and so when it
gets three-dimensional inputs this bash
term layer will reduce over zero and two
instead of zero and one so it basically
Pi torch batch number one D layer
assumes that c will always be the First
Dimension whereas we'll we assume here
that c is the last Dimension and there
are some number of batch Dimensions
beforehand
um
and so
it expects n by C or M by C by all we
expect and by C or n by L by C
and so it's a deviation
um
I think it's okay I prefer it this way
honestly so this is the way that we will
keep it for our purposes
so I redefined the layers re-initialize
the neural net and did a single forward
pass with a break just for one step
looking at the shapes along the way
they're of course identical all the
shapes are the same but the way we see
that things are actually working as we
want them to now is that when we look at
the bathroom layer the running mean
shape is now one by one by 68. so we're
only maintaining 68 means for every one
of our channels and we're treating both
the zeroth and the First Dimension as a
batch Dimension which is exactly what we
want so let me retrain the neural lot
now okay so I retrained the neural net
with the bug fix we get a nice curve and
when we look at the validation
performance we do actually see a slight
Improvement so we went from 2.029 to
2.022 so basically the bug inside the
bathroom was holding up us back like a
little bit it looks like and we are
getting a tiny Improvement now but it's
not clear if this is statistical
significant
um
and the reason we slightly expect an
improvement is because we're not
maintaining so many different means and
variances that are only estimated using
using 32 numbers effectively now we are
estimating them using 32 times 4 numbers
so you just have a lot more numbers that
go into any one estimate of the mean and
variance and it allows things to be a
bit more stable and less Wiggly inside
those estimates of those statistics so
pretty nice with this more General
architecture in place we are now set up
to push the performance further by
increasing the size of the network so
for example I bumped up the number of
embeddings to 24 instead of 10 and also
increased number of hidden units but
using the exact same architecture we now
have 76 000 parameters and the training
takes a lot longer but we do get a nice
curve and then when you actually
evaluate the performance we are now
getting validation performance of 1.993
so we've crossed over the 2.0 sort of
territory and right about 1.99 but we
are starting to have to wait quite a bit
longer and we're a little bit in the
dark with respect to the correct setting
of the hyper parameters here and the
learning rates and so on because the
experiments are starting to take longer
to train and so we are missing sort of
like an experimental harness on which we
could run a number of experiments and
really tune this architecture very well
so I'd like to conclude now with a few
notes we basically improved our
performance from a starting of 2.1 down
to 1.9 but I don't want that to be the
focus because honestly we're kind of in
the dark we have no experimental harness
we're just guessing and checking and
this whole thing is terrible we're just
looking at the training loss normally
you want to look at both the training
and the validation loss together and the
whole thing looks different if you're
actually trying to squeeze out numbers
that said we did implement this
architecture from the wavenet paper but
we did not implement this specific uh
forward pass of it where you have a more
complicated a linear layer sort of that
is this gated linear layer kind of and
there's residual connections and Skip
connections and so on so we did not
Implement that we just implemented this
structure I would like to briefly hint
or preview how what we've done here
relates to convolutional neural networks
as used in the wavenet paper and
basically the use of convolutions is
strictly for efficiency it doesn't
actually change the model we've
implemented
so here for example
let me look at a specific name to work
with an example so there's a name in our
training set and it's DeAndre and it has
seven letters so that is eight
independent examples in our model so all
these rows here are independent examples
of the Android
now you can forward of course any one of
these rows independently so I can take
my model and call call it on any
individual index notice by the way here
I'm being a little bit tricky
the reason for this is that extra at
seven that shape is just
um one dimensional array of eight so you
can't actually call the model on it
you're going to get an error because
there's no batch dimension
so when you do extra at
a list of seven then the shape of this
becomes one by eight so I get an extra
batch dimension of one and then we can
forward the model
so
that forwards a single example and you
might imagine that you actually may want
to forward all of these eight
um at the same time
so pre-allocating some memory and then
doing a for Loop eight times and
forwarding all of those eight here will
give us all the logits in all these
different cases
now for us with the model as we've
implemented it right now this is eight
independent calls to our model
but what convolutions allow you to do is
it allow you to basically slide this
model efficiently over the input
sequence and so this for Loop can be
done not outside in Python but inside of
kernels in Cuda and so this for Loop
gets hidden into the convolution
so the convolution basically you can
cover this it's a for Loop applying a
little linear filter over space of some
input sequence and in our case the space
we're interested in is one dimensional
and we're interested in sliding these
filters over the input data
so this diagram actually is fairly good
as well
basically what we've done is here they
are highlighting in Black one individ
one single sort of like tree of this
calculation so just calculating the
single output example here
um
and so this is basically what we've
implemented here we've implemented a
single this black structure we've
implemented that and calculated a single
output like a single example
but what collusions allow you to do is
it allows you to take this black
structure and kind of like slide it over
the input sequence here and calculate
all of these orange outputs at the same
time or here that corresponds to
calculating all of these outputs of
um at all the positions of DeAndre at
the same time
and the reason that this is much more
efficient is because number one as I
mentioned the for Loop is inside the
Cuda kernels in the sliding so that
makes it efficient but number two notice
the variable reuse here for example if
we look at this circle this node here
this node here is the right child of
this node but is also the left child of
the node here
and so basically this node and its value
is used twice
and so right now in this naive way we'd
have to recalculate it but here we are
allowed to reuse it
so in the convolutional neural network
you think of these linear layers that we
have up above as filters and we take
these filters and they're linear filters
and you slide them over input sequence
and we calculate the first layer and
then the second layer and then the third
layer and then the output layer of the
sandwich and it's all done very
efficiently using these convolutions
so we're going to cover that in a future
video the second thing I hope you took
away from this video is you've seen me
basically Implement all of these layer
Lego building blocks or module building
blocks and I'm implementing them over
here and we've implemented a number of
layers together and we've also
implemented these these containers and
we've overall pytorchified our code
quite a bit more
now basically what we're doing here is
we're re-implementing torch.nn which is
the neural networks library on top of
torch.tensor and it looks very much like
this except it is much better because
because it's in pi torch instead of
jingling my Jupiter notebook so I think
going forward I will probably have
considered us having unlocked
um torch.nn we understand roughly what's
in there how these modules work how
they're nested and what they're doing on
top of torture tensor so hopefully we'll
just uh we'll just switch over and
continue and start using torch.net
directly the next thing I hope you got a
bit of a sense of is what the
development process of building deep
neural networks looks like which I think
was relatively representative to some
extent so number one we are spending a
lot of time in the documentation page of
pytorch and we're reading through all
the layers looking at documentations
where the shapes of the inputs what can
they be what does the layer do and so on
unfortunately I have to say the
patreon's documentation is not are very
good they spend a ton of time on
Hardcore engineering of all kinds of
distributed Primitives Etc but as far as
I can tell no one is maintaining any
documentation it will lie to you it will
be wrong it will be incomplete it will
be unclear so unfortunately it is what
it is and you just kind of do your best
um with what they've given us
um number two
uh the other thing that I hope you got a
sense of is there's a ton of trying to
make the shapes work and there's a lot
of gymnastics around these
multi-dimensional arrays and are they
two-dimensional three-dimensional
four-dimensional uh what layers take
what shapes is it NCL or NLC and you're
promoting and viewing and it just can
get pretty messy and so that brings me
to number three I very often prototype
these layers and implementations in
jupyter notebooks and make sure that all
the shapes work out and I'm spending a
lot of time basically babysitting the
shapes and making sure everything is
correct and then once I'm satisfied with
the functionality in the Jupiter
notebook I will take that code and copy
paste it into my repository of actual
code that I'm training with and so then
I'm working with vs code on the side so
I usually have jupyter notebook and vs
code I develop in Jupiter notebook I
paste into vs code and then I kick off
experiments from from the reaper of
course from the code repository so
that's roughly some notes on the
development process of working with
neurons lastly I think this lecture
unlocks a lot of potential further
lectures because number one we have to
convert our neural network to actually
use these dilated causal convolutional
layers so implementing the comnet number
two potentially starting to get into
what this means whatever residual
connections and Skip connections and why
are they useful
number three we as I mentioned we don't
have any experimental harness so right
now I'm just guessing checking
everything this is not representative of
typical deep learning workflows you have
to set up your evaluation harness you
can kick off experiments you have lots
of arguments that your script can take
you're you're kicking off a lot of
experimentation you're looking at a lot
of plots of training and validation
losses and you're looking at what is
working and what is not working and
you're working on this like population
level and you're doing all these hyper
parameter searches and so we've done
none of that so far so how to set that
up and how to make it good I think as a
whole another topic number three we
should probably cover recurring neural
networks RNs lstm's grooves and of
course Transformers so many uh places to
go and we'll cover that in the future
for now bye sorry I forgot to say that
if you are interested I think it is kind
of interesting to try to beat this
number 1.993 because I really haven't
tried a lot of experimentation here and
there's quite a bit of fruit potentially
to still purchase further so I haven't
tried any other ways of allocating these
channels in this neural net maybe the
number of dimensions for the embedding
is all wrong maybe it's possible to
actually take the original network with
just one hidden layer and make it big
enough and actually beat my fancy
hierarchical Network it's not obvious
that would be kind of embarrassing if
this did not do better even once you
torture it a little bit maybe you can
read the weight net paper and try to
figure out how some of these layers work
and Implement them yourselves using what
we have
and of course you can always tune some
of the initialization or some of the
optimization and see if you can improve
it that way so I'd be curious if people
can come up with some ways to beat this
and yeah that's it for now byehi everyone
so by now you have probably heard of
Chachi PT it has taken the world and the
AI Community by storm and it is a system
that allows you to interact with an AI
and give it text-based tasks so for
example we can ask chatgpt to write us a
small haiku about how important it is
that people understand Ai and then they
can use it to improve the world and make
it more prosperous so when we run this
AI knowledge brings prosperity for all
to see Embrace its power okay not bad
and so you could see that Chachi PT went
from left to right and generated all
these words seek sort of sequentially
now I asked it already the exact same
prompt a little bit earlier and it
generated a slightly different outcome
AI is power to grow ignorance holds us
back learn Prosperity weights
so uh pretty good in both cases and
slightly different so you can see that
chatgpt is a probabilistic system and
for any one prompt it can give us
multiple answers sort of replying to it
now this is just one example of a prompt
people have come up with many many
examples and there are entire websites
that index interactions with charge EBT
and so many of them are quite humorous
explain HTML to me like I'm a dog write
release notes for chess 2. write a note
about Elon Musk buying on Twitter
and so on
so as an example please write a breaking
news article about a leaf falling from a
tree
uh and a shocking turn of events a leaf
has fallen from a treat in the local
park Witnesses report that the leaf
which was previously attached to a
branch of a tree detached itself and
fell to the ground very dramatic so you
can see that this is a pretty remarkable
system and it is what we call a language
model because it it models the sequence
of words or characters or tokens more
generally and it knows how sort of words
follow each other in English language
and so from its perspective what it is
doing is it is completing the sequence
so I give it the start of a sequence and
it completes the sequence with the
outcome and so it's a language model in
that sense
now I would like to focus on the under
the hood of
um under the hood components of what
makes chat GPT work so what is the
neural network under the hood that
models the sequence of these words
and that comes from this paper called
attention is all you need in 2017 a
landmark paper a landmark paper and AI
that produced and proposed the
Transformer architecture
so GPT is short for generally
generatively pre-trained Transformer so
Transformer is the neural nut that
actually does all the heavy lifting
under the hood it comes from this paper
in 2017. now if you read this paper this
reads like a pretty random machine
translation paper and that's because I
think the authors didn't fully
anticipate the impact that the
Transformer would have on the field and
this architecture that they produced in
the context of machine translation in
their case actually ended up taking over
the rest of AI in the next five years
after and so this architecture with
minor changes was copy pasted into a
huge amount of applications in AI in
more recent years and that includes at
the core of chat GPT
now we are not going to what I'd like to
do now is I'd like to build out
something like chatgpt but we're not
going to be able to of course reproduce
chatgpt this is a very serious
production grade system it is trained on
a good chunk of internet and then
there's a lot of pre-training and
fine-tuning stages to it and so it's
very complicated what I'd like to focus
on is just to train a Transformer based
language model and in our case it's
going to be a character level
a language model I still think that is a
very educational with respect to how
these systems work so I don't want to
train on the chunk of Internet we need a
smaller data set in this case I propose
that we work with my favorite toy data
set it's called tiny Shakespeare and
what it is is basically it's a
concatenation of all of the works of
Shakespeare in my understanding and so
this is all of Shakespeare in a single
file this file is about one megabyte
and it's just all of Shakespeare
and what we are going to do now is we're
going to basically model how these
characters follow each other so for
example given a chunk of these
characters like this
are given some context of characters in
the past the Transformer neural network
will look at the characters that I've
highlighted and is going to predict that
g is likely to come next in the sequence
and it's going to do that because we're
going to train that Transformer on
Shakespeare and it's just going to try
to produce uh character sequences that
look like this
and in that process is going to model
all the patterns inside this data so
once we've trained the system I'd just
like to give you a preview we can
generate infinite Shakespeare and of
course it's a fake thing that looks kind
of like Shakespeare
um
apologies for there's some junk that I'm
not able to resolve in in here but
um
you can see how this is going character
by character and it's kind of like
predicting Shakespeare like language so
verily my Lord the sights have left the
again the king coming with my curses
with precious pale and then tronio says
something else Etc and this is just
coming out of the Transformer in a very
similar manner as it would come out in
Chachi PT in our case character by
character in Chachi PT it's coming out
on the token by token level and tokens
are these a sort of like little sub word
pieces so they're not Word level they're
kind of like work chunk level
um and now the I've already written this
entire code to train these Transformers
um and it is in a GitHub repository that
you can find and it's called a nano GPT
so Nano GPT is a repository that you can
find on my GitHub and it's a repository
for training Transformers
um On Any Given text
and what I think is interesting about it
because there's many ways to train
Transformers but this is a very simple
implementation so it's just two files of
300 lines of code each one file defines
the GPT model the Transformer and one
file trains it on some given Text data
set and here I'm showing that if you
train it on a open webtext data set
which is a fairly large data set of web
pages then I reproduce the the
performance of gpt2
so gpt2 is an early version of openai's
GPT from 2017 if I occur correctly and
I've only so far reproduced the the
smallest 124 million parameter model but
basically this is just proving that the
code base is correctly arranged and I'm
able to load the neural network weights
that open AI has released later
so you can take a look at the finished
code here in Nano GPT but what I would
like to do in this lecture is I would
like to basically write this repository
from scratch so we're going to begin
with an empty file and we're going to
define a Transformer piece by piece
we're going to train it on the tiny
Shakespeare data set and we'll see how
we can then generate infinite
Shakespeare and of course this can copy
paste to any arbitrary Text data set
that you like but my goal really here is
to just make you understand and
appreciate how under the hood chat GPT
works and really all that's required is
a Proficiency in Python and some basic
understanding of calculus and statistics
and it would help if you also see my
previous videos on the same YouTube
channel in particular my make more
series where I
Define smaller and simpler neural
network language models so multilevel
perceptrons and so on it really
introduces the language modeling
framework and then here in this video
we're going to focus on the Transformer
neural network itself
okay so I created a new Google collab uh
jupyter notebook here and this will
allow me to later easily share this code
that we're going to develop together
with you so you can follow along so this
will be in the video description later
now here I've just done some
preliminaries I downloaded the data set
the tiny Shakespeare data set at this
URL and you can see that it's about a
one megabyte file
then here I open the input.txt file and
just read in all the text as a string
and we see that we are working with 1
million characters roughly
and the first 1000 characters if we just
print them out are basically what you
would expect this is the first 1000
characters of the tiny Shakespeare data
set roughly up to here
so so far so good next we're going to
take this text and the text is a
sequence of characters in Python so when
I call the set Constructor on it I'm
just going to get the set of all the
characters that occur in this text
and then I call list on that to create a
list of those characters instead of just
a set so that I have an ordering an
arbitrary ordering
and then I sort that
so basically we get just all the
characters that occur in the entire data
set and they're sorted now the number of
them is going to be our vocabulary size
these are the possible elements of our
sequences and we see that when I print
here the characters
there's 65 of them in total there's a
space character and then all kinds of
special characters
and then capitals and lowercase letters
so that's our vocabulary and that's the
sort of like possible characters that
the model can see or emit
okay so next we would like to develop
some strategy to tokenize the input text
now when people say tokenize they mean
convert the raw text as a string to some
sequence of integers According to some
notebook According to some vocabulary of
possible elements
so as an example here we are going to be
building a character level language
model so we're simply going to be
translating individual characters into
integers
so let me show you a chunk of code that
sort of does that for us
so we're building both the encoder and
the decoder and let me just talk through
What's Happening Here
when we encode an arbitrary text like hi
there we're going to receive a list of
integers that represents that string so
for example 46 47 Etc
and then we also have the reverse
mapping so we can take this list and
decode it to get back the exact same
string
so it's really just like a translation
two integers and back for arbitrary
string and for us it is done on a
character level
now the way this was achieved is we just
iterate over all the characters here and
create a lookup table from the character
to the integer and vice versa and then
to encode some string we simply
translate all the characters
individually and to decode it back we
use the reverse mapping and concatenate
all of it
now this is only one of many possible
encodings or many possible sort of
tokenizers and it's a very simple one
but there's many other schemas that
people have come up with in practice so
for example Google uses a sentence piece
uh so sentence piece will also encode
text into integers but in a different
schema and using a different vocabulary
and sentence piece is a sub word sort of
tokenizer and what that means is that
you're not encoding entire words but
you're not also encoding individual
characters it's it's a sub word unit
level and that's usually what's adopted
in practice for example also openai has
this Library called tick token that uses
a pipe pair encoding tokenizer
um and that's what GPT uses
and you can also just encode words into
like hello world into a list of integers
so as an example I'm using the tick
token Library here
I'm getting the encoding for gpt2 or
that was used for gpt2
instead of just having 65 possible
characters or tokens they have 50 000
tokens
and so when they encode the exact same
string High there we only get a list of
three integers but those integers are
not between 0 and 64. they are between 0
and 5000 50 256.
so basically you can trade off the code
book size and the sequence lengths so
you can have a very long sequences of
integers with very small vocabularies or
you can have a short
um
sequences of integers with very large
vocabularies and so typically people use
in practice the sub word encodings but
I'd like to keep our tokenizer very
simple so we're using character level
tokenizer
and that means that we have very small
code books we have very simple encode
and decode functions but we do get very
long sequences as a result but that's
the level at which we're going to stick
with this lecture because it's the
simplest thing okay so now that we have
an encoder and a decoder effectively a
tokenizer we can tokenize the entire
training set of Shakespeare so here's a
chunk of code that does that
and I'm going to start to use the
pytorch library and specifically the
torch.tensor from the pytorch library
so we're going to take all of the text
in tiny Shakespeare encode it and then
wrap it into a torch.tensor to get the
data tensor so here's what the data
tensor looks like when I look at just
the first 1000 characters or the 1000
elements of it
so we see that we have a massive
sequence of integers and this sequence
of integers here is basically an
identical translation of the first 1000
characters here
so I believe for example that zero is a
new line character and maybe one is a
space not 100 sure but from now on the
entire data set of text is
re-represented as just it just stretched
out as a single very large uh sequence
of integers
let me do one more thing before we move
on here I'd like to separate out our
data set into a train and a validation
split so in particular we're going to
take the first 90 of the data set and
consider that to be the training data
for the Transformer and we're going to
withhold the last 10 percent at the end
of it to be the validation data and this
will help us understand to what extent
our model is overfitting so we're going
to basically hide and keep the
validation data on the side because we
don't want just a perfect memorization
of this exact Shakespeare we want a
neural network that sort of creates
Shakespeare like text and so it should
be fairly likely for it to produce
the actual like stowed away uh true
Shakespeare text
um and so we're going to use this to get
a sense of the overfitting okay so now
we would like to start plugging these
text sequences or integer sequences into
the Transformer so that it can train and
learn those patterns
now the important thing to realize is
we're never going to actually feed the
entire text into Transformer all at once
that would be computationally very
expensive and prohibitive so when we
actually train a Transformer on a lot of
these data sets we only work with chunks
of the data set and when we train the
Transformer we basically sample random
little chunks out of the training set
and train them just chunks at a time and
these chunks have basically some kind of
a length
and as a maximum length now the maximum
length typically at least in the code I
usually write is called block size
you can you can find it on the different
names like context length or something
like that let's start with the block
size of just eight and let me look at
the first train data characters the
first block size plus one characters
I'll explain why plus one in a second
so this is the first nine characters in
the sequence in the training set
now what I'd like to point out is that
when you sample a chunk of data like
this so say that these nine characters
out of the training set
this actually has multiple examples
packed into it
and that's because all of these
characters follow each other
and so what this thing is going to say
when we plug it into a Transformer is
we're going to actually simultaneously
train it to make prediction at every one
of these positions
now in the in a chunk of nine characters
there's actually eight individual
examples packed in there
so there's the example that one 18 when
in the context of 18 47 likely comes
next in the context of 18 and 47 56
comes next in the context of 1847-56 57
can come next and so on so that's the
eight individual examples let me
actually spell it out with code
so here's a chunk of code to illustrate
X are the inputs to the Transformer it
will just be the first block size
characters
y will be the next block size characters
so it's offset by one
and that's because y are the targets for
each position in the input
and then here I'm iterating over all the
block size of 8. and the context is
always all the characters in X up to T
and including t
and the target is always the teeth
character but in the targets array why
so let me just run this
and basically it spells out what I've
said in words these are the eight
examples hidden in a chunk of nine
characters that we uh sampled from the
training set
I want to mention one more thing we
train on all the eight examples here
with context between one all the way up
to context of block size
and we train on that not just for
computational reasons because we happen
to have the sequence already or
something like that it's not just done
for efficiency it's also done to make
the Transformer Network be used to
seeing contexts all the way from as
little as one all the way to block size
and we'd like the transform to be used
to seeing everything in between and
that's going to be useful later during
inference because while we're sampling
we can start the sampling generation
with as little as one character of
context and the Transformer knows how to
predict the next character with all the
way up to just one context of one and so
then it can predict everything up to
block size and after block size we have
to start truncating because the
Transformer will never receive more than
block size inputs when it's predicting
the next character
Okay so we've looked at the time
dimension of the tensors that are going
to be feeding into the Transformer
there's one more Dimension to care about
and that is the batch dimension and so
as we're sampling these chunks of text
we're going to be actually every time
we're going to feed them into a
Transformer we're going to have many
batches of multiple chunks of text that
are all like stacked up in a single
tensor and that's just done for
efficiency just so that we can keep the
gpus busy because they are very good at
parallel processing of
um of data and so we just want to
process multiple chunks all at the same
time but those chunks are processed
completely independently they don't talk
to each other and so on so let me
basically just generalize this and
introduce a batch Dimension here's a
chunk of code
let me just run it and then I'm going to
explain what it does
so here because we're going to start
sampling random locations in the data
set to pull chunks from I am setting the
seed so that
um in the random number generator so
that the numbers I see here are going to
be the same numbers you see later if you
try to reproduce this
now the back size here is how many
independent sequences we are processing
every forward backward pass of the
Transformer
the block size as I explained is the
maximum context length to make those
predictions
so let's say by size 4 block size 8 and
then here's how we get batch
for any arbitrary split if the split is
a training split then we're going to
look at train data otherwise and
validata
that gets us the data array and then
when I Generate random positions to grab
a chunk out of
I actually grab I actually generate
batch size number of
random offsets
so because this is four we are IX is
going to be a four numbers that are
randomly generated between 0 and Len of
data minus block size so it's just
random offsets into the training set
and then X's as I explained are the
first block size characters starting at
I
the Y's are the offset by one of that so
just add plus one
and then we're going to get those chunks
for every one of integers I in IX and
use a torch.stack to take all those
one-dimensional tensors as we saw here
and we're going to
um stack them up at rows
and so they all become a row in a four
by eight tensor
so here's where I'm printing then
when I sample a batch XP and YB
the input the Transformer now are
the input X is the four by eight tensor
four uh rows of eight columns
and each one of these is a chunk of the
training set
and then the targets here are in the
associated array Y and they will come in
through the Transformer all the way at
the end to create the loss function so
they will give us the correct answer for
every single position inside X
and then these are the four independent
rows
so spelled out as we did before
this four by eight array contains a
total of 32 examples and they're
completely independent as far as the
Transformer is concerned
uh so when the
input is 24 the target is 43 or rather
43 here in the Y array when the input is
2443 the target is 58.
when the input is 24 43.58 the target is
5 Etc or like when it is a 5258 one the
target is 58.
right so you can sort of see this
spelled out these are the 32 independent
examples packed in to a single batch of
the input X and then the desired targets
are in y
and so now this integer tensor of X is
going to feed into the Transformer
and that Transformer is going to
simultaneously process all these
examples and then look up the correct
um integers to predict in every one of
these positions in the tensor y okay so
now that we have our batch of input that
we'd like to feed into a Transformer
let's start basically feeding this into
neural networks now we're going to start
off with the simplest possible neural
network which in the case of language
modeling in my opinion is the bigram
language model and we've covered the
background language model in my make
more series in a lot of depth and so
here I'm going to sort of go faster and
let's just implement the pytorch module
directly that implements the bigram
language model
so I'm importing the pytorch and then
module
uh for reproducibility
and then here I'm constructing a diagram
language model which is a subclass of NN
module
and then I'm calling it and I'm passing
in the inputs and the targets
and I'm just printing now when the
inputs and targets come here you see
that I'm just taking the index the
inputs X here which I rename to idx and
I'm just passing them into this token
embedding table
so what's going on here is that here in
the Constructor
we are creating a token embedding table
and it is of size vocab size by vocab
size
and we're using nn.embedding which is a
very thin wrapper around basically a
tensor of shape both capsized by vocab
size
and what's happening here is that when
we pass idx here every single integer in
our input is going to refer to this
embedding table and is going to pluck
out a row of that embedding table
corresponding to its index so 24 here
we'll go to the embedding table and
we'll pluck out the 24th row and then 43
will go here and block out the 43rd row
Etc and then Pi torch is going to
arrange all of this into a batch by Time
by Channel tensor in this case batch is
4 time is 8 and C which is the channels
is vocab size or 65. and so we're just
going to pluck out all those rows
arrange them in a b by T by C and now
we're going to interpret this as the
logits which are basically the scores
for the next character in the sequence
and so what's happening here is we are
predicting what comes next based on just
the individual identity of a single
token and you can do that because
um I mean currently the tokens are not
talking to each other and they're not
seeing any context except for they're
just seeing themselves so I'm a I'm a
token number five and then I can
actually make pretty decent predictions
about what comes next just by knowing
that I'm token five because some
characters know cert follow other
characters in in typical scenarios so we
saw a lot of this in a lot more depth in
the make more series and here if I just
run this then we currently get the
predictions the scores the logits for
every one of the four by eight positions
now that we've made predictions about
what comes next we'd like to evaluate
the loss function and so in make more
series we saw that a good way to measure
a loss or like a quality of the
predictions is to use the negative log
likelihood loss which is also
implemented in pytorch under the name
cross entropy
so what we'd like to do here is
loss is the cross entropy on the
predictions and the targets and so this
measures the quality of the logits with
respect to the Targets in other words we
have the identity of the next character
so how well are we predicting the next
character based on Illusions and
intuitively the correct
um the correct dimension of logits uh
depending on whatever the target is
should have a very high number and all
the other dimensions should be very low
number right
now the issue is that this won't
actually this is what we want we want to
basically output the logits and the loss
this is what we want but unfortunately
uh this won't actually run
we get an error message but intuitively
we want to measure this now when we go
to the pi torch cross entropy
a documentation here
um
we're trying to call the cross entropy
in its functional form so that means we
don't have to create like a module for
it
but here when we go to the documentation
you have to look into the details of how
pytorch expects these inputs and
basically the issue here is by torch
expects if you have multi-dimensional
input which we do because we have a b by
T by C tensor then it actually really
wants the channels to be the second
dimension here
so if you um so basically it wants a b
by C by T instead of a b by T by C
and so it's just the details of how
pytorch treats
um these kinds of inputs and so we don't
actually want to deal with that so what
we're going to do instead is we need to
basically reshape our logits so here's
what I like to do I like to take
basically give names to the dimensions
so launches.shape is B by T by C and
unpack those numbers
and then let's say that logits equals
logits.view
and we want it to be a b times c b times
T by C so just a two-dimensional array
right so we're going to take all the
we're going to take all of these
um
positions here and we're going to uh
stretch them out in a one-dimensional
sequence
and preserve the channel Dimension as
the second dimension
so we're just kind of like stretching
out the array so it's two-dimensional
and in that case it's going to better
conform to what pi torch sort of expects
in its dimensions
now we have to do the same to targets
because currently targets
are of shape B by T and we want it to be
just B times T so one dimensional now
alternatively you could always still
just do -1 because Pi torch will guess
what this should be if you want to lay
it out but let me just be explicit on
say Q times t
once we've reshaped this it will match
the cross entropy case
and then we should be able to evaluate
our loss
okay so with that right now and we can
do loss and So currently we see that the
loss is 4.87
now because our we have 65 possible
vocabulary elements we can actually
guess at what the loss should be and in
particular
we covered negative log likelihood in a
lot of detail we are expecting log or
long of
um 1 over 65 and negative of that
so we're expecting the loss to be about
4.1217 but we're getting 4.87 and so
that's telling us that the initial
predictions are not super diffuse
they've got a little bit of entropy and
so we're guessing wrong
uh so uh yes but actually we're I able
we are able to evaluate the loss okay so
now that we can evaluate the quality of
the model on some data we'd likely also
be able to generate from the model so
let's do the generation now I'm going to
go again a little bit faster here
because I covered all this already in
previous videos
so
here's a generate function for the model
so we take some uh we take the the same
kind of input idx here
and basically
this is the current context of some
characters in a batch in some batch
so it's also B by T and the job of
generate is to basically take this B by
T and extend it to be B by T plus one
plus two plus three and so it's just
basically it contains the generation in
all the batch dimensions in the time
dimension
So that's its job and we'll do that for
Max new tokens
so you can see here on the bottom
there's going to be some stuff here but
on the bottom whatever is predicted is
concatenated on top of the previous idx
along the First Dimension which is the
time Dimension to create a b by T plus
one
so that becomes the new idx so the job
of generators to take a b by T and make
it a b by T plus one plus two plus three
as many as we want maximum tokens so
this is the generation from the model
now inside the generation what we're
what are we doing we're taking the
current indices we're getting the
predictions so we get those are in the
logits
and then the loss here is going to be
ignored because
um we're not we're not using that and we
have no targets that are sort of ground
truth targets that we're going to be
comparing with
then once we get the logits we are only
focusing on the last step so instead of
a b by T by C we're going to pluck out
the negative one the last element in the
time dimension
because those are the predictions for
what comes next
so that this is the logits which we then
convert to probabilities via softmax and
then we use torch that multinomial to
sample from those probabilities and we
ask by torch to give us one sample
and so idx next will become a b by one
because in each one of the batch
Dimensions we're going to have a single
prediction for what comes next so this
num samples equals one will make this be
a one
and then we're going to take those
integers that come from the sampling
process according to the probability
distribution given here
and those integers got just concatenated
on top of the current sort of like
running stream of integers and this
gives us a p by T plus one
and then we can return that now one
thing here is you see how I'm calling
self of idx which will end up going to
the forward function I'm not providing
any Targets So currently this would give
an error because targets is uh is uh
sort of like not given so target has to
be optional so targets is none by
default and then if targets is none then
there's no loss to create so it's just
loss is none but else all of this
happens and we can create a loss
so this will make it so
um
if we have the targets we provide them
and get a loss if we have no targets
we'll just get the logits
so this here will generate from the
model
um and let's take that for a ride now
oops
so I have another code chunk here which
will generate for the model from the
model and okay this is kind of crazy so
maybe let me let me break this down
so these are the idx right
I'm creating a batch will be just one
time will be just one
so I'm creating a little one by one
tensor and it's holding a zero
and the D type the data type is integer
so 0 is going to be how we kick off the
generation and remember that zero is uh
is the element standing for a new line
character so it's kind of like a
reasonable thing to to feed in as the
very first character in a sequence to be
the new line
um so it's going to be idx which we're
going to feed in here then we're going
to ask for 100 tokens
and then enter generate will continue
that
now because uh generate works on the
level of batches we then have to index
into the zero throw to basically unplug
the um
the single bash Dimension that exists
and then that gives us a um
time steps it's just a one-dimensional
array of all the indices which we will
convert to simple python list
from pytorch tensor so that that can
feed into our decode function and
convert those integers into text
so let me bring this back and we're
generating 100 tokens let's run
and uh here's the generation that we
achieved so obviously it's garbage and
the reason it's garbage is because this
is a totally random model so next up
we're going to want to train this model
now one more thing I wanted to point out
here is
this function is written to be General
but it's kind of like ridiculous right
now because
we're feeding in all this we're building
out this context and we're concatenating
it all and we're always feeding it all
into the model
but that's kind of ridiculous because
this is just a simple background model
so to make for example this prediction
about K we only needed this W but
actually what we fed into the model is
we fed the entire sequence and then we
only looked at the very last piece and
predicted k
so the only reason I'm writing it in
this way is because right now this is a
bygram model but I'd like to keep this
function fixed and I'd like it to work
later when our character is actually
basically look further in the history
and so right now the history is not used
so this looks silly but eventually the
history will be used and so that's why
we want to do it this way so just a
quick comment on that so now we see that
this is um random so let's train the
model so it becomes a bit less random
okay let's Now train the model so first
what I'm going to do is I'm going to
create a pytorch optimization object
so here we are using the optimizer
atom W
now in the make more series we've only
ever used stochastic gradient descent
the simplest possible Optimizer which
you can get using the SGD instead but I
want to use Adam which is a much more
advanced and popular Optimizer and it
works extremely well for a typical good
setting for the learning rate is roughly
3E negative four but for very very small
networks luck is the case here you can
get away with much much higher learning
rates running negative three or even
higher probably
but let me create the optimizer object
which will basically take the gradients
and update the parameters using the
gradients
and then here
our batch size up above was only four so
let me actually use something bigger
let's say 32 and then for some number of
steps
um we are sampling a new batch of data
we're evaluating the loss we're zeroing
out all the gradients from the previous
step getting the gradients for all the
parameters and then using those
gradients to update our parameters so
typical training loop as we saw in the
make more series
so let me now uh run this
for say 100 iterations and let's see
what kind of losses we're gonna get
so we started around 4.7
and now we're going to down to like 4.6
4.5
Etc so the optimization is definitely
happening but
um let's uh sort of try to increase the
number of iterations and only print at
the end
because we probably will not train for
longer
okay so we're down to 3.6 roughly
roughly down to three
this is the most janky optimization
okay it's working let's just do ten
thousand
and then from here we want to copy this
and hopefully we're going to get
something reasonable and of course it's
not going to be Shakespeare from a
background model but at least we see
that the loss is improving and hopefully
we're expecting something a bit more
reasonable
okay so we're down there about 2.5 ish
let's see what we get
okay
dramatic improvements certainly on what
we had here
so let me just increase the number of
tokens
okay so we see that we're starting to
get something at least like
reasonable ish
um
certainly not Shakespeare but the model
is making progress so that is the
simplest possible model
so now what I'd like to do is
obviously that this is a very simple
model because the tokens are not talking
to each other so given the previous
context of whatever was generated we're
only looking at the very last character
to make the predictions about what comes
next so now these uh now these tokens
have to start talking to each other and
figuring out what is in the context so
that they can make better predictions
for what comes next and this is how
we're going to kick off the Transformer
okay so next I took the code that we
developed in this Jupiter notebook and I
converted it to be a script and I'm
doing this because I just want to
simplify our intermediate work into just
the final product that we have at this
point
so in the top here I put all the hyper
parameters that we've defined I
introduced a few and I'm going to speak
to that in a little bit otherwise a lot
of this should be recognizable
reproducibility
read data get the encoder in the decoder
create the training test splits I use
the uh kind of like data loader that
gets a batch of the inputs and targets
this is new and I'll talk about it in a
second
now this is the background language
model that we developed and it can
forward and give us a logits and loss
and it can generate
and then here we are creating the
optimizer and this is the training Loop
so everything here should look pretty
familiar now some of the small things
that I added number one I added the
ability to run on a GPU if you have it
so if you have a GPU then you can this
will use Cuda instead of just CPU and
everything will be a lot more faster now
when device becomes screwed up then we
need to make sure that when we load the
data we move it to device
when we create the model we want to move
the model parameters to device
so as an example here we have the NN
embedding table and it's got a double
weight inside it which stores the sort
of lookup table so that would be moved
to the GPU so that all the calculations
here happen on the GPU and they can be a
lot faster
and then finally here when I'm creating
the context that feeds into generate I
have to make sure that I create on the
device
number two what I introduced is
the fact that here in the training Loop
here I was just printing the Lost dot
item
inside the training Loop but this is a
very noisy measurement of the current
loss because every batch will be more or
less lucky
and so what I want to do usually is I
have an estimate loss function and the
estimated loss basically then goes up
here
and it averages up the loss over
multiple batches
so in particular we're going to iterate
invalider times and we're going to
basically get our loss and then we're
going to get the average loss for both
splits and so this will be a lot less
noisy
so here what we call the estimate loss
we're going to report the pretty
accurate train and validation loss
now when we come back up you'll notice a
few things here I'm setting the model to
evaluation phase and down here I'm
resetting it back to training phase
now right now for our model as is this
this doesn't actually do anything
because the only thing inside this model
is this nn.embedding and
um this this network would behave both
would be have the same in both
evaluation mode and training mode we
have no Dropout layers we have no
bathroom layers Etc but it is a good
practice to Think Through what mode your
neural network is in because some layers
will have different Behavior at
inference time or training time
and
there's also this context manager
torch.nograd and this is just telling
pytorch that everything that happens
inside this function we will not call
that backward on and so Patrick can be a
lot more efficient with its memory use
because it doesn't have to store all the
intermediate variables because we're
never going to call backward and so it
can it can be a lot more memory
efficient in that way so also a good
practice to tell Pi torch when we don't
intend to do back propagation
so right now the script is about 120
lines of code of and that's kind of our
starter code
I'm calling it background.pi and I'm
going to release it later now running
this script gives us output in the
terminal and it looks something like
this
it basically as I ran this code it was
giving me the train loss and Val loss
and we see that we convert to somewhere
around 2.5
with the migrant model and then here's
the sample that we produced at the end
and so we have everything packaged up in
the script and we're in a good position
now to iterate on this okay so we are
almost ready to start writing our very
first self-attention block for
processing these tokens
now before we actually get there I want
to get you used to a mathematical trick
that is used in the self attention
inside a Transformer and is really just
like at the heart of an efficient
implementation of self-attention and so
I want to work with this toy example you
just get used to this operation and then
it's going to make it much more clear
once we actually get to um to it in the
script again
so let's create a b by T by C where B T
and C are just 4 8 and 2 in the story
example and these are basically channels
and we have batches and we have the time
component and we have some information
at each point in the sequence so C
now what we would like to do is we would
like these um tokens so we have up to
eight tokens here in a batch and these
eight tokens are currently not talking
to each other and we would like them to
talk to each other we'd like to couple
them
and in particular we don't we we want to
couple them in a very specific way so
the token for example at the fifth
location it should not communicate with
tokens in the sixth seventh and eighth
location
because those are future tokens in the
sequence
the token on the fifth location should
only talk to the one in the fourth third
second and first
so it's only so information only flows
from previous context to the current
timestamp and we cannot get any
information from the future because we
are about to try to predict the future
so
what is the easiest way for tokens to
communicate okay the easiest way I would
say is okay if we are up to if we're a
fifth token and I'd like to communicate
with my past the simplest way we can do
that is to just do a weight is to just
do an average of all the um of all the
preceding elements so for example if I'm
the fifth token I would like to take the
channels that make up that are
information at my step but then also the
channels from the four step third step
second step in the first step I'd like
to average those up and then that would
become sort of like a feature Vector
that summarizes me in the context of my
history
now of course just doing a sum or like
an average is an extremely weak form of
interaction like this communication is
extremely lossy we've lost a ton of
information about the spatial
Arrangements of all those tokens but
that's okay for now we'll see how we can
bring that information back later
for now what we would like to do is
for every single batch element
independently
for every teeth token in that sequence
we'd like to now calculate the average
of all the vectors in all the previous
tokens and also at this token
so let's write that out
um I have a small snippet here and
instead of just fumbling around let me
just copy paste it and talk to it
so in other words we're going to create
X
and bow is short for backup words
because backup words is um is kind of
like um
a term that people use when you are just
averaging up things so it's just a bag
of words basically there's a word stored
on every one of these eight locations
and we're doing a bag of words such as
averaging
so in the beginning we're going to say
that it's just initialized at Zero and
then I'm doing a for Loop here so we're
not being efficient yet that's coming
but for now we're just iterating over
all the batch Dimensions independently
iterating over time
and then the previous tokens are at this
batch Dimension and then everything up
to and including the teeth token okay
so when we slice out X in this way xrev
Becomes of shape
um how many T elements there were in the
past and then of course C so all the two
dimensional information from these log
tokens
so that's the previous sort of chunk of
um tokens from my current sequence
and then I'm just doing the average or
the mean over the zeroth dimension so
I'm averaging out the time here
and I'm just going to get a little C
one-dimensional Vector which I'm going
to store in X background words
so I can run this and uh this is not
going to be very informative because
let's see so this is x sub 0. so this is
the zeroth batch element and then expo
at zero now
you see how the at the first location
here you see that the two are equal and
that's because it's we're just doing an
average of this one token
but here this one is now an average of
these two
and now this one is an average of these
three
and so on
so uh and this last one is the average
of all of these elements so vertical
average just averaging up all the tokens
now gives this outcome here
so this is all well and good but this is
very inefficient now the trick is that
we can be very very efficient about
doing this using matrix multiplication
so that's the mathematical trick and let
me show you what I mean let's work with
the toy example here
let me run it and I'll explain
I have a simple Matrix here that is a
three by three of all ones
a matrix B of just random numbers and
it's a three by two
and a matrix C which will be three by
three multiply three by two which will
give out a three by two
so here we're just using
um
matrix multiplication
so a multiply B gives us C
okay so how are these numbers in C
achieved right so this number in the top
left is the first row of a DOT product
with the First Column of B
and since all the the row of a right now
is all just once
then the dot product here with with this
column of B is just going to do a sum of
these of this column so 2 plus 6 plus 6
is 14.
the element here and the output of C is
also the first column here the first row
of a multiplied now with the second
column of B so 7 plus 4 plus plus 5 is
16.
now you see that there's repeating
elements here so this 14 again is
because this row is again all once and
it's multiplying the First Column of B
so we get 14. and this one is and so on
so this last number here is the last row
dot product last column
now the trick here is uh the following
this is just a boring number of
um it's just a boring array of all ones
but torch has this function called trell
which is short for a triangular
uh something like that and you can wrap
it in torched at once and it will just
return the lower triangular portion of
this
okay
so now it will basically zero out uh
these guys here so we just get the lower
triangular part well what happens if we
do that
so now we'll have a like this and B like
this and now what are we getting here in
C
well what is this number well this is
the first row times the First Column and
because this is zeros
uh these elements here are now ignored
so we just get a two
and then this number here is the first
row times the second column and because
these are zeros they get ignored and
it's just seven the seven multiplies
this one
but look what happened here because this
is one and then zeros we what ended up
happening is we're just plucking out the
row of this row of B and that's what we
got
now here we have 1 1 0. so here one one
zero dot product with these two columns
will now give us two plus six which is
eight and seven plus four which is 11.
and because this is one one one we ended
up with the addition of all of them
and so basically depending on how many
ones and zeros we have here we are
basically doing a sum currently of a
variable number of these rows and that
gets deposited into C
So currently we're doing sums because
these are ones but we can also do
average right and you can start to see
how we could do average of the rows of B
uh sort of in an incremental fashion
because we don't have to we can
basically normalize these rows so that
they sum to one and then we're going to
get an average
so if we took a and then we did a equals
a divide a torch.sum
in the um
of a in the warmth
Dimension and then let's keep them as
true so therefore the broadcasting will
work out
so if I rerun this you see now that
these rows now sum to one so this row is
one this row is 0.5.50 and here we get
one thirds
and now when we do a multiply B what are
we getting
here we are just getting the first row
first row
here now we are getting the average of
the first two rows
okay so 2 and 6 average is four and four
and seven average is 5.5
and on the bottom here we are now
getting the average of these three rows
so the average of all of elements of B
are now deposited here
and so you can see that by manipulating
these uh elements of this multiplying
Matrix and then multiplying it with any
given Matrix we can do these averages in
this incremental fashion because we just
get
um
and we can manipulate that based on the
elements of a okay so that's very
convenient so let's swing back up here
and see how we can vectorize this and
make it much more efficient using what
we've learned
so in particular
we are going to produce an array a but
here I'm going to call it way short for
weights
but this is r a
and this is how much of every row we
want to average up and it's going to be
an average because you can see it in
these rows sum to 1.
so this is our a and then our B in this
example of course is
X
so it's going to happen here now is that
we are going to have an expo 2.
and this Expo 2 is going to be way
multiplying
RX
so let's think this through way is T by
T and this is Matrix multiplying in pi
torch a b by T by C
and it's giving us
uh the what shape so pytorch will come
here and then we'll see that these
shapes are not the same so it will
create a batch Dimension here and this
is a batched matrix multiply
and so it will apply this matrix
multiplication in all the batch elements
in parallel
and individually and then for each batch
element there will be a t by T
multiplying T by C exactly as we had
below
so this will now create
B by T by C
and X both 2 will now become identical
to Expo
so
we can see that torch.all close
of Expo and Expo 2 should be true now
so this kind of like misses us that uh
these are in fact the same
so Expo and Expo 2 if I just print them
uh okay we're not going to be able to
okay we're not going to be able to just
stare it down but
um
well let me try Expo basically just at
the zeroth element and Expo two at the
zeroth element so just the first batch
and we should see that this and that
should be identical which they are
right so what happened here the trick is
we were able to use batched Matrix
multiply
to do this uh aggregation really and
it's awaited aggregation and the weights
are specified in this T by T array
and we're basically doing weighted sums
and uh these weighted sums are according
to the weights inside here they take on
sort of this triangular form
and so that means that a token at the
teeth Dimension will only get uh sort of
um information from the um tokens
preceding it so that's exactly what we
want and finally I would like to rewrite
it in one more way
and we're going to see why that's useful
so this is the third version and it's
also identical to the first and second
but let me talk through it it uses
softmax
so
Trill here is this Matrix lower
triangular ones
way begins as all zero
okay so if I just print way in the
beginning it's all zero
then I used
masked fill
so what this is doing is
wait that masked fill it's all zeros and
I'm saying for all the elements where
Trill is equals equals zero make them be
negative Infinity
so all the elements where Trill is zero
will become negative Infinity now
so this is what we get
and then the final one here is softmax
so if I take a soft Max along every
single so dim is negative one so along
every single row
if I do a soft Max what is that going to
do
well softmax is um
it's also like a normalization operation
right
and so spoiler alert you get the exact
same Matrix
let me bring back the softmax
and recall that in softmax we're going
to exponentiate every single one of
these
and then we're going to divide by the
sum
and so for if we exponentiate every
single element here we're going to get a
one and here we're going to get uh
basically zero zero zero zero zero
everywhere else
and then when we normalize we just get
one here we're going to get 1 1 and then
zeros and then softmax will again divide
and this will give us 0.5.5 and so on
and so this is also the uh the same way
to produce this mask
now the reason that this is a bit more
interesting and the reason we're going
to end up using it and solve a tension
is that
these weights here begin uh with zero
and you can think of this as like an
interaction strength or like an affinity
so basically it's telling us how much of
each token from the past do we want to
Aggregate and average up
and then this line is saying tokens from
the past cannot communicate by setting
them to negative Infinity we're saying
that we will not aggregate anything from
those tokens
and so basically this then goes through
softmax and through the weighted and
this is the aggregation through matrix
multiplication
and so what this is now is you can think
of these as
um these zeros are currently just set by
us to be zero but a quick preview is
that these affinities between the tokens
are not going to be just constant at
zero they're going to be data dependent
these tokens are going to start looking
at each other and some tokens will find
other tokens more or less interesting
and depending on what their values are
they're going to find each other
interesting to different amounts and I'm
going to call those affinities I think
and then here we are saying the future
cannot communicate with the past we're
going to clamp them
and then when we normalize and sum we're
going to aggregate sort of their values
depending on how interesting they find
each other
and so that's the preview for
self-attention and basically long story
short from this entire section is that
you can do weighted aggregations of your
past elements
by having by using matrix multiplication
of a lower triangular fashion
and then the elements here in the lower
triangular part are telling you how much
of each element fuses into this position
so we're going to use this trick now to
develop the self-attention block so
first let's get some quick preliminaries
out of the way
first the thing I'm kind of bothered by
is that you see how we're passing in
vocab size into the Constructor there's
no need to do that because vocab size
has already defined up top as a global
variable so there's no need to pass this
stuff around
next one I want to do is I don't want to
actually create I want to create like a
level of interaction here where we don't
directly go to the embedding for the um
logits but instead we go through this
intermediate phase because we're going
to start making that bigger so let me
introduce a new variable and embed a
short for a number of embedding
dimensions
so an embed
here
will be say 32. that was a suggestion
from GitHub by the way it also showed us
to 32 which is a good number
so this is an embedding table and only
32 dimensional embeddings
so then here this is not going to give
us logits directly instead this is going
to give us token embeddings
that's what I'm going to call it and
then to go from the token embeddings to
the logits we're going to need a linear
layer so self.lm head let's call it
short for language modeling head
is n linear from an embed up to vocab
size
and then when we swing over here we're
actually going to get the logits by
exactly what the copilot says
now we have to be careful here because
this C and this C are not equal
this is an embedded C and this is vocab
size
so let's just say that an embed is equal
to C
and then this just creates one spurious
layer of interaction through a linear
layer but this should basically run
so we see that this runs and uh this
currently looks kind of spurious but
we're going to build on top of this now
next up so far we've taken these in in
the seas and we've encoded them based on
the identity of the tokens inside idx
the next thing that people very often do
is that we're not just encoding the
identity of these tokens but also their
position
so we're going to have a second position
uh embedding table here so solve that
position embedding table
is an embedding of block size by an
embed and so each position from 0 to
block size minus 1 will also get its own
embedding vector
and then here first let me decode a b by
T from idx.shape
and then here we're also going to have a
positive bedding which is the positional
embedding and these are this is tour
Dutch arrange so this will be basically
just integers from 0 to T minus 1.
and all of those integers from 0 to T
minus 1 get embedded through the table
to create a t by C
and then here this gets renamed to just
say x and x will be
the addition of the token embeddings
with the positional embeddings
and here the broadcasting note will work
out so B by T by C plus T by C this gets
right aligned a new dimension of one
gets added and it gets broadcasted
across batch
so at this point x holds not just the
token identities but the positions at
which these tokens occur
and this is currently not that useful
because of course we just have a simple
migrain model so it doesn't matter if
you're in the fifth position the second
position or wherever it's all
translation invariant at this stage so
this information currently wouldn't help
but as we work on the self potential
block we'll see that this starts to
matter
okay so now we get the Crux of
self-attention so this is probably the
most important part of this video to
understand
we're going to implement a small
self-attention for a single individual
head as they're called
so we start off with where we were so
all of this code is familiar
so right now I'm working with an example
where I change the number of channels
from 2 to 32 so we have a 4x8
arrangement of tokens and each and the
information at each token is currently
32 dimensional but we just are working
with random numbers
now we saw here that
the code as we had it before does a
simple weight a simple average of all
the past tokens and the current token so
it's just the previous information and
current information is just being mixed
together in an average
and that's what this code currently
achieves and it does so by creating this
lower triangular structure which allows
us to mask out this weight Matrix that
we create
so we mask it out and then we normalize
it and currently when we initialize the
affinities between all the different
sort of tokens or nodes I'm going to use
those terms interchangeably
so when we initialize the affinities
between all the different tokens to be
zero
then we see that way gives us this
structure where every single row has
these um
uniform numbers and so that's what
that's what then uh in this Matrix
multiply makes it so that we're doing a
simple average
now
we don't actually want this to be
All Uniform because different uh tokens
will find different other tokens more or
less interesting and we want that to be
data dependent so for example if I'm a
vowel then maybe I'm looking for
consonants in my past and maybe I want
to know what those consonants are and I
want that information to Flow To Me
and so I want to now gather information
from the past but I want to do it in a
data dependent way and this is the
problem that self-attention solves
now the way self-attention solves this
is the following every single node or
every single token at each position will
emit two vectors
it will emit a query and it will emit a
key
now the query Vector roughly speaking is
what am I looking for
and the key Vector roughly speaking is
what do I contain
and then the way we get affinities
between these tokens now in a sequence
is we basically just do a DOT product
between the keys and the queries
so my query dot products with all the
keys of all the other tokens and that
dot product now becomes way
and so um if the key and the query are
sort of aligned they will interact to a
very high amount and then I will get to
learn more about that specific token as
opposed to any other token in the
sequence so let's implement this tab
we're going to implement a single
what's called head of self-attention
so this is just one head there's a hyper
parameter involved with these heads
which is the head size
and then here I'm initializing the
linear modules and I'm using bias equals
false so these are just going to apply a
matrix multiply with some fixed weights
and now let me produce a
key and Q K and Q by forwarding these
modules on x
so the size of this will not become
B by T by 16 because that is the head
size and the same here B by T by 16.
so this being that size
so you see here that when I forward this
linear on top of my X all the tokens in
all the positions in the B by T
Arrangement all of them in parallel and
independently produce a key and a query
so no communication has happened yet
but the communication comes now all the
queries will dot product with all the
keys
so basically what we want is we want way
now or the affinities between these to
be query multiplying key
but we have to be careful with uh we
can't Matrix multiply this we actually
need to transpose uh K but we have to be
also careful because these are when you
have the batch Dimension so in
particular we want to transpose uh the
last two Dimensions Dimension negative
one and dimension negative two
so negative 2 negative 1.
and so this Matrix multiplied now will
basically do the following B by T by 16
Matrix multiplies B by 16 by T to give
us
B by T by T
right
so for every row of B we're not going to
have a t-square matrix giving us the
affinities and these are now the way
so they're not zeros they are now coming
from this dot product between the keys
and the queries
so this can now run I can I can run this
and the weighted aggregation now is a
function in a data dependent manner
between the keys and queries of these
nodes
so just inspecting what happened here
the way takes on this form
and you see that before way was just a
constant so it was applied in the same
way to all the batch elements but now
every single batch elements will have
different sort of way because uh every
single batch element contains different
tokens at different positions and so
this is not data dependent
so when we look at just the zeroth row
for example in the input these are the
weights that came out and so you can see
now that they're not just exactly
uniform
and in particular as an example here for
the last row this was the eighth token
and the eighth token knows what content
it has and it knows at what position
it's in
and now the eighth token based on that
creates a query hey I'm looking for this
kind of stuff I'm a vowel I'm on the
eighth position I'm looking for any
consonants at positions up to four
and then all the nodes get to emit keys
and maybe one of the channels could be I
am a I am a consonant and I am in a
position up to four
and that key would have a high number in
that specific Channel and that's how the
query and the key when they dot product
they can find each other and create a
high affinity
and when they have a high Affinity like
say this token was pretty interesting to
uh to this eighth token
when they have a high Affinity then
through the soft Max I will end up
aggregating a lot of its information
into my position
and so I'll get to learn a lot about it
now just this we're looking at way after
this has already happened
um
let me erase this operation as well so
let me erase the masking and the softmax
just to show you the under the hood
internals and how that works
so without the masking in the softmax
way comes out like this right this is
the outputs of the dot products
and these are the raw outputs and they
take on values from negative you know
two to positive two Etc
so that's the raw interactions and raw
affinities between all the nodes
but now if I'm a if I'm a fifth node I
will not want to aggregate anything from
the six node seventh node and the eighth
node so actually we use the upper
triangular masking so those are not
allowed to communicate
and now we actually want to have a nice
uh distribution so we don't want to
aggregate negative 0.11 of this node
that's crazy so instead we exponentiate
and normalize and now we get a nice
distribution that seems to one
and this is telling us now in the data
dependent manner how much of information
to aggregate from any of these tokens in
the past
so that's way and it's not zeros anymore
but but it's calculated in this way now
there's one more uh part to a single
self-attention head and that is that
when you do the aggregation we don't
actually aggregate the tokens exactly we
aggregate we produce one more value here
and we call that the value
so in the same way that we produced p
and query we're also going to create a
value
and then
here
we don't aggregate
X we calculate a v which is just
achieved by propagating this linear on
top of X again and then we
output way multiplied by V so V is the
elements that we aggregate or the the
vector that we aggregate instead of the
raw X
and now of course this will make it so
that the output here of the single head
will be 16 dimensional because that is
the head size
so you can think of X as kind of like a
private information to this token if you
if you think about it that way so X is
kind of private to this token so I'm a
fifth token at some and I have some
identity and my information is kept in
Vector X
and now for the purposes of the single
head here's what I'm interested in
here's what I have
and if you find me interesting here's
what I will communicate to you and
that's stored in v
and so V is the thing that gets
aggregated for the purposes of this
single head between the different nodes
and that's uh
basically the self attention mechanism
this is this is what it does
there are a few notes that I would make
like to make about attention number one
attention is a communication mechanism
you can really think about it as a
communication mechanism where you have a
number of nodes in a directed graph
where basically you have edges pointing
between those like this
and what happens is every node has some
Vector of information and it gets to
aggregate information via a weighted sum
from all the nodes that point to it
and this is done in a data dependent
manner so depending on whatever data is
actually stored at each node at any
point in time
now
our graph doesn't look like this our
graph has a different structure we have
eight nodes because the block size is
eight and there's always eight tokens
and the first node is only pointed to by
itself the second node is pointed to by
the first node and itself all the way up
to the eighth node which is pointed to
by all the previous nodes and itself
and so that's the structure that our
directed graph has or happens happens to
have in other aggressive sort of
scenario like language modeling but in
principle attention can be applied to
any arbitrary directed graph and it's
just a communication mechanism between
the nodes
the second note is that notice that
there is no notion of space so attention
simply acts over like a set of vectors
in this graph and so by default these
nodes have no idea where they are
positioned in a space and that's why we
need to encode them positionally and
sort of give them some information that
is anchored to a specific position so
that they sort of know where they are
and this is different than for example
from convolution because if you run for
example a convolution operation over
some input there's a very specific sort
of layout of the information in space in
the convolutional filters sort of act in
space and so it's it's not like an
attention in attention is just a set of
vectors out there in space they
communicate and if you want them to have
a notion of space you need to
specifically add it which is what we've
done when we calculated the um relative
the position loan code encodings and
added that information to the vectors
the next thing that I hope is very clear
is that the elements across the batch
Dimension which are independent examples
never talk to each other don't always
processed independently and this is a
bashed Matrix multiply that applies
basically a matrix multiplication kind
of in parallel across the batch
Dimension so maybe it would be more
accurate to say that in this analogy of
a directed graph we really have because
the batch size is four we really have
four separate pools of eight nodes and
those eight nodes only talk to each
other but in total there's like 32 nodes
that are being processed but there's um
sort of four separate pools of eight you
can look at it that way
the next note is that here in the case
of language modeling uh we have this
specific structure of directed graph
where the future tokens will not
communicate to the Past tokens but this
doesn't necessarily have to be the
constraint in the general case and in
fact in many cases you may want to have
all of the nodes talk to each other
fully so as an example if you're doing
sentiment analysis or something like
that with a Transformer you might have a
number of tokens and you may want to
have them all talk to each other fully
because later you are predicting for
example the sentiment of the sentence
and so it's okay for these nodes to talk
to each other
and so in those cases you will use an
encoder block of self-attention and all
it means that it's an encoder block is
that you will delete this line of code
allowing all the nodes to completely
talk to each other what we're
implementing here is sometimes called a
decoder block and it's called a decoder
because it is sort of like a decoding
language and it's got this Auto
aggressive format where you have to mask
with the Triangular Matrix so that nodes
from the future never talk to the Past
because they would give away the answer
and so basically in encoder blocks you
would delete this allow all the nodes to
talk in decoder blocks this will always
be present so that you have this
triangular structure but both are
allowed and attention doesn't care
attention supports arbitrary
connectivity between nodes
the next thing I wanted to comment on is
you keep me you keep hearing me say
attention self-attention Etc there's
actually also something called cross
attention what is the difference
so
basically the reason this attention is
self-attention is because the keys
queries and the values are all coming
from the same Source from X so the same
Source X produces case queries and
values so these nodes are self-attending
but in principle attention is much more
General than that so for example an
encoder decoder Transformers uh you can
have a case where the queries are
produced from X but the keys and the
values come from a whole separate
external source and sometimes from
encoder blocks that encode some context
that we'd like to condition on and so
the keys and the values will actually
come from a whole separate Source those
are nodes on the side and here we're
just producing queries and we're reading
off information from the side
so cross attention is used when there's
a separate source of nodes we'd like to
pull information from into our nodes and
it's self-attention if we just have
nodes that would like to look at each
other and talk to each other
so this attention here happens to be
self-attention
but in principle
um
attention is a lot more General okay in
the last note at this stage is if we
come to the attention is all you need
paper here we've already implemented
attention so given query key and value
we've multiplied the query on the key
we've softmaxed it and then we are
aggregating the values
there's one more thing that we're
missing here which is the dividing by
one over square root of the head size
the DK here is the head size why aren't
they doing this once it's important so
they call it a scaled attention
and it's kind of like an important
normalization to basically have
the problem is if you have unit gaussian
inputs so zero mean unit variance K and
Q are unit caution and if you just do
way naively then you see that your way
actually will be uh the variance will be
on the order of head size which in our
case is 16.
but if you multiply by one over head
size square root so this is square root
and this is one over
then the variance of way will be one so
it will be preserved
now why is this important you'll notice
that way here
will feed into softmax
and so it's really important especially
at initialization that way be fairly
diffuse
so in our case here we sort of lucked
out here and weigh had a fairly diffuse
numbers here so
um like this now the problem is that
because of softmax if weight takes on
very positive and very negative numbers
inside it softmax will actually converge
towards one hot vectors and so I can
illustrate that here
um
say we are applying softmax to a tensor
of values that are very close to zero
then we're going to get a diffuse thing
out of softmax
but the moment I take the exact same
thing and I start sharpening it making
it bigger by multiplying these numbers
by eight for example you'll see that the
soft Max will start to sharpen and in
fact it will sharpen towards the max so
it will sharpen towards whatever number
here is the highest
and so
um basically we don't want these values
to be too extreme especially the
initialization otherwise softmax will be
way too peaky and you're basically
aggregating
um information from like a single node
every node just Aggregates information
from a single other node that's not what
we want especially its initialization
and so the scaling is used just to
control the variance at initialization
okay so having said all that let's now
take our soft retention knowledge and
let's take it for a spin
so here in the code I created this head
module and implements a single head of
self-attention
so you give it a head size and then here
it creates the key query and the value
linear layers typically people don't use
biases in these
so those are the linear projections that
we're going to apply to all of our nodes
now here I'm creating this Trill
variable Trill is not a parameter of the
module so in sort of pythonomic
conventions this is called a buffer it's
not a parameter and you have to call it
you have to assign it to the module
using a register buffer so that creates
the trail
uh the triangle lower triangular Matrix
and when we're given the input X this
should look very familiar now we
calculate the keys the queries we call
it clock in the attentions course inside
way we normalize it so we're using
scaled attention here
then we make sure that a feature doesn't
communicate with the past so this makes
it a decoder block
and then softmax and then aggregate the
value and output
then here in the language model I'm
creating a head in the Constructor and
I'm calling it self attention head and
the head size I'm going to keep as the
same and embed just for now
and then here once we've encoded the
information with the token embeddings
and the position embeddings we're simply
going to feed it into the
self-attentioned head and then the
output of that is going to go into uh
the decoder language modeling head and
create the logits so this is the sort of
the simplest way to plug in a
self-attention component into our
Network right now
I had to make one more change which is
that here
in the generate we have to make sure
that our idx that we feed into the model
because now we're using positional
embeddings we can never have more than
block size coming in because if idx is
more than block size then our position
embedding table is going to run out of
scope because it only has embeddings for
up to block size
and so therefore I added some code here
to crop the context that we're going to
feed into self
so that we never pass in more than block
size elements
so those are the changes and let's Now
train the network okay so I also came up
to the script here and I decreased the
learning rate because the self-attention
can't tolerate very very high learning
rates and then I also increase the
number of iterations because the
learning rate is lower and then I
trained it and previously we were only
able to get to up to 2.5 and now we are
down to 2.4 so we definitely see a
little bit of an improvement from 2.5 to
2.4 roughly but the text is still not
amazing so clearly the self-attention
head is doing some useful communication
but
um we still have a long way to go okay
so now we've implemented the
scale.product attention now next up in
the attention is all you need paper
there's something called multi-head
attention and what is multi-head
attention it's just applying multiple
attentions in parallel and concatenating
the results
so they have a little bit of diagram
here I don't know if this is super clear
it's really just multiple attentions in
parallel
so let's Implement that fairly
straightforward
if we want a multi-head attention then
we want multiple heads of self-attention
running in parallel
so in pytorch we can do this by simply
creating multiple heads
so however heads how many however many
heads you want and then what is the head
size of each
and then we run all of them in parallel
into a list and simply concatenate all
of the outputs and we're concatenating
over the channel dimension
so the way this looks now is we don't
have just a single attention
that has a hit size of 32 because
remember and in bed is 32.
instead of having one Communication
channel we now have four communication
channels in parallel and each one of
these communication channels typically
will be smaller correspondingly so
because we have four communication
channels we want eight dimensional
self-attention and so from each
Communication channel we're going to
gather eight dimensional vectors and
then we have four of them and that
concatenates to give us 32 which is the
original and embed
and so this is kind of similar to um if
you're familiar with convolutions this
is kind of like a group convolution
because basically instead of having one
large convolution we do convolutional
groups and uh that's multi-headed
self-attention
and so then here we just use sa heads
self-attussion Heads instead
now I actually ran it and uh scrolling
down
I ran the same thing and then we now get
this down to 2.28 roughly and the output
is still the generation is still not
amazing but clearly the validation loss
is improving because we were at 2.4 just
now
and so it helps to have multiple
communication channels because obviously
these tokens have a lot to talk about
and they want to find the consonants the
vowels they want to find the vowels just
from certain positions they want to find
any kinds of different things and so it
helps to create multiple independent
channels of communication gather lots of
different types of data and then decode
the output now going back to the paper
for a second of course I didn't explain
this figure in full detail but we are
starting to see some components of what
we've already implemented we have the
positional encodings the token encodings
that add we have the masked multi-headed
attention implemented now here's another
multi-headed tension which is a cross
attention to an encoder which we haven't
we're not going to implement in this
case I'm going to come back to that
later
but I want you to notice that there's a
feed forward part here and then this is
grouped into a block that gets repeated
again and again
now the feed forward part here is just a
simple multi-layer perceptron
um
so the multi-headed so here position
wise feed forward networks is just a
simple little MLP
so I want to start basically in a
similar fashion also adding computation
into the network
and this computation is on the per node
level so
I've already implemented it and you can
see the diff highlighted on the left
here when I've added or changed things
now before we had the multi-headed
self-attention that did the
communication but we went way too fast
to calculate the logits so the tokens
looked at each other but didn't really
have a lot of time to think on what they
found from the other tokens
and so what I've implemented here is a
little feed forward single layer and
this little layer is just a linear
followed by a relative nonlinearity and
that's that's it
so it's just a little layer and then I
call it feed forward
and embed
and then this feed forward is just
called sequentially right after the
self-attention so we self-attend then we
feed forward and you'll notice that the
feet forward here when it's applying
linear this is on a per token level all
the tokens do this independently so the
self-attention is the communication and
then once they've gathered all the data
now they need to think on that data
individually
and so that's what feed forward is doing
and that's why I've added it here now
when I train this the validation loss
actually continues to go down now to
2.24 which is down from 2.28 the output
still look kind of terrible but at least
we've improved the situation
and so as a preview
we're going to now start to intersperse
the communication with the computation
and that's also what the Transformer
does when it has blocks that communicate
and then compute and it groups them and
replicates them
okay so let me show you what we like to
do we'd like to do something like this
we have a block and this block is
basically this part here except for the
cross attention
now the block basically intersperses
communication and then computation the
computation the communication is done
using multi-headed self-attention and
then the computation is done using the
feed forward Network on all the tokens
independently
now what I've added here also is you'll
notice
this takes the number of embeddings in
the embedding Dimension and number of
heads that we would like which is kind
of like group size in group convolution
and I'm saying that number of heads we'd
like is for and so because this is 32 we
calculate that because this 32 the
number of hats should be four
um there's num the head size should be
eight so that everything sort of works
out Channel wise
um so this is how the Transformer
structures uh sort of the uh the sizes
typically
so the head size will become eight and
then this is how we want to intersperse
them and then here I'm trying to create
blocks which is just a sequential
application of block block so that we're
interspersing communication feed forward
many many times and then finally we
decode
now actually try to run this and the
problem is this doesn't actually give a
very good uh answer a very good result
and the reason for that is we're
starting to actually get like a pretty
deep neural net and deep neural Nets uh
suffer from optimization issues and I
think that's where we're kind of like
slightly starting to run into so we need
one more idea that we can borrow from
the
um Transformer paper to resolve those
difficulties now there are two
optimizations that dramatically help
with the depth of these networks and
make sure that the networks remain
optimizable let's talk about the first
one
the first one in this diagram is you see
this Arrow here
and then this arrow and this Arrow those
are skip connections or sometimes called
residual connections
they come from this paper uh the
procedural learning form and recognition
from about 2015. that introduced the
concept
now these are basically what it means is
you transform the data but then you have
a skip connection with addition
from the previous features now the way I
like to visualize it that I prefer is
the following here the computation
happens from the top to bottom and
basically you have this uh residual
pathway and you are free to Fork off
from the residual pathway perform some
computation and then project back to the
residual pathway via addition
and so you go from the the inputs to the
targets only the plus and plus and plus
and the reason this is useful is because
during that propagation remember from
our micrograd video earlier addition
distributes gradients equally to both of
its branches that that fat as the input
and so the supervision or the gradients
from the loss basically hop
through every addition node all the way
to the input and then also Fork off into
the residual blocks
but basically you have this gradient
Super Highway that goes directly from
the supervision all the way to the input
unimpeded and then these virtual blocks
are usually initialized in the beginning
so they contribute very very little if
anything to the residual pathway they
they are initialized that way so in the
beginning they are sort of almost kind
of like not there but then during the
optimization they come online over time
and they start to contribute but at
least at the initialization you can go
from directly supervision to the input
gradient is unimpeded and just close and
then the blocks over time kick in and so
that dramatically helps with the
optimization so let's implement this so
coming back to our block here basically
what we want to do is we want to do x
equals X Plus
solve the tension and x equals X Plus
solve that feed forward
so this is X and then we Fork off and do
some communication and come back and we
Fork off and we do some computation and
come back
so those are residual connections and
then swinging back up here
we also have to introduce this
projection
so nn.linear
and this is going to be from
after we concatenate this this is the
precise and embed so this is the output
of the soft tension itself
but then we actually want the uh to
apply the projection
and that's the result
so the projection is just a linear
transformation of the outcome of this
layer
so that's the projection back into the
residual pathway
and then here in the feed forward it's
going to be the same thing I could have
a soft.projection here as well but let
me just simplify it
and let me
couple it inside the same sequential
container
and so this is the projection layer
going back into the residual pathway
and so
that's uh well that's it so now we can
train this so I implemented one more
small change when you look into the
paper again you see that the
dimensionality of input and output is
512 for them and they're saying that the
inner layer here in the feed forward has
dimensionality of 2048. so there's a
multiplier of four
and so the inner layer of the feed
forward Network
should be multiplied by four in terms of
Channel sizes so I came here and I
multiplied to four times embed here for
the feed forward and then from four
times n embed coming back down to an
embed when we go back to the project to
the projection so adding a bit of
computation here and growing that layer
that is in the residual block on the
side of the residual pathway
and then I trained this and we actually
get down all the way to uh 2.08
validation loss and we also see that the
network is starting to get big enough
that our train loss is getting ahead of
validation loss so we're starting to see
like a little bit of overfitting
and um our our um
Generations here are still not amazing
but at least you see that we can see
like is here this now grieve sank
like this starts to almost look like
English so
um yeah we're starting to really get
there okay and the second Innovation
that is very helpful for optimizing very
deep neural networks is right here so we
have this addition now that's the
residual part but this Norm is referring
to something called layer Norm
so layer Norm is implemented in pi torch
it's a paper that came out a while back
here
um
and layer Norm is very very similar to
Bachelor so remember back to our make
more series part three we implemented
batch normalization
and patch normalization basically just
made sure that across the batch
Dimension any individual neuron had unit
gaussian
distribution so it was zero mean and
unit standard deviation one standard
deviation output
so what I did here is I'm copy pasting
The Bachelor 1D that we developed in our
makemore series
and see here we can initialize for
example this module and we can have a
batch of 32 100 dimensional vectors
feeding through the bathroom layer
so what this does is it guarantees
that when we look at just the zeroth
column
it's a zero mean one standard deviation
so it's normalizing every single column
of this input
now the rows are not going to be
normalized by default because we're just
normalizing columns so let's now
implement the layer Norm uh it's very
complicated look we come here we change
this from 0 to 1. so we don't normalize
The Columns we normalize the rows
and now we've implemented layer Norm
so now the columns are not going to be
normalized
but the rows are going to be normalized
for every individual example it's 100
dimensional Vector is normalized in this
way and because our computation Now does
not span across examples we can delete
all of this buffers stuff because we can
always apply this operation and don't
need to maintain any running buffers so
we don't need the buffers
we don't There's no distinction between
training and test time
and we don't need these running buffers
we do keep gamma and beta we don't need
the momentum we don't care if it's
training or not
and this is now a layer Norm
and it normalizes the rows instead of
the columns and this here
is identical to basically this here
so let's now Implement layer Norm in our
Transformer before I incorporate the
layer Norm I just wanted to note that as
I said very few details about the
Transformer have changed in the last
five years but this is actually
something that slightly departs from the
original paper you see that the ADD and
Norm is applied after the transformation
but um in now it is a bit more basically
common to apply the layer Norm before
the transformation so there's a
reshuffling of the layer Norms uh so
this is called the pre-norm formulation
and that's the one that we're going to
implement as well so slight deviation
from the original paper
basically we need two layer Norms layer
Norm one is an N dot layer norm and we
tell it how many
um what is the embedding dimension
and we need the second layer Norm
and then here the layer rooms are
applied immediately on x
so self.layer number one in applied on x
and salt on layer number two applied on
X before it goes into sulfur tension and
feed forward
and the size of the layer Norm here is
an embeds of 32. so when the layer Norm
is normalizing our features it is the
normalization here
happens the mean and the variance are
taking over 32 numbers so the batch and
the time act as batch Dimensions both of
them so this is kind of like a per token
transformation that just normalizes the
features and makes them a unit mean unit
gaussian at initialization
but of course because these layer Norms
inside it have these gamma and beta
trainable parameters
the layer normal eventually create
outputs that might not be unit gaussian
but the optimization will determine that
so for now this is the uh this is
incorporating the layer norms and let's
train them up okay so I let it run and
we see that we get down to 2.06 which is
better than the previous 2.08 so a
slight Improvement by adding the layer
norms and I'd expect that they help even
more if we had bigger and deeper Network
one more thing I forgot to add is that
there should be a layer Norm here also
typically as at the end of the
Transformer and right before the final
linear layer that decodes into
vocabulary so I added that as well so at
this stage we actually have a pretty
complete Transformer according to the
original paper and it's a decoder only
Transformer I'll I'll talk about that in
a second but at this stage the major
pieces are in place so we can try to
scale this up and see how well we can
push this number
now in order to scale out the model I
had to perform some cosmetic changes
here to make it nicer so I introduced
this variable called end layer which
just specifies how many layers of the
blocks we're going to have I create a
bunch of blocks and we have a new
variable number of heads as well
I pulled out the layer Norm here and so
this is identical now one thing that I
did briefly change is I added a dropout
so Dropout is something that you can add
right before the residual connection
back
or right before the connection back into
the original pathway
so we can drop out that as the last
layer here
we can drop out uh here at the end of
the multi-headed extension as well
and we can also drop out here when we
calculate the um basically affinities
and after the soft Max we can drop out
some of those so we can randomly prevent
some of the nodes from communicating
and so Dropout comes from this paper
from 2014 or so
and basically it takes your neural net
and it randomly every forward backward
pass shuts off some subset of neurons
so randomly drops them to zero and
trains without them and what this does
effectively is because the mask of
what's being dropped out has changed
every single forward backward pass it
ends up kind of training an ensemble of
sub Networks and then at this time
everything is fully enabled and kind of
all of those sub networks are merged
into a single Ensemble if you can if you
want to think about it that way so I
would read the paper to get the full
detail for now we're just going to stay
on the level of this is a regularization
technique and I added it because I'm
about to scale up the model quite a bit
and I was concerned about overfitting
so now when we scroll up to the top uh
we'll see that I changed a number of
hyper parameters here about our neural
net so I made the batch size B much
larger now with 64.
I changed the block size to be 256 so
previously it was just eight eight
characters of context now it is 256
characters of context to predict the
257th
uh I brought down the learning rate a
little bit because the neural net is now
much bigger so I brought down the
learning rate
the embedding Dimension is now 384 and
there are six heads so 384 divide 6
means that every head is 64 dimensional
as it as a standard
and then there are going to be six
layers of that
and the Dropout will be of 0.2 so every
forward backward passed 20 percent of
all of these um
intermediate calculations are disabled
and dropped to zero
and then I already trained this and I
ran it so uh drum roll how well does it
perform
so let me just scroll up here
we get a validation loss of 1.48 which
is actually quite a bit of an
improvement on what we had before which
I think was 2.07 so we went from 2.07
all the way down to 1.48 just by scaling
up this neural nut with the code that we
have and this of course ran for a lot
longer this may be trained for I want to
say about 15 minutes on my a100 GPU so
that's a pretty good GPU and if you
don't have a GPU you're not going to be
able to reproduce this on a CPU this
would be
um I would not run this on the CPU or a
Macbook or something like that you'll
have to break down the number of layers
and the embedding Dimension and so on
but in about 15 minutes we can get this
kind of a result and
um I'm printing
some of the Shakespeare here but what I
did also is I printed 10 000 characters
so a lot more and I wrote them to a file
and so here we see some of the outputs
so it's a lot more recognizable as the
input text file so the input text file
just for reference looked like this
so there's always like someone speaking
in this matter and uh
our predictions now take on that form
except of course they're they're
nonsensical when you actually read them
so
it is every crimpy bee house oh those
preparation we give heed
um you know
Oho sent me you mighty Lord
anyway so you can read through this
um it's nonsensical of course but this
is just a Transformer trained on the
Character level for 1 million characters
that come from Shakespeare so they're
sort of like blabbers on and Shakespeare
like manner but it doesn't of course
make sense at this scale
uh but I think I think still a pretty
good demonstration of what's possible
so now
I think uh that kind of like concludes
the programming section of this video we
basically kind of did a pretty good job
in um of implementing this Transformer
but the picture doesn't exactly match up
to what we've done so what's going on
with all these additional Parts here so
let me finish explaining this
architecture and why it looks so funky
basically what's happening here is what
we implemented here is a decoder only
Transformer so there's no component here
this part is called the encoder and
there's no cross attention block here
our block only has a self-attention and
the feed forward so it is missing this
third in between piece here this piece
does cross attention so we don't have it
and we don't have the encoder we just
have the decoder and the reason we have
a decoder only
is because we are just generating text
and it's unconditioned on anything we're
just we're just blabbering on according
to a given data set
what makes it a decoder is that we are
using the Triangular mask in our
Transformer so it has this Auto
regressive property where we can just go
and sample from it
so the fact that it's using the
Triangular triangular mask to mask out
the attention makes it a decoder and it
can be used for language modeling now
the reason that the original paper had
an encoder decoder architecture is
because it is a machine translation
paper so it is concerned with a
different setting in particular
it expects some tokens that encode say
for example French
and then it is expected to decode the
translation in English
so so you typically these here are
special tokens so you are expected to
read in this and condition on it and
then you start off the generation with a
special token called start so this is a
special new token that you introduce and
always place in the beginning
and then the network is expected to put
neural networks are awesome and then a
special end token to finish a generation
so this part here will be decoded
exactly as we we've done it neural
networks are awesome will be identical
to what we did
but unlike what we did they want to
condition the generation on some
additional information and in that case
this additional information is the
French sentence that they should be
translating
so what they do now
is they bring in the encoder now the
encoder reads this part here so we're
only going to take the part of French
and we're going to create tokens from it
exactly as we've seen in our video and
we're going to put a Transformer on it
but there's going to be no triangular
mask and so all the tokens are allowed
to talk to each other as much as they
want and they're just encoding
whatever's the content of this French
sentence
once they've encoded it
they've they basically come out in the
top here
and then what happens here is in our
decoder which does the language modeling
there's an additional connection here to
the outputs of the encoder
and that is brought in through a cross
attention
so the queries are still generated from
X but now the keys and the values are
coming from the side the keys and the
values are coming from the top
generated by the nodes that came outside
of the encoder
and those tops the keys and the values
there the top of it
feeding on the side into every single
block of the decoder and so that's why
there's an additional cross attention
and really what it's doing is it's
conditioning the decoding not just on
the past of this current decoding but
also on having seen the full fully
encoded French
prompt sort of
and so it's an encoder decoder model
which is why we have those two
Transformers an additional block and so
on so we did not do this because we have
no we have nothing to encode there's no
conditioning we just have a text file
and we just want to imitate it and
that's why we are using a decoder only
Transformer exactly as done in GPT
okay so now I wanted to do a very brief
walkthrough of Nano GPT which you can
find on my GitHub and uh Nano GPT is
basically two files of Interest there's
train.pi and model.pi trained at Pi is
all the boilerplate code for training
the network it is basically all the
stuff that we had here it's the training
Loop
it's just that it's a lot more
complicated because we're saving and
loading checkpoints and pre-trained
weights and we are decaying the learning
rate and compiling the model and using
distributed training across multiple
nodes or gpus so the training that Pi
gets a little bit more hairy complicated
there's more options Etc
but the model.pi should look very very
um similar to what we've done here in
fact the model is is almost identical
so first here we have the causal
self-attention block and all of this
should look very very recognizable to
you we're producing queries Keys values
we're doing Dot products we're masking
applying softmax optionally dropping out
and here we are pooling the values
what is different here is that in our
code
I have separated out the multi-headed
attention into just a single individual
head
and then here I have multiple heads and
I explicitly concatenate them
whereas here all of it is implemented in
a batched manner inside a single causal
self-attention and so we don't just have
a b and a T and A C Dimension we also
end up with a fourth dimension which is
the heads
and so it just gets a lot more sort of
hairy because we have four dimensional
array tensors now but it is equivalent
mathematically so the exact same thing
is happening is what we have it's just
it's a bit more efficient because all
the heads are not treated as a batch
Dimension as well
then we have to multiply perceptron it's
using the gallon nonlinearity which is
defined here except instead of relu and
this is done just because openingi used
it and I want to be able to load their
checkpoints
uh the blocks of the Transformer are
identical the communicate and the
compute phase as we saw
and then the GPT will be identical we
have the position encodings token
encodings the blocks the layer Norm at
the end the final linear layer
and this should look all very
recognizable
and there's a bit more here because I'm
loading checkpoints and stuff like that
I'm separating out the parameters into
building that should be weight decayed
and those that shouldn't
um but the generate function should also
be very very similar so a few details
are different but you should definitely
be able to look at this uh file and be
able to understand a lot of the pieces
now so let's now bring things back to
chat GPT
what would it look like if we wanted to
train chatgpt ourselves and how does it
relate to what we learned today
well to train in chat GPT there are
roughly two stages first is the
pre-training stage and then the fine
tuning stage in the pre-training stage
we are training on a large chunk of
internet and just trying to get a first
decoder only Transformer to Babel text
so it's very very similar to what we've
done ourselves
except we've done like a tiny little
baby pre-training step
and so in our case uh this is how you
print a number of parameters I printed
it and it's about 10 million so this
Transformer that I created here to
create little Shakespeare
um Transformer was about 10 million
parameters our data set is roughly 1
million uh characters so roughly 1
million tokens but you have to remember
that opening uses different vocabulary
they're not on the Character level they
use these um subword chunks of words and
so they have a vocabulary of 50 000
roughly elements and so their sequences
are a bit more condensed
so our data set the Shakespeare data set
would be probably around 300 000 tokens
in the openai vocabulary roughly
so we trained about 10 million parameter
model and roughly 300 000 tokens
now when you go to the gpd3 paper
and you look at the Transformers that
they trained
they trained a number of Transformers of
different sizes but the biggest
Transformer here has 175 billion
parameters so ours is again 10 million
they used this number of layers in the
Transformer This is the End embed
this is the number of heads and this is
the head size
and then this is the batch size so ours
was 65.
and the learning rate is similar now
when they train this Transformer they
trained on 300 billion tokens
so again remember ours is about 300 000
so this is uh about a million fold
increase and this number would not be
even that large by today's standards
you'd be going up uh one trillion and
above
so they are training a significantly
larger model
on a good chunk of the internet and that
is the pre-training stage but otherwise
these hyper parameters should be fairly
recognizable to you and the architecture
is actually like nearly identical to
what we implemented ourselves but of
course it's a massive infrastructure
challenge to train this you're talking
about typically thousands of gpus having
to you know talk to each other to train
models of this size so that's just a
pre-training stage now after you
complete the pre-training stage you
don't get something that responds to
your questions with answers and is not
helpful and Etc you get a document
completer right so it babbles but it
doesn't Babble Shakespeare in Babel's
internet it will create arbitrary news
articles and documents and it will try
to complete documents because that's
what it's trained for it's trying to
complete the sequence so when you give
it a question it would just uh
potentially just give you more questions
it would follow with more questions it
will do whatever it looks like the some
closed document would do in the training
data on the internet and so who knows
you're getting kind of like undefined
Behavior it might basically answer with
two questions with other questions it
might ignore your question it might just
try to complete some news article it's
totally underlined as we say
so the second fine tuning stage is to
actually align it to be an assistant and
this is the second stage
and so this Chachi PT blog post from
open AI talks a little bit about how the
stage is achieved we basically
um
there's roughly three steps to the to
this stage uh so what they do here is
they start to collect training data that
looks specifically like what an
assistant would do so if you have
documents that have the format where the
question is on top and then an answer is
below and they have a large number of
these but probably not on the order of
the internet this is probably on the
order of maybe thousands of examples
and so they they then fine-tuned the
model to basically only focus on
documents that look like that and so
you're starting to slowly align it so
it's going to expect a question at the
top and it's going to expect to complete
the answer
and uh these very very large models are
very sample efficient during their fine
tuning so this actually somehow works
but that's just step one that's just
fine-tuning so then they actually have
more steps where okay the second step is
you let the model respond and then
different Raiders look at the different
responses and rank them for their
preference as to which one is better
than the other they use that to train a
reward model so they can predict
basically using a different network how
much of any candidate response would be
desirable
and then once they have a reward model
they run PPO which is a form of policy
policy gradient um reinforcement
learning optimizer
to fine-tune this sampling policy so
that the answers that the GPT GPT now
generates are expected to score a high
reward according to the reward model
and so basically there's a whole the
lining stage here or fine-tuning stage
it's got multiple steps in between there
as well and it takes the model from
being a document completer to a question
answer and that's like a whole separate
stage a lot of this data is not
available publicly it is internal to
open Ai and it's much harder to
replicate this stage
um and so that's roughly what would give
you a child GPD and Nano GPT focuses on
the pre-training stage okay and that's
everything that I wanted to cover today
so we trained to summarize a decoder
only Transformer following this famous
paper attention is all you need from
2017.
and so that's basically a GPT we trained
it on a tiny Shakespeare and got
sensible results
all of the training code is roughly
200 lines of code I will be releasing
this um code base so also it comes with
all the git log commits along the way as
we built it up
in addition to this code I'm going to
release the notebook of course the
Google collab
and I hope that gave you a sense for how
you can train
um
these models like say gpt3 there will be
architecturally basically identical to
what we have but they are somewhere
between ten thousand and one million
times bigger depending on how you count
and so that's all I have for now we did
not talk about any of the fine tuning
stages that would typically go on top of
this so if you're interested in
something that's not just language
modeling but you actually want to you
know say perform tasks or you want them
to be aligned in a specific way or you
want to detect sentiment or anything
like that basically anytime you don't
want something that's just a document
completer you have to complete further
stages of fine tuning which we did not
cover
uh and that could be simple supervised
fine tuning or it can be something more
fancy like we see in chargept we
actually train a reward model and then
do rounds of PPO to align it with
respect to the reward model
so there's a lot more that can be done
on top of it I think for now we're
starting to get to about two hours Mark
so I'm going to
um kind of finish here
I hope you enjoyed the lecture and uh
yeah go forth and transform see you
later