 Hi everyone. So today we are once again continuing our implementation of make more.
 Now so far we've come up to here multilial perceptrons and our neural net looked like this
 and we were implementing this over the last few lectures. Now I'm sure everyone is very excited to
 go into recurrent neural networks and all of their variants and how they work and the diagrams look
 cool and it's very exciting and interesting and we're going to get a better result. But unfortunately
 I think we have to remain here for one more lecture and the reason for that is we've already
 trained this multilial perceptron right and we are getting pretty good loss and I think we have a
 pretty decent understanding of the architecture and how it works but the line of code here that I
 take an issue with is here, lost up backward. That is we are taking a pytorch autograph and using
 it to calculate all of our gradients along the way and I would like to remove the use of lost up
 backward and I would like us to write our backward pass manually on the level of tensors and I think
 that this is a very useful exercise for the following reasons. I actually have an entire blog post on
 this topic but I like to call backpropagation a leaky abstraction and what I mean by that is
 backpropagation doesn't just make your neural networks just work magically. It's not the case
 that you can just stack up arbitrary LEGO blocks of differentiable functions and just cross your
 fingers and back propagate and everything is great. Things don't just work automatically. It is a
 leaky abstraction in the sense that you can shoot yourself in a foot if you do not understanding
 its internals. It will magically not work or not work optimally and you will need to understand
 how it works under the hood if you're hoping to debug it and if you are hoping to address it in
 your neural nut. So this blog post here from a while ago goes into some of those examples.
 So for example we've already covered them some of them already. For example the flat tails of
 these functions and how you do not want to saturate them too much because your gradients will die.
 The case of dead neurons which I've already covered as well. The case of exploding or vanishing
 gradients in the case of a current neural networks which we are about to cover. And then also you
 will often come across some examples in the wild. This is a snippet that I found in a random code
 based on the internet where they actually have like a very subtle but pretty major bug in their
 implementation. And the bug points at the fact that the author of this code does not actually
 understand back propagation. So what they're trying to do here is they're trying to clip the loss
 at a certain maximum value. But actually what they're trying to do is they're trying to clip
 the gradients to have a maximum value instead of trying to clip the loss at a maximum value.
 And indirectly they're basically causing some of the outliers to be actually ignored. Because
 when you clip a loss of an outlier you are setting its gradient to zero. And so have a look through
 this and read through it. But there's basically a bunch of subtle issues that you're going to
 avoid if you actually know what you're doing. And that's why I don't think it's the case that
 because PyTorch or other frameworks offer autograd it is okay for us to ignore how it works.
 Now we've actually already covered autograd and we wrote micrograd. But micrograd was an autograd
 engine only on the level of individual scalars. So the atoms were single individual numbers.
 And you know I don't think it's enough and I'd like us to basically think about back propagation
 on level of tensors as well. And so in a summary I think it's a good exercise. I think it is very
 very valuable. You're going to become better at debugging neural networks and making sure that
 you understand what you're doing. It is going to make everything fully explicit so you're not going
 to be nervous about what is hidden away from you. And basically in general we're going to emerge
 stronger. And so let's get into it. A bit of a fun historical note here is that today
 writing your backward pass by hand and manually is not recommended and no one does it except for
 the purposes of exercise. But about 10 years ago in deep learning this was fairly standard and in
 fact pervasive. So at the time everyone used to write their backward pass by hand manually including
 myself. And it's just what you would do. So we used to write backward pass by hand and now everyone
 just called lost a backward. We've lost something. I wanted to give you a few examples of this.
 So here's a 2006 paper from Jeff Hinton and Russell Select Enough in science that was influential
 at the time. And this was training some architectures called restricted bulletin machines. And basically
 it's an auto encoder trained here. And this is from roughly 2010. I had a library for training
 restrictable machines. And this was at the time written in MATLAB. So Python was not used for
 deep learning pervasively. It was all MATLAB. And MATLAB was this scientific computing package
 that everyone would use. So we would write MATLAB, which is barely a programming language
 in a big as well. But it had a very convenient tensor class. And was this a computing environment
 and you would run here, it would all run on the CPU of course, but you would have very nice plots
 to go with it and a built in debugger. And it was pretty nice. Now the code in this package in 2010
 that I wrote for fitting researchable machines to a large extent is recognizable. But I wanted
 to show you how you would well, I'm creating the data and the XY batches. I'm initializing the
 neural nut. So it's got weights and biases just like we're used to. And then this is the training
 loop where we actually do the forward pass. And then here, at this time, they didn't even necessarily
 use back propagation to train neural networks. So this in particular implements contrastive
 divergence, which estimates a gradient. And then here we take that gradient and use it for a
 parameter update along lines that we're used to. Yeah, here. But you can see that basically people
 are meddling with these gradients directly and inline and themselves. It wasn't that common to
 use an autograd engine. Here's one more example from a paper of mine from 2014 called the Fragment
 Embeddings. And here what I was doing is I was aligning images and text. And so it's kind of like
 clip if you're familiar with it. But instead of working on the level of entire images and entire
 sentences, it was working on the level of individual objects and little pieces of sentences. And I
 was embedding them and then calculating a very much like a clip like loss. And I dug up the code
 from 2014 of how I implemented this. And it was already in numpy and Python. And here I'm implementing
 the cost function. And it was standards to implement not just the cost, but also the backward pass
 manually. So here I'm calculating the image embeddings, sentence embeddings, the last function
 I calculate this course. This is the last function. And then once I have the last function, I do the
 backward pass right here. So I backward through the loss function and through the neural net. And
 I have had regularization. So everything was done by hand manually. And you're just right
 out the backward pass. And then you would use a gradient checker to make sure that your numerical
 estimate of the gradient agrees with the one you calculated during back propagation. So this was
 very standard for a long time. But today, of course, it is standard to use an autograph engine.
 But it was definitely useful. And I think people sort of understood how these neural networks work
 on a very intuitive level. And so I think it's a good exercise again. And this is where we want to
 be. Okay, so just as a reminder from our previous lecture, this is the Jupyter Notebook that we
 implemented at the time. And we're going to keep everything the same. So we're still going to have
 a two layer multialing perception with a batch normalization layer. So the forward pass will be
 basically identical to this lecture. But here we're going to get rid of lost backward. And instead,
 we're going to write the backward pass manually. Now here's the starter code for this lecture.
 We are becoming a back prop ninja in this notebook. And the first few cells here are identical to
 what we are used to. So we are doing some imports loading the data set and processing the data set.
 None of this changed. Now here, I'm introducing a utility function that we're going to use later
 to compare the gradients. So in particular, we are going to have the gradients that we estimate
 manually ourselves. And we're going to have gradients that PyTorch calculates. And we're going to be
 checking for correctness, assuming of course that PyTorch is correct. Then here, we have the
 initialization that we are quite used to. So we have our embedding table for the characters,
 the first layer, second layer, and a batch normalization in between. And here's where we create all the
 parameters. Now you will note that I changed the initialization a little bit to be small numbers.
 So normally you would set the biases to be all zero. Here I am setting them to be small random
 numbers. And I'm doing this because if your variables are initialized to exactly zero,
 sometimes what can happen is that can mask an incorrect implementation of a gradient.
 Because when everything is zero, it sort of simplifies and gives you a much simpler expression
 of the gradient than you would otherwise get. And so by making it small numbers, I'm trying to
 unmask those potential errors in these calculations. You also notice that I'm using B1 in the first
 layer. I'm using a bias despite batch normalization right afterwards. So this would typically not be
 what you do because we talked about the fact that you don't need a bias. But I'm doing this here
 just for fun, because we're going to have a gradient with respect to it. And we can check that we are
 still calculating it correctly, even though this bias is asparious. So here I'm calculating a single
 batch. And then here I am doing a forward pass. Now you'll notice that the forward pass is significantly
 expanded from what we are used to. Here the forward pass was just here. Now the reason that
 the forward pass is longer is for two reasons. Number one here, we just had an F dot cross entropy.
 But here I am bringing back a explicit implementation, the loss function. And number two, I broke
 up the implementation into manageable trunks. So we have a lot, a lot more intermediate tensors
 along the way in the forward pass. And that's because we are about to go backwards and calculate
 the gradients in this back propagation from the bottom to the top. So we're going to go upwards.
 And just like we have, for example, the lock props tensor in a forward pass, in a backward pass,
 we're going to have a D lock props, which is going to store the derivative of the loss with
 respect to the lock props tensor. And so we're going to be pretending D to every one of these
 tensors and calculating it along the way of this back propagation. So as an example, we have a B
 and raw here, we're going to be calculating a DB and raw. So here I'm telling PyTorch that we
 want to retain the grad of all these intermediate values. Because here in exercise one, we're going
 to calculate the backward pass. So we're going to calculate all these D variables and use the CMP
 function I've introduced above to check our correctness with respect to what PyTorch is telling
 us. This is going to be exercise one, where we sort of back propagate through this entire graph.
 Now, just to give you a very quick preview of what's going to happen in exercise two and below,
 here we have fully broken up the loss and back propagated through it manually in all the little
 atomic pieces that make it up. But here we're going to collapse the loss into a single cross
 entropy call. And instead we're going to analytically derive using math and paper and pencil, the gradient
 of the loss with respect to the logits. And instead of back propagating through all of its little chunks
 one at a time, we're just going to analytically drive what that gradient is. And we're going to
 implement that, which is much more efficient, as we'll see in a bit. Then we're going to do the
 exact same thing for batch normalization. So instead of breaking up batch room into all the little
 tiny components, we're going to use a pen and paper and mathematics and calculus to derive the
 gradient through the batch, batch room layer. So we're going to calculate the backward pass
 through a batch room layer in a much more efficient expression, instead of backward propagating
 through all of its little pieces independently. So it's going to be exercise three. And then
 exercise four, we're going to put it all together. And this is the full code of training this two
 layer MLP. And we're going to basically insert our manual backdrop. We're going to take out
 loss the backward. And you will basically see that you can get all the same results using fully
 your own code. And the only thing we're using from PyTorch is the torch.tensor to make the
 calculations efficient. But otherwise, you will understand fully what it means to forward and
 backward in your land and train it. And I think that'll be awesome. So let's get to it.
 Okay, so I ran all the cells of this notebook all the way up to here. And I'm going to erase this,
 and I'm going to start implementing backward pass, starting with D log problems. So we want to
 understand what should go here to calculate the gradient of the loss with respect to all the
 elements of the log props tensor. Now I'm going to give away the answer here, but I wanted to put
 a quick note here that I think would be most pedagogically useful for you is to actually go into the
 description of this video and find the link to this stupid notebook. You can find it both on
 GitHub, but you can also find Google collab with it. So you don't have to install anything. You'll
 just go to a website on Google collab. And you can try to implement these derivatives or gradients
 yourself. And then if you are not able to come to my video and see me do it. And so work in tandem
 and try to first yourself and then see me give away the answer. And I think that would be most
 valuable to you. And that's how I recommend you go through this lecture. So we are starting here
 with D log props. Now D log props will hold the derivative of the loss with respect to all the
 elements of log props. What is inside log props? The shape of this is 32 by 27. So it's not going to
 surprise you that D log props should also be an array of size 32 by 27. Because we want the derivative
 of the loss with respect to all of its elements. So the sizes of those are always going to be equal.
 Now, how does log props influence the loss? Okay, loss is negative log props indexed with range of N
 and YB and then the mean of that. Now, just as a reminder, YB is just basically an array of all the
 correct indices. So what we're doing here is we're taking the log props array of size 32 by 27.
 Right. And then we are going every single row. And in each row, we are plugging,
 plugging out the index eight and then 14 and 15 and so on. So we're going down the rows. That's
 the iterator range of N. And then we are always plugging out the index at the column specified
 by this tensor YB. So in the zero throw, we are taking the eighth column. In the first row,
 we're taking the 14th column, etc. And so log props at this plucks out all those
 log probabilities of the correct next character in a sequence. So that's what that does. And the
 shape of this or the size of it is of course 32, because our batch size is 32. So these elements
 get plucked out and then their mean and the negative of that becomes loss. So I always like
 to work with simpler examples to understand the numerical form of derivative. What's going on here
 is once we've plucked out these examples, we're taking the mean and then the negative. So the loss
 basically, I can write it this way is the negative of say a plus b plus c. And the mean of those
 three numbers would be say, negative with divide three, that would be how we achieve the mean of
 three numbers a, b, c, although we actually have 32 numbers here. And so what is basically the loss
 by say like dA, right? Well, if we simplify this expression mathematically, this is negative 1
 over 3 of a and negative plus negative 1 over 3 of b plus negative 1 over 3 of c. And so what
 is the loss by dA? It's just negative 1 over 3. And so you can see that if we don't just have a,
 b and c, but we have 32 numbers, then d loss by d, you know, every one of those numbers is going to
 be 1 over n more generally, because n is the size of the batch 32 in this case. So d loss by d
 lockprobs is negative 1 over n in all these places. Now what about the other elements inside
 lockprobs? Because lockprobs is a large array. You see that lockprobs are checked is 32 by 27,
 but only 32 of them participate in the loss calculation. So what's the derivative of all the
 other most of the elements that do not get plucked out here? Well, their loss intuitively is zero,
 sorry, their gradient intuitively is zero. And that's because they did not participate in the loss.
 So most of these numbers inside this tensor does not feed into the loss. And so if we were to
 change these numbers, then the loss doesn't change, which is the equivalent of way of saying that
 the rate of the loss with respect to them is zero, they don't impact it. So here's a way to
 implement this derivative then we start out with torched at zeros of shape 32 by 27. Or let's just
 say, instead of doing this, because we don't want to hard code numbers, let's do torched at zeros
 like lockprobs. So basically, this is going to create an array of zeros exactly in the shape of
 lockprobs. And then we need to set the derivative negative one over n inside exactly these locations.
 So here's what we can do. The lockprobs indexed in the identical way will be just set to negative
 one over zero, divide n, right, just like we derived here. So now let me erase all these reasoning.
 And then this is the candidate derivative for delockprobs. Let's uncomment the first line and
 check that this is correct. Okay, so CMP ran and let's go back to CMP. And you see that what
 is doing is it's calculating if the calculated value by us, which is dt, is exactly equal to t
 dot grad as calculated by pytorch. And then this is making sure that all the elements are exactly
 equal, and then converting this to a single Boolean value, because we don't want to Boolean tensor,
 we just want to Boolean value. And then here, we are making sure that, okay, if they're not exactly
 equal, maybe they are approximately equal because of some floating point issues, but they're very,
 very close. So here we are using torched at all close, which has a little bit of a wiggle available,
 because sometimes you can get very, very close. But if you use a slightly different calculation,
 because of floating point arithmetic, you can get a slightly different result. So this is
 checking if you get an approximately close result. And then here we are checking the maximum,
 basically the value that has the highest difference, and what is the difference,
 and the absolute value difference between those two. And so we are printing whether we have an
 exact equality, an approximate equality, and what is the largest difference. And so here,
 we see that we actually have exact equality. And so therefore, of course, we also have an
 approximate equality. And the maximum difference is exactly zero. So basically, our delog props is
 exactly equal to what pytorch calculated to be log props dot grad in its back propagation.
 So so far, we're doing pretty well. Okay, so let's now continue our back propagation.
 We have that log props depends on props through a log. So all the elements of props are being
 element wise applied log two. Now, if we want deep props, then then remember your micro graph training,
 we have like a log node, it takes in props and creates log props. And deep props will be the
 local derivative of that individual operation log times the derivative loss with respect to its
 output, which in this case is D log props. So what is the local derivative of this operation?
 Well, we are taking log element wise, and we can come here and we can see, well, from all
 files, your friend, that D by DX of log of X is just simply one of our X. So therefore,
 in this case, X is problems. So we have D by DX is one over X, which is one of our props.
 And then this is the local derivative. And then times we want to train it. So this is chain rule
 times the log props. Then let me uncomment this and let me run the cell in place. And we see that
 the derivative of props as we calculated here is exactly correct. And so notice here how this works.
 Props that are props is going to be inverted and then element wise multiplied here. So if your
 props is very, very close to one, that means your network is currently predicting the character
 correctly, then this will become one over one and the log props is just passed through.
 But if your probabilities are incorrectly assigned, so if the correct character here
 is getting a very low probability, then 1.0 dividing by it will boost this and then multiply by the
 log props. So basically, what this line is doing intuitively is it's taking the examples that
 have a very low probability currently assigned and it's boosting their gradient. You can again,
 look at it that way. Next up is count some imp. So we want the river of this. Now, let me just pause
 here and kind of introduce what's happening here in general, because I know it's a little bit confusing.
 We have the logis that come out of the neural net. Here, what I'm doing is I'm finding the
 maximum in each row, and I'm subtracting it for the purpose of numerical stability. And we talked
 about how if you do not do this, you run numerical issues of some of the logits take on two large
 values, because we end up exponentiating them. So this is done just for safety numerically.
 Then here's the exponentiation of all the sort of like logits to create our counts.
 And then we want to take the sum of these counts and normalize so that all of the props sum to one.
 Now here, instead of using one over counts sum, I use raise to the power of negative one.
 Mathematically, they are identical. I just found that there's something wrong with the
 pytorch implementation of the backward pass of division. And it gives like a real result,
 but that doesn't happen for star star negative one. So I'm using this formula instead. But basically,
 all that's happening here is we got the logits, we want to exponentiate all of them, and we want
 to normalize the counts to create our probabilities. It's just that it's happening across multiple
 lines. So now here, we want to first take the derivative, we want to back propagate into counts
 and then into counts as well. So what should be the count sum? Now we actually have to be careful
 here, because we have to scrutinize and be careful with the shapes. So counts that shape,
 and then counts some in that shape are different. So in particular, counts is 32 by 27, but this
 counts some is 32 by one. And so in this multiplication here, we also have an implicit broadcasting
 that pytorch will do, because it needs to take this column tensor of 32 numbers and replicate it
 horizontally 27 times to align these two tensors. So we can do an element twice multiply. So really,
 what this looks like is the following using a toy example again. What we really have here is just
 props is counts times consummive. So it's a equals a times B. But a is three by three, and b is just
 three by one, a column tensor. And so pytorch internally replicated this elements of B, and it did that
 across all the columns. So for example, B one, which is the first element of B, would be replicated
 here across all the columns in this multiplication. And now we're trying to back propagate through
 this operation to count some in. So when we are calculating this derivative, it's important to
 realize that these two, this looks like a single operation, but actually is two operations applied
 sequentially. The first operation that I touched it is it took this column tensor and replicated it
 across all the, across all the columns basically 27 times. So that's the first operation, it's a
 replication. And then the second operation is the multiplication. So let's first background
 through the multiplication. If these two arrays were of the same size, and we just have a and b
 both of them three by three, then how do we, how do we back propagate through a multiplication?
 So if you just have scalars and not tensors, then if you have sequels a times b, then what is the
 root of the of c with respect to b? Well, it's just a. And so that's the local derivative.
 So here in our case, undoing the multiplication and back propagate through just multiplication
 itself, which is element wise, is going to be the local derivative, which in this case is simply
 counts, because counts is the a. So it's the local derivative, and then times, because the
 chain rule deprops. So this here is the derivative or the gradient, but with respect to replicated
 b. But we don't have a replicated b, we just have a single b column. So how do we now back propagate
 through the replication? And intuitively, this b one is the same variable, and it's just reused
 multiple times. And so you can look at it as being equivalent to a case wave encountered in
 micro grad. And so here, I'm just pulling out a random graph we used in micro grad. We had an
 example where a single node has its output feeding into two branches of basically the graph until
 the loss function. And we're talking about how the correct thing to do in the backward pass is,
 we need to sum all the gradients that arrive at any one node. So across these different branches,
 the gradients would sum. So if a note is used multiple times, the gradients for all of its uses,
 sum during back propagation. So here, b one is used multiple times in all these columns. And
 therefore, the right thing to do here is to sum horizontally across all the rows. So we'll just
 sum in dimension one, but we want to retain this dimension so that the, so that counts them in,
 and its gradient are going to be exactly the same shape. So we want to make sure that we keep them
 as true. So we don't lose this dimension. And this will make the count some m be exactly shape 32 by
 one. So revealing this comparison as well and running this, we see that we get an exact match.
 So this derivative is exactly correct. And let me erase this. Now let's also back propagate into
 counts, which is the other variable here to create props. So from props to count some
 in, we just did that, let's go into counts as well. So the counts will be
 the counts are a. So dc by da is just b. So therefore it's count some in. And then times
 chain rule, the props. Now, councilman is three to my one. D props is 32 by 27. So
 those will broadcast fine and will give us decounts. There's no additional summation required here.
 There will be a broadcasting that happens in this multiply here, because councilman needs to be
 replicated again to correctly multiply d props. But that's going to give the correct result.
 So as far as the single operation is concerned, so we back propagate from props to counts,
 but we can't actually check the derivative of counts. I have it much later on. And the reason for that
 is because count some in depends on counts. And so there's a second branch here that we have to finish
 because councilman back propagates into count some and councilman will back propagate into counts.
 And so counts is a node that is being used twice. It's used right here into props and it goes through
 this other branch through councilman. So even though we've calculated the first contribution of it,
 we still have to calculate the second contribution of it later. Okay, so we're continuing with this
 branch. We have the derivative for councilman. Now we want the derivative counts some. So decount
 sum equals what is the local derivative of this operation? So this is basically an element
 wise one over counts some. So count sum raised to the power of negative one is the same as
 one over councilman. If we go to all from alpha, we see that x is negative one d by d by d x of it
 is basically negative x to the negative two. Right one negative one over square is the same as
 negative x to the negative two. So decount sum here will be local derivative is going to be negative
 counts sum to the negative two. That's the local derivative times chain rule, which is
 decount sum in. So that's decount sum. Let's uncomment this and check that I am correct.
 Okay, so we have perfect equality. And there's no sketching us going on here with any shapes
 because these are of the same shape. Okay, next up we want to back propagate through this line.
 We have that count sum is counts that sum along the rows. So I wrote out some help here.
 We have to keep in mind that counts of course is 32 by 27 and count sum is 32 by one. So in
 this back propagation, we need to take this column of the rudis and transform it into a
 array of the roots to the machine array. So what is this operation doing? We're taking
 in some kind of an input like say a three by three matrix A and we are summing up the rows into
 column tensor B, B1 B2 B3. That is basically this. So now we have the derivatives of the loss with
 respect to B, all the elements of B. And now we want to derivative loss with respect to all these
 little a's. So how do the b's depend on the a's is basically what we're after. What is the local
 derivative of this operation? Well, we can see here that B1 only depends on these elements here.
 The derivative of B1 with respect to all of these elements down here is zero. But for these
 elements here like a 1 1 a 1 2 etc, the local derivative is one, right? So dB1 by dA1 1,
 for example is one. So it's one one and one. So when we have the derivative loss with respect to B1,
 the local derivative of B1 with respect to these inputs is zeros here, but it's one on these guys.
 So in the chain rule, we have the local derivative times sort of the derivative of B1. And so because
 the local derivative is one on these three elements, the local derivative of multiplying the derivative
 of B1 will just be the derivative of B1. And so you can look at it as a router. Basically an
 addition is a router of gradient. Whatever gradient comes from above, it just gets routed equally
 to all the elements that participate in that addition. So in this case, the derivative of B1
 will just flow equally to the derivative of a 1 1, a 1 2 and a 1 3. So if we have a derivative
 of all the elements of B in this column tensor, which is d counts some that we've calculated just now,
 we basically see that what that amounts to is all of these are now flowing to all these elements of A,
 and they're doing that horizontally. So basically what we want is we want to take the
 d counts sum of size 32 by 1, and we just want to replicate it 27 times horizontally to create 32
 by 27 array. So there's many ways to implement this operation. You could of course just replicate
 the tensor. But I think maybe one clean one is that d counts is simply torch dot once like
 so just a two dimensional arrays of ones in the shape of counts. So 32 by 27 times d counts sum.
 So this way we're letting the broadcasting here basically implement the replication. You can look
 at it that way. But then we have to also be careful because d counts was all already calculated.
 We calculated earlier here, and that was just the first branch and we're now finishing the second
 branch. So we need to make sure that these gradients add so plus equals. And then here,
 let's comment out the comparison. And let's make sure crossing fingers that we have the correct
 result. So pytorch agrees with us on this gradient as well. Okay, hopefully we're getting a hang of
 this now counts as an element wise exp of norm logits. So now we want d norm logits.
 And because it's an element has operation, everything is very simple. What is the local
 derivative of e to the x? It's famously just e to the x. So this is the local derivative.
 That is the local derivative. Now we already calculated it and it's inside counts. So we
 made as well potentially just reuse counts. That is the local derivative times d counts.
 Funny as that looks, counts times d counts is the derivative on the norm logits. And now let's
 erase this and let's verify and it looks good. So that's norm logits. Okay, so we are here on this
 line now, d norm logits. We have that and we're trying to calculate the logits and the logit
 maxes. So back propagating through this line. Now we have to be careful here because the shapes
 again are not the same. And so there's an implicit broadcasting happening here. So normal
 just has this shape 32 by 27, logits does as well, but logit maxes is only 32 by one. So there's a
 broadcasting here in the minus. Now here I tried to sort of write out a toy example again. We
 basically have that this is our C equals a minus b. And we see that because of the shape, these
 are three by three, but this one is just a column. And so for example, every element of C, we have
 to look at how it came to be. And every element of C is just the corresponding element of a minus
 basically that associated B. So it's very clear now that the derivatives of every one of these
 C's with respect to their inputs are one for the corresponding a and it's a negative one for the
 corresponding B. And so therefore, the derivatives on the C will flow equally to the corresponding
 A's. And then also to the corresponding B's. But then in addition to that, the B's are broadcast.
 So we'll have to do the additional sum just like we did before. And of course, derivatives for B's
 will undergo a minus because the local derivative here is negative one. So the C 32 by D B3 is negative
 one. So let's just implement that. Basically, D logits will be exactly copying the derivative
 on normal digits. So D logits equals D normal logits, and I'll do a dot clone for safety. So
 we're just making a copy. And then we have that D logit maxis will be the negative of D normal
 logits, because of the negative sign. And then we have to be careful because logit maxis is a column.
 And so just like we saw before, because we keep replicating the same elements across all the
 columns, then in the backward pass, because we keep reusing this, these are all just like separate
 branches of use of that one variable. And so therefore, we have to do a sum along one would
 keep them equals true, so that we don't destroy this dimension. And then the logit maxis will be the
 same shape. Now we have to be careful because this D logits is not the final D logits. And that's
 because not only do we get gradient signal into logits through here, but the logit maxis is a
 function of logits. And that's a second branch into logits. So this is not yet our final derivative
 for logits, we will come back later for the second branch. For now, the logit maxis is the final
 derivative. So let me uncomment this CMP here, and let's just run this. And logit maxis hit by torch
 agrees with us. So that was the derivative into through this line. Now, before we move on, I want
 to pause here briefly, and I want to look at these logit maxis and especially their gradients.
 We've talked previously in the previous route lecture that the only reason we're doing this
 is for the numerical stability of the softmax that we are implementing here. And we talked about
 how if you take these logits for any one of these examples, so one row of this logits tensor,
 if you add or subtract any value equally to all the elements, then the value of the probes will be
 unchanged. You're not changing the softmax. The only thing that this is doing is it's making sure
 that X doesn't overflow. And the reason we're using a max is because then we are guaranteed
 that each row of logits, the highest number is zero. And so this will be safe. And so basically what
 that has repercussions. If it is the case that changing logit maxis does not change the props,
 and therefore there's not change the loss, then the gradient on logit maxis should be zero,
 right? Because saying those two things is the same. So indeed, we hope that this is very,
 very small numbers. And you'd be hope this is zero. Now, because of floating point sort of
 wonkiness, there's doesn't come out exactly zero, only in some of the rows it does. But we get
 extremely small values like one in negative nine or 10. And so this is telling us that the values
 of logit maxis are not impacting the loss as they shouldn't. It feels kind of weird to back
 propagate through this branch, honestly, because if you have any implementation of like f dot cross
 entropy and pytorch and you you block together all these elements and you're not doing the back
 propagation piece by piece, then you would probably assume that the derivative through here is exactly
 zero. So you would be sort of skipping this branch because it's only done for numerical stability.
 But it's interesting to see that even if you break up everything into the full atoms and you still
 do the computation as you'd like with respect to numerical stability, the correct thing happens.
 And you still get a very, very small gradients here, basically reflecting the fact that the
 values of these do not matter with respect to the final loss. Okay, so let's now continue
 back propagation through this line here. We've just calculated the logit maxis and now we want
 to back prop into logits through this second branch. Now here, of course, we took logits and we took
 the max along all the rows. And then we looked at its values here. Now, the way this works is that
 in pytorch, this thing here, the max returns both the values and it returns the indices at which
 those values to call the maximum value. Now, in the forward pass, we only used values because
 that's all we needed. But in the backward pass, it's extremely useful to know about where those
 maximum values occurred. And we have the indices at which they occurred. And this will of course
 helps us to help us do the back propagation, because what should the backward pass be here in this case?
 We have the logis tensor, which is 32 by 27. And in each row, we find the maximum value.
 And then that value gets plucked out into logit maxis. And so intuitively, basically,
 the derivative flowing through here then should be one times the local derivative is one for the
 appropriate entry that was plucked out. And then times the global derivative of the logit maxis.
 So really what we're doing here, if you think through it, is we need to take the delogit maxis,
 and we need to scatter it to the correct positions in these logits from where the maximum values came.
 And so I came up with one line of code, sort of that does that. Let me just erase a bunch of stuff
 here. So the line of, you could do it kind of very similar to what we've done here, where we create
 a zeros, and then we populate the correct elements. So we use the indices here, and we would set them
 to be one. But you can also use one hat. So at that one hat, and then I'm taking the logis that max
 over the first dimension that indices. And I'm telling PyTorch that the
 dimension of every one of these tensors should be 27. And so what this is going to do is, okay,
 I apologize, this is crazy. Be guilty that I am sure of this. It's really just an array of
 where the max is came from in each row, and that element is one, and the other elements are zero.
 So it's a one-hat vector in each row, and these indices are now populating a single one in the
 proper place. And then what I'm doing here is I'm multiplying by the logit maxis. And keep in mind
 that this is a column of 32 by one. And so when I'm doing this times the logit maxis, the logit maxis
 will broadcast, and that column will get replicated, and then the element-wise multiply will ensure
 that each of these just gets routed to whichever one of these bits is turned on. And so that's
 another way to implement this kind of an operation. And both of these can be used. I just thought I
 would show an equivalent way to do it. And I'm using plus equals because we already calculated
 the logits here, and this is now the second branch. So let's look at logits and make sure that this
 is correct. And we see that we have exactly the correct answer. Next up, we want to continue with
 logits here. That is an outcome of a matrix multiplication and a bias offset in this linear
 layer. So I've printed out the shapes of all these intermediate tensors. We see that logits is of
 course 32 by 27, as we've just seen. Then the age here is 32 by 64. So these are 64-dimensional
 hidden states. And then this w matrix projects those 64-dimensional vectors into 27 dimensions.
 And then there's a 27-dimensional offset, which is a one-dimensional vector. Now we should note
 that this plus here actually broadcasts because h multiplied by w2 will give us a 32 by 27.
 And so then this plus b2 is a 27-dimensional vector here. Now in the rules of broadcasting,
 what's going to happen with this bias vector is that this one-dimensional vector of 27 will get
 aligned with an padded dimension of 1 on the left. And it will basically become a row vector,
 and then it will get replicated vertically 32 times to make it 32 by 27. And then there's
 an element bias multiply. Now the question is how do we back propagate from logits to the hidden
 states, the weight matrix of w2 and the bias b2. And you might think that we need to go to
 some matrix calculus. And then we have to look up the derivative for a matrix multiplication.
 But actually you don't have to do any of that. And you can go back to first principles and derive
 this yourself on a piece of paper. And specifically what I like to do and what I find works well for
 me is you find a specific small example that you then fully write out. And then in the process of
 analyzing how that individual small example works, you will understand a broader pattern.
 And you'll be able to generalize and write out the full general formula for how these
 derivatives flow in an expression like this. So let's try that out. So part in the low budget
 production here, but what I've done here is I'm writing it out on a piece of paper. Really what
 we are interested in is we have a multiply b plus c. And that creates a d. And we have the
 derivative of the loss with respect to d. And we'd like to know what the derivative of the loss is
 with respect to a b and c. Now these here are a little two dimensional examples of matrix
 multiplication. Two by two times a two by two, plus a two, a vector of just two elements,
 c one and c two, gives me a two by two. Now notice here that I have a bias vector here called c.
 And the bias vector c one and c two. But as I described over here, that bias vector will become
 a row vector in the broadcasting and will replicate vertically. So that's what's happening here as
 well. C one c two is replicated vertically. And we see how we have two rows of C one C two as a result.
 So now when I say write it out, I just mean like this, basically break up this matrix multiplication
 into the actual thing that that's going on under the hood. So as a result of matrix multiplication
 and how it works, d one one is the result of a dot product between the first row of a and the first
 column of B. So a one one b one one plus a one two b two one plus c one. And so on so forth for all
 the other elements of D. And once you actually write it out, it becomes obvious this is just a
 bunch of multiplies and ads. And we know from micro grad how to differentiate multiplies and ads.
 And so this is not scary anymore. It's not just matrix multiplication. It's just tedious
 unfortunately. But this is completely tractable. We have d l by D for all these. And we want d l by
 all these little other variables. So how do we achieve that? And how do we actually get the
 gradients? Okay, so the low budget production continues here. So let's for example derive the
 derivative of the loss with respect to a one one. We see here that a one one occurs twice in our
 simple expression right here, right here. And influence is D one one and D one two. So this is
 so what is D L by D a one one? Well, it's D L by D one one times the local derivative of D one one,
 which in this case is just B one one, because that's what's multiplying a one one here. So,
 and likewise here the local derivative of D one two with respect to a one one is just B one two.
 And so B one two will in the chain rule therefore multiply D L by D one two. And then because a one
 one is used both to produce D one one and D one two, we need to add up the contributions of both
 of those sort of chains that are running in parallel. And that's why we get a plus just adding up those
 two, those two contributions. And that gives us D L by D a one one. We can do the exact same
 analysis for the other one for all the other elements of a. And when you simply write it out,
 it's just super simple taking ingredients on, you know, expressions like this. You find that
 this matrix D L by D a that we're after, right, if we just arrange all of them in the same shape as
 A takes. So A is just two or two matrix. So D L by D a here will be also just the same shape
 tester with the derivatives now. So D L by D a one one, etc. And we see that actually we can
 express what we've written out here as a matrix multiply. And so it just so happens that D L by,
 that all of these formulas that we've derived here by taking gradients can actually be expressed
 as a matrix multiplication. And in particular, we see that it is the matrix multiplication of these
 two matrices. So it is the D L by D. And then matrix multiplying B, but B transpose actually.
 So you see that B two one and B one two have changed place. Whereas before we had, of course,
 B one one B one two, B two one B two two. So you see that this other matrix B is transposed.
 And so basically what we have on story short, just by doing very simple reasoning here,
 by breaking up the expression in the case of a very simple example, is that D L by D a is,
 which is this, is simply equal to D L by D D matrix multiplied with B transpose.
 So that is what we have so far. Now we also want the derivative with respect to B and C.
 Now, for B, I'm not actually doing the full derivation because honestly it's, it's not deep,
 it's just annoying, it's exhausting. You can actually do this analysis yourself.
 You'll also find that if you take this, these expressions and you differentiate with respect
 to B instead of A, you will find that D L by D B is also a matrix multiplication. In this case,
 you have to take the matrix A and transpose it. And matrix multiply that with D L by D D.
 And that's what gives you a deal by D B. And then here for the offsets C one and C two,
 if you again just differentiate with respect to C one, you will find an expression like this.
 And C two and expression like this. And basically you'll find that D L by D C is simply because
 they're just offsetting these expressions. You just have to take the deal by D D matrix
 of the derivatives of D and you just have to sum across the columns. And that gives you the
 derivatives for C. So long story short, the backward pass of a matrix multiply is a matrix multiply.
 And instead of just like we had D equals A times B plus C, in a scalar case, we sort of like
 arrive at something very, very similar, but now with a matrix multiplication instead of a scalar
 multiplication. So the derivative of D with respect to A is D L by D D matrix multiply B
 Trespos. And here it's a transpose multiply D L by D D. But in both cases, matrix multiplication
 with the derivative and the other term in the multiplication. And for C it is a sum.
 Now I'll tell you a secret. I can never remember the formulas that we just arrived for back
 propagate information multiplication. And I can back propagate through these expressions just fine.
 And the reason this works is because the dimensions have to work out. So let me give you an example.
 Say I want to create D H. Then what should D H be number one? I have to know that the shape of D
 H must be the same as the shape of H. And the shape of H is 30 to by 64. And then the other piece of
 information I know is that D H must be some kind of matrix multiplication of D logits with W two.
 And D logits is 32 by 27. And W two is 64 by 27. There is only a single way to make the shape or
 count in this case. And it is indeed the correct result. In particular here, H needs to be 32 by 64.
 The only way to achieve that is to take a D logits and matrix multiply it with you see how I have
 to take W two, but I have to transpose it to make the dimensions work out. So W to transpose.
 And it's the only way to make these two matrix multiply those two pieces to make the shapes
 work out. And that turns out to be the correct formula. So if we come here, we want D H, which is
 D A. And we see that D A is DL by D D matrix multiply B transpose. So that's D logits multiply
 and B is W two. So W two transpose, which is exactly what we have here. So there's no need to
 remember these formulas. Similarly, now if I want D W two, well, I know that it must be a matrix
 multiplication of D logits and H. And maybe there's a few transpose, or there's one transpose in
 there as well. And I don't know which way it is. So I have to come to W two. And I see that it's
 shape is 64 by 27. And that has to come from some matrix multiplication of these two. And so to get
 a 64 by 27, I need to take H, I need to transpose it. And then I need to matrix multiply it. So that
 will become 64 by 32. And then I need to make your small bind with the 32 by 27. And that's going to
 give me a 64 by 27. So I need to make your small, apply this with the logis that shape, just like
 that. That's the only way to make the dimensions work out. And just use matrix multiplication. And
 if we come here, we see that that's exactly what's here. So a transpose, a for us is H,
 multiply with the logis. So that's W two. And then DB two is just the vertical sum. And actually,
 in the same way, there's only one way to make the shapes work out. I don't have to remember that
 it's a vertical sum along the zero of axis, because that's the only way that this makes sense,
 because B two shape is 27. So in order to get a D logits here, it's 32 by 27. So knowing that
 it's just some over D logits in some direction, that direction must be zero, because I need to
 eliminate this dimension. So it's this. So this is, so this kind of like the hacky way. Let me
 copy paste and delete that and let me swing over here. And this is our backward pass for the linear
 layer, hopefully. So now let's uncomment these three. And we're checking that we got all the three
 derivatives correct, and run. And we see that H, W two and B two are all exactly correct. So we
 back propagated through a linear layer. Now next up, we have derivative for the H already,
 and we need to back propagate through 10 H into H preact. So we want to derive D H preact.
 And here we have to back propagate through a 10 H, and we've already done this in micrograds.
 And we remember that 10 H is a very simple backward formula. Now, unfortunately, if I just
 put in D by DX of 10 H of X into Wolfram alpha, it lets us down. It tells us that it's a hyperbolic
 secant function squared of X. It's not exactly helpful. But luckily, Google image search does not
 let us down. And it gives us the simpler formula. And in particular, if you have that A is equal to
 10 H of Z, then D A by D Z, back propagating through 10 H, is just one minus a square. And take note
 that one minus a square A here is the output of the 10 H, not the input to the 10 H Z. So the D A
 by D Z is here formulated in terms of the output of that 10 H. And here also in Google image search,
 we have the full derivation. If you want to actually take the actual definition of 10 H and
 work through the math to figure out one minus 10 square of Z. So one minus a square is the local
 derivative. In our case, that is one minus the output of 10 H square, which here is H. So it's
 H square. And that is the local derivative. And then times the chain rule, D H. So that is going to be
 our candidate implementation. So if we come here, and then uncomment this, let's hope for the best.
 And we have the right answer. Okay, next up, we have D H P react. And we want to back propagate
 into the gain, the B and raw and the B and bias. So here, this is the bashroom parameters, B and
 gain and bias inside the bashroom that take the B and raw, that is exact unit Gaussian,
 and they scale it and shift it. And these are the parameters of the bashroom. Now, here, we have a
 multiplication, but it's worth noting that this multiply is very, very different from this matrix
 multiply here. Matrix multiply our dot products between rows and columns of these matrices involved.
 This is an element twice multiply. So things are quite a bit simpler. Now, we do have to be careful
 with some of the broadcasting happening in this line of code, though. So you see how B and gain
 and B and bias are one by 64. But H preact and B and raw are 32 by 64. So we have to be careful
 with that and make sure that all the shapes work out fine. And that the broadcasting is correctly
 back propagated. So in particular, let's start with D B and gain. So D B and gain should be.
 And here, this is again, element twice multiply. And whenever we have a times B equals C,
 we saw that the local derivative here is just if this is a local derivative is just the B, the
 other. So the local derivative is just B and raw. And then times chain rule. So D H preact.
 So this is the candidate gradient. Now again, we have to be careful because B and gain is of
 size one by 64. But this here would be 32 by 64. And so the correcting to do in this case, of course,
 is that B and gain here is a rule vector of 64 numbers. It gets replicated vertically in this
 operation. And so therefore the correcting to do is to sum because it's being replicated.
 And therefore, all the gradients in each of the rows that are now flowing backwards need to sum
 up to that same tensor D B and gain. So if to sum across all the zero, all the examples, basically,
 which is the direction which just gets replicated. And now we have to be also careful because we
 be in game is of shape one by 64. So in fact, I need to keep them as true. Otherwise, I will
 just get 64. Now I don't actually really remember why the B and gain and the B and bias. I made them
 be one by 64. But the biases be one and B two, I just made them be one dimensional vectors.
 They're not two dimensional tensors. So I can't recall exactly why I left the gain and the bias
 as two dimensional. But it doesn't really matter as long as you are consistent and you're keeping
 it the same. So in this case, we want to keep the dimension so that the tensor shapes work.
 Next up, we have B and raw. So D B and raw will be B and gain multiplying D H preact. That's our
 chain rule. Now what about the dimensions of this? We have to be careful, right? So D H preact is
 32 by 64. B and gain is one by 64. So we'll just get replicated and to create this multiplication,
 which is the correct thing, because in a forward pass, it also gets replicated in just the same way.
 So in fact, we don't need the brackets here, we're done. And the shapes are already correct.
 And finally, for the bias, very similar, this bias here is very, very similar to the bias we saw in
 the linear layer. And we see that the gradients from H preact will simply flow into the biases
 and add up, because these are just these are just offsets. And so basically, we want this to be D
 H preact, but it needs to sum along the right dimension. And in this case, similar to the gain,
 we need to sum across the zero dimension, the examples, because of the way that the bias gets
 replicated very quickly. And we also want to have keep them as true. And so this will basically take
 this and sum it up and give us a one by 64. So this is the candidate implementation, it makes all
 the shapes work. Let me bring it up down here. And then let me uncomment these three lines
 to check that we are getting the correct result for all the three tensors. And indeed, we see that
 all of that got back propagated correctly. So now we get to the batch norm layer. We see how here
 being gained and being biased are the parameters. So the back propagation ends. But being raw now
 is the output of the standardization. So here, what I'm doing, of course, is I'm breaking up the
 batch norm into manageable pieces. So we can back propagate through each line individually. But
 basically what's happening is B and mean I is the sum. So this is the B and mean I, I apologize for
 the variable naming. B and diff is X minus mu. B and diff two is X minus mu squared here inside
 the variance. B and var is the variance. So sigma squared, this is B and var. And it's basically the
 sum of squares. So this is the X minus mu squared, and then the sum. Now you'll notice one departure
 here. Here it is normalized as one over M, which is the number of examples. Here I'm normalizing as
 one over N minus one instead of M. And this is deliberate. I'll come back to that in a bit when
 we are at this line. It is something called the bestial correction. But this is how I want it in our case.
 B and var in then becomes basically B and var plus epsilon. Epsilon is one negative five. And then
 it's one over square root is the same as raising to the power of negative point five, right? Because
 point five is squared. And then negative makes it one over square root. So B and var in is one over
 this denominator here. And then we can see that B and raw, which is the X hat here, is equal to
 the B and diff, the numerator, multiplied by the B and var in. And this line here that creates
 pre H pre act was the last piece we've already back propagated through it. So now what we want to
 do is we are here and we have B and raw, and we have to first back propagate into B and diff and B
 and var in. So now we're here and we have DB and raw. And we need to back propagate through this line.
 Now I've written out the shapes here and indeed B and var in is a shape one by 64. So there is a
 broadcasting happening here that we have to be careful with. But it is just an element-wise simple
 multiplication. By now we should be pretty comfortable with that to get DB and diff. We know that this
 is just B and var in multiplied with DB and raw. And conversely, to get DB and var in, we need to
 take B and diff and multiply that by DB and raw. So this is the candidate. But of course, we need
 to make sure that broadcasting is obeyed. So in particular, B and var in multiplying with DB and
 raw will be okay and give us 32 by 64 as we expect. But DB and var in would be taking a 32 by 64
 multiplying it by 32 by 64. So this is a 32 by 64. But of course, DB, this B and var in is only
 one by 64. So the second line here needs a sum across the examples. And because there's this
 dimension here, we need to make sure that keep them is true. So this is the candidate.
 Let's erase this and let's swing down here and implement it. And then let's comment out
 DB and var in and DB and diff. Now, we'll actually notice that DB and diff, by the way, is going to
 be incorrect. So when I run this, B and var in this correct, B and diff is not correct.
 And this is actually expected because we're not done with B and diff. So in particular,
 when we slide here, we see here that B and raw is a function of B and diff. But actually,
 B and var is a function of B and var, which is a function of B and diff to, which is a function
 of B and diff. So it comes here. So B, D and diff, these variable names are crazy. I'm sorry.
 It branches out into two branches, and we've only done one branch of it. We have to continue
 our back propagation and eventually come back to be in diff. And then we'll be able to do a plus
 equals and get the actual current gradient. For now, it is good to verify that CBMP also works.
 It doesn't just lie to us and tell us that everything is always correct. It can in fact
 detect when your gradient is not correct. So it's that's good to see as well. Okay. So now we have
 the derivative here, and we're trying to back propagate through this line. And because we're
 raising to a power of negative point five, I brought up the power rule. And we see that basically,
 we have that the B and var will now be we bring down the exponent. So negative point five times
 X, which is this. And now race to the power of negative point five minus one, which is a negative
 one point five. Now, we would have to also apply a small chain rule here in our head, because we
 need to take further derivative of B and var with respect to this expression here inside the
 bracket. But because it's an element wise operation, and everything is fairly simple,
 that's just one. And so there's nothing to do there. So this is the local derivative. And then
 times the global derivative to create the chain rule. This is just times the B and var.
 So this is our candidate. Let me bring this down. And uncommon to the check.
 And we see that we have the correct result. Now, before we got propagate through the next line,
 I want to briefly talk about the note here, where I'm using the bestness correction,
 dividing by n minus one, instead of dividing by n, when I normalize here, the sum of squares.
 Now, you'll notice that this is the departure from the paper, which uses one over n instead,
 not one over n minus one. There m is rn. And so it turns out that there are two ways of estimating
 variance of an array. One is the biased estimate, which is one over n. And the other one is the
 unbiased estimate, which is one over n minus one. Now, confusingly, in the paper, this is
 not very clearly described. And also, it's a detail that kind of matters, I think.
 They are using the biased version of train time. But later, when they are talking about the inference,
 they are mentioning that when they do the inference, they are using the unbiased estimate,
 which is the n minus one version, in basically for inference, and to calibrate the running mean
 and running variance, basically. And so they actually introduce a train test mismatch,
 where in training, they use the biased version. And in the test time, they use the unbiased version.
 I find this extremely confusing. You can read more about the Bessel's correction, and why
 dividing by n minus one gives you a better estimate of the variance. In a case where you have population
 size, or samples for a population, they are very small. And that is indeed the case for us,
 because we are dealing with mini batches. And these mini matches are a small sample of a larger
 population, which is the entire training set. And so it just turns out that if you just estimate
 it using one over n, that actually almost always underestimates the variance. And it is a biased
 estimator, and it is advised that you use the unbiased version and divide by n minus one. And
 you can go through this article here that I liked, that actually describes the full reasoning,
 and I'll link it in the video description. Now, when you calculate the torso variance,
 you'll notice that they take the unbiased flag, whether or not you want to divide by n,
 or n minus one. Confusingly, they do not mention what the default is for unbiased. But I believe
 unbiased by default is true. I'm not sure why the docs here don't cite that. Now, in the Bessel
 Room, 1D, the documentation again is kind of wrong and confusing. It says that the standard
 deviation is calculated via the biased estimator. But this is actually not exactly right. And people
 have pointed out that it is not right in a number of issues since then. Because actually,
 the rabbit hole is deeper, and they follow the paper exactly. And they use the biased version
 for training. But when they're estimating the running standard deviation, we are using the unbiased
 version. So again, there's the train test mismatch. So long story short, I'm not a fan of train
 test discrepancies. I basically kind of consider the fact that we use the biased version, the
 training time, and the unbiased test time, I basically consider this to be a bug. And I don't
 think that there's a good reason for that. It's not really, they don't really go into the detail
 of the reasoning behind it in this paper. So that's why I basically prefer to use the Bessel's
 correction in my own work. Unfortunately, Bessel Room does not take a keyword argument that tells
 you whether or not you want to use the unbiased version or the biased version in both training
 tests. And so therefore anyone using Bessel Roomization, basically in my view has a bit of a bug in the
 code. And this turns out to be much less of a problem if your batch, many batch sizes are a bit
 larger. But still, I just might have kind of a unpodable. So maybe someone can explain why this is okay.
 But for now, I prefer to use the unbiased version consistently both during training and at test time.
 And that's why I'm using one over n minus one here. Okay, so let's now actually back propagate
 through this line. So the first thing that I always like to do is I like to scrutinize the
 shapes first. So in particular here, looking at the shapes of what's involved, I see that B and
 var shape is one by 64. So it's a row vector and B and if two dot shape is 32 by 64. So clearly
 here we're doing a sum over the zero axis to squash the first dimension of of the shapes here,
 using a sum. So that right away actually hints to me that there will be some kind of a replication
 or broadcasting in the backward pass. And maybe you're noticing the pattern here, but basically,
 anytime you have a sum in the forward pass, that turns into a replication or broadcasting in the
 backward pass along the same dimension. And conversely, when we have a replication or a broadcasting in
 the forward pass, that indicates a variable reuse. And so in the backward pass, that turns into a
 sum over the exact same dimension. And so hopefully you're noticing that duality that those two are
 kind of like the opposite of each other in the forward and backward pass. Now, once we understand
 the shapes, the next thing I like to do always is I like to look at a two example in my head to
 sort of just like understand roughly how the variable dependencies go in the mathematical formula.
 So here we have a two dimensional array at the end of two, which we are scaling by a constant.
 And then we are summing vertically over the columns. So if we have a two by two matrix A,
 and then we sum over the columns and scale, we would get a row vector b1 b2. And b1 depends on a
 in this way, whereas just some, they're scaled of a and b2 in this way, where it's the second column,
 sum and scale. And so looking at this basically, what we want to do now is we have the derivatives
 on b1 and b2, and we want to back propagate them into a's. And so it's clear that just
 differentiating in your head, the local derivative here is 1 over n minus 1 times 1
 for each one of these a's. And basically the derivative of b1 has to flow through the columns of a
 scaled by 1 over n minus 1. And that's roughly what's happening here. So intuitively, the derivative
 flow tells us that d bn df2 will be the local derivative of this operation. And there are many
 ways to do this by the way, but I like to do something like this, torch dot one's like of bn
 df2. So I'll create a large array to the measure of ones. And then I will scale it. So 1.0 divided by
 n minus 1. So this is a array of 1 over n minus 1. And that's sort of like the local derivative.
 And now for the chain rule, I will simply just multiply it by d bn bar.
 And notice here what's going to happen. This is 32 by 64. And this is just 1 by 64. So I'm letting
 the broadcasting do the replication, because internally in pytorch, basically d bn bar,
 which is 1 by 64 row vector, well, in this multiplication, get copied vertically until
 the two are of the same shape. And then there will be an element wise multiply. And so that
 so that the broadcasting is basically doing the replication. And I will end up with the derivatives
 of d bn df2 here. So this is the kentexolution. Let's bring it down here. Let's uncomment this line
 where we check it. And let's hope for the best. And indeed, we see that this is the correct formula.
 Next up, let's differentiate here into b and df. So here we have that b and df is element wise
 squared to create b and df2. So this is a relatively simple derivative, because it's a simple
 element wise operation. So it's kind of like the scalar case. And we have that d b and df
 should be, if this is x squared, then derivative of it is 2x. Right? So it's simply two times
 b and df. That's the local derivative. And then times chain rule. And the shape of these is the
 same, they are of the same shape. So times this. So that's the backward pass for this variable.
 Let me bring that down here. And now we have to be careful, because we already calculated db and
 df, right? So this is just the end of the other, you know, other branch coming back to b and df.
 Because b and df were already back propagated to way over here from b and raw. So we now completed
 the second branch. And so that's why I have to do plus equals. And if you recall, we had an
 incorrect derivative for b and df before. And I'm hoping that once we append this last missing
 piece, we have the exact correctness. So let's run and b and df2, b and df now actually shows
 the exact correct derivative. So that's comforting. Okay, so let's now back propagate through
 this line here. The first thing we do, of course, is we check the shapes. And I wrote them out here.
 And basically the shape of this is 32 by 64. HPBN is the same shape. But b and me and I is a
 row vector one by 64. So this minus here will actually do broadcasting. And so we have to be
 careful with that. And as a hint to us, again, because of the duality, a broadcasting in a forward
 pass means variable reuse. And therefore, there will be a sum in the backward pass.
 So let's write out the backward pass here now. Back propagate into the HPBN. Because these are
 the same shape, then the local derivative for each one of the elements here is just one for the
 corresponding element in here. So basically, what this means is that the gradient just simply copies
 is just a variable assignment, its quality. So I'm just going to clone this tensor, just for safety
 to create an exact copy of DB and diff. And then here to back propagate into this one,
 what I'm inclined to do here is DB and me and I will basically be what is the local derivative?
 Well, it's negative torch dot one, like of the shape of B and diff, right? And then times the
 derivative here, DB and diff. And this here is the back propagation for the replicated B and
 mean I. So I still have to back propagate through the replication in the broadcasting,
 and I do that by doing a sum. So I'm going to take this whole thing, and I'm going to do a sum
 over the zero dimension, which was the replication. So if you scrutinize this, by the way, you'll
 notice that this is the same shape as that. And so what I'm doing, what I'm doing here,
 doesn't actually make that much sense because it's just a array of one, it's multiplying the
 B and diff. So in fact, I can just do this. And there's a equivalent. So this is the candidate
 backward pass, let me copy it here. And then let me comment out this one and this one.
 Enter. And it's wrong. Damn. Actually, sorry, this is supposed to be wrong. And it's supposed to be
 wrong because we are back propagating from a B and diff into H preb and, but we're not done,
 because B and mean I depends on H preb and there will be a second portion of that derivative
 coming from this second branch. So we're not done yet, and we expect it to be incorrect. So
 there you go. So let's not back propagate from B and mean I into H preb and. And so here again,
 we have to be careful because there's a broadcasting along, or there's a sum along the zero dimension.
 So this will turn into broadcasting in the backward pass now. And I'm going to go a little bit faster
 on this line because it is very similar to the line that we had before, and multiplies in the
 past. In fact, so the H preb and will be the gradient will be scaled by one over N. And then
 basically this gradient here, DBM, mean, I is going to be scaled by one over N. And then it's going to
 flow across all the columns and deposit itself into D H preb and. So what we want is this thing
 scaled by one over N. Let me put the constant up front here. So scaled on the gradient, and now
 we need to replicate it across all the across all the rows here. So we I like to do that by
 torch dot ones like off basically H preb and. And I will let broadcasting do the work of
 replication. So like that. So this is the H preb and hopefully we can plus equals that.
 So this here is broadcasting. And then this is the scaling. So this should be correct.
 Okay, so that completes the back propagation of the bathroom layer. And we are now here.
 Let's back propagate through the linear layer one here. Now, because everything is getting a
 little vertically crazy, I copy pasted the line here. And let's just back propagate through this
 one line. So first, of course, we inspect the shapes and we see that this is 3,2, by 64.
 Emcat is 32 by 30. W one is 30 30 by 64. And B one is just 64. So as I mentioned,
 back propagating through linear layers is fairly easy just by matching the shapes. So let's do that.
 We have that D empat. Should be some matrix multiplication of D H preb and with W one and one
 transpose thrown in there. So to make empat be 32 by 30, I need to take D H preb and
 32 by 64 and multiply it by W one that transpose.
 To get D only one, I need to end up with 30 by 64. So to get that, I need to take
 empat transpose and multiply that by D H preb and finally to get D B one.
 This is a addition and we saw that basically I need to just sum the elements in D H preb
 and along some dimension. And to make the dimensions work out, I need to sum along the zero access here
 to eliminate this dimension. And we do not keep them. So that we want to just get a single one
 dimensional lecture of 64. So these are the claimed derivatives. Let me put that here and
 let me uncommon three lines and cross our fingers. Everything is great. Okay, so we now continue
 almost there. We have the derivative of emcat and we want to derivative, we want to back propagate
 into em. So I again copied this line over here. So this is the forward pass and then this is the
 shapes. So remember that the shape here was 32 by 30 and the original shape of em was 32 by 3 by 10.
 So this layer in the forward pass, as you recall, did the concatenation of these three 10 dimensional
 character vectors. And so now we just want to undo that. So this is actually relatively straight
 forward operation, because the backward pass of the, what is the view view is just a repress
 representation of the array. It's just a logical form of how you interpret the array. So let's
 just reinterpret it to be what it was before. So in other words, the em is not 32 by 30.
 It is basically the emcat. But if you view it as the original shape, so just m dot shape.
 You can pass intubples into view. And so this should just be, okay,
 we just rerepresent that view. And then we uncomment this line here and hopefully,
 yeah, so the derivative of em is correct. So in this case, we just have to rerepresent
 the shape of those derivatives into the original view. So now we are at the final line. And the
 only thing that's left to back propagate through is this indexing operation here, m is c at xb.
 So as I did before, I copy pasted this line here. And let's look at the shapes of everything
 that's involved and remind ourselves how this worked. So m dot shape was 32 by three by 10.
 So it's 32 examples. And then we have three characters. Each one of them has a 10 dimensional
 embedding. And this was achieved by taking the lookup table C, which have 27 possible characters,
 each of them 10 dimensional. And we looked up at the rows that were specified inside this
 tensor xb. So xb is 32 by three. And it's basically giving us for each example, the identity or the
 index of which character is part of that example. And so here I'm showing the first five rows of
 of this tensor xb. And so we can see that, for example, here it was the first example in this batch,
 is that the first character in the first character and the fourth character comes into the neural
 net. And then we want to predict the next character in a sequence after the character is 114.
 So basically what's happening here is there are integers inside xb. And each one of these
 integers is specifying which row of C we want to pluck out. Right. And then we arrange those
 rows that we've plucked out into three, two by three by 10 tensor, and we just package them
 in, we just package them into this tensor. And now what's happening is that we have d amp.
 So for every one of these basically plucked out rows, we have their gradients now. But they're
 arranged inside this 32 by three by 10 tensor. So all we have to do now is we just need to route
 this gradient backwards through this assignment. So we need to find which row of C that every one
 of these 10 dimensional embeddings come from. And then we need to deposit them into DC.
 So we just need to undo the indexing. And of course, if any of these rows of C was used multiple
 times, which almost certainly is the case, like the row one and one was used multiple times,
 then we have to remember that the gradients that arrive there have to add. So for each occurrence,
 we have to have an addition. So let's now write this out. And I don't actually know of like a
 much better way to do this than a for loop, unfortunately, in Python. So maybe someone can come up with
 a vectorized efficient operation. But for now, let's just use for loops. So let me create a torch
 dot zeros like C to initialize just 27 by 10 tensor of all zeros. And then honestly,
 4k in range, XB dot shape at zero. Maybe someone has a better way to do this. But for J in range,
 XB dot shape at one. This is going to iterate over all the, all the elements of XB, all these
 integers. And then let's get the index at this position. So the index is basically XB at kj.
 So that an example of that is 11 or 14 and so on. And now in a forward pass, we took,
 we basically took the row of C at index, and we deposited it into M at k a j. That's what happened.
 That's where they are packaged. So now we need to go backwards, and we just need to route
 DM at the position kj. We now have these derivatives for each position and it's 10
 dimensional. And you just need to go into the correct row of C. So DC rather at IX is this,
 but plus equals, because there could be multiple occurrences, like the same row could have been
 used many, many times. And so all of those derivatives will just go backwards through the indexing,
 and they will add. So this is my candidate solution. Let's copy it here.
 Let's uncomment this and cross our fingers. Hey, so that's it. We've back propagated through
 this entire beast. So there we go. Totally makes sense. So now we come to exercise two.
 It basically turns out that in this first exercise, we were doing way too much work.
 We were back propagating way too much. And it was all good practice and so on,
 but it's not what you would do in practice. And the reason for that is, for example,
 here I separated out this loss calculation over multiple lines, and I broke it up all,
 all two, like its smallest atomic pieces, and we back propagated through all of those individually.
 But it turns out that if you just look at the mathematical expression for the loss,
 then actually you can do the differentiation on pen and paper, and a lot of terms cancel and
 simplify. And the mathematical expression you end up with can be significantly shorter and
 easier to implement than back propagating through all the pieces of everything you've done.
 So before we had this complicated forward pass, going from logits to the loss,
 but in PyTorch, everything can just be glued together into a single call f dot cross entropy.
 You just pass in logits and the labels, and you get the exact same loss, as I verify here.
 So our previous loss and the fast loss coming from the chunk of operations as a single mathematical
 expression is the same, but it's much, much faster and forward pass. It's also much, much faster
 and backward pass. And the reason for that is if you just look at the mathematical form of this
 and differentiate again, you will end up with a very small and short expression. So that's what
 we want to do here. We want to in a single operation or in a single go, or like very quickly, go directly
 to delogits. And we need to implement delogits as a function of logits and YBs. But it will be
 significantly shorter than whatever we did here, where to get to delogits, we have to go all the way
 here. So all of this work can be skipped in a much, much simpler mathematical expression
 that you can implement here. So you can give it a shot yourself, basically look at what exactly
 is the mathematical expression of loss and differentiate with respect to the logits.
 So let me show you a hint. You can of course try it fully yourself. But if not, I can give you some
 hint of how to get started mathematically. So basically what's happening here is we have
 logits, then there's the softmax that takes the logits and gives you probabilities. Then we are
 using the identity of the correct next character to pluck out a row of probabilities, take the
 negative log of it to get our negative log probability. And then we average up all the
 log probabilities or negative log probabilities to get our loss. So basically what we have is for
 a single individual example, rather, we have that loss is equal to negative log probability,
 where P here is kind of like thought of as a vector of all the probabilities. So at the y
 position, where y is the label. And we have that P here, of course, is the softmax. So the
 i component of P of this probability vector is just the softmax function. So raising all the
 logits, basically to the power of E and normalizing. So everything comes to one. Now if you write out
 P of y here, you can just write out the softmax. And then basically what we're interested in is
 we're interested in the derivative of the loss with respect to the i-th logit.
 And so basically it's a d by dli of this expression here, where we have l indexed with the specific
 label y. And on the bottom we have a sum over j of e to the lj and the negative log of all that.
 So potentially give it a shot pen and paper and see if you can actually derive the expression
 for the loss by dli. And then we're going to implement it here. Okay, so I am going to give away
 the result here. So this is some of the math I did to derive the gradients analytically. And so we
 see here that I'm just applying the rules of calculus from your first or second year of bachelor's
 degree if you took it. And we see that the expression is actually simplified quite a bit. You have to
 separate out the analysis in the case where the i-th index that you're interested in inside
 logits is either equal to the label or it's not equal to the label. And then the expression
 simplifies and cancels in a slightly different way. And what we end up with is something very,
 very simple. We either end up with basically p at i where p is again this vector of probabilities
 after a softmax or p at i minus one, where we just simply subtract to one. But in any case we just
 need to calculate the softmax p and then in the correct dimension we need to subtract to one.
 And that's the gradient, the form that it takes analytically. So let's implement this basically.
 And we have to keep in mind that this is only done for a single example. But here we are working
 with batches of examples. So we have to be careful of that. And then the loss for a batch is the
 average loss over all the examples. So in other words, it's the example for all the individual
 examples is the loss for each individual example summed up and then divided by n. And we have to
 back propagate through that as well and be careful with it. So d logits is going to be f dot softmax.
 PyTorch has a softmax function that you can call. And we want to apply the softmax on the logits
 and we want to go in the dimension that is one. So basically we want to do the softmax along the
 rows of these logits. Then at the correct positions we need to subtract a one. So d logits at
 iterating over all the rows and indexing into the columns provided by the correct labels inside yb.
 We need to subtract one. And then finally, it's the average loss that is the loss. And in the
 average there's a one over n of all the losses added up. And so we need to also back propagate
 through that division. So the gradient has to be scaled down by n as well, because of the mean.
 But this otherwise should be the result. So now if we verify this, we see that we don't get an
 exact match. But at the same time, the maximum difference from logits from PyTorch and our d
 logits here is on the order of 5e negative nine. So it's a tiny, tiny number. So because of loading
 point of onkiness, we don't get the exact bitwise result. But we basically get the correct answer
 approximately. Now I'd like to pause here briefly before we move on to the next exercise,
 because I'd like us to get an intuitive sense of what d logits is, because it has a beautiful and
 very simple explanation, honestly. So here I'm taking the logits and I'm visualizing it. And we
 can see that we have a batch of 32 examples of 27 characters. And what is the logits intuitively,
 right? D logits is the probabilities that the probabilities matrix in a forward pass. But then
 here, these black squares are the positions of the correct indices where we subtracted a one.
 And so what is this doing, right? These are the derivatives on d logits. And so let's look at
 just the first row here. So that's what I'm doing here. I'm calculating the probabilities of these
 logits, and then I'm taking just the first row. And this is the probability row. And then the
 logits of the first row and multiplying by n just for us so that we don't have the scaling
 by n in here, and everything is more interpretable. We see that it's exactly equal to the probability,
 of course, but then the position of the correct index has a minus equals one. So minus one on that
 position. And so notice that if you take the logits at zero, and you sum it, it actually
 sums to zero. And so you should think of these gradients here at each cell as like a force.
 We are going to be basically pulling down on the probabilities of the incorrect characters.
 And we're going to be pulling up on the probability at the correct index. And that's what's basically
 happening in each row. And the amount of push and pull is exactly equalized because the sum is zero.
 So the amount to which we pulled down on the probabilities and the demand that we push up on
 the probability of the correct character is equal. So it's sort of the repulsion and attraction are
 equal. And think of the neural mat now as a, as a like a massive pulley system or something like
 that, we're up here on top of the logits, and we're pulling up, we're pulling down the probabilities
 and correct and pulling up the property of the correct. And in this complicated pulley system,
 because everything is mathematically just determined, just think of it as sort of like
 this tension translating to this complicating pulley mechanism. And then eventually we get a tug
 on the weights and the biases. And basically in each update, we just kind of like tug in the
 direction that we like for each of these elements. And the parameters are slowly given in to the tug.
 And that's what training in neural mat kind of like looks like on a high level. And so I think the
 the forces of push and pull in these gradients are actually very intuitive here. We're pushing
 and pulling on the correct answer and the incorrect answers. And the amount of force that we're
 applying is actually proportional to the probabilities that came out in the forward pass. And so for
 example, if our probabilities came out exactly correct, so they would have had zero everywhere,
 except for one at the correct position, then the the logits would be all row of zeros for that
 example, there would be no push and pull. So the amount to which your prediction is incorrect
 is exactly the amount by which you're going to get a pull or push in that dimension.
 So if you have, for example, a very confidently mispredicted element here,
 then what's going to happen is that element is going to be pulled down very heavily. And the
 correct answer is going to be pulled up to the same amount. And the other characters are not going
 to be influenced too much. So the amount to which you mis-predict is then proportional to the
 strength of the pull. And that's happening independently in all the dimensions of this of this tensor.
 And it's sort of very intuitive and very used to think through. And that's basically the magic
 of the cross entropy loss and what is doing dynamically in the backward pass of the neural mat.
 So now we get to exercise number three, which is a very fun exercise, depending on your definition
 of fun. And we are going to do for batch normalization exactly what we did for cross entropy loss in
 exercise number two. That is we are going to consider it as a glued single mathematical expression
 and back propagate through it in a very efficient manner, because we are going to derive a much
 simpler formula for the backward pass of batch normalization. And we're going to do that using
 pen and paper. So previously we've broken up batch normalization into all of the intermediate
 pieces and all the atomic operations inside it. And then we back propagate it through it one by one.
 Now we just have a single sort of forward pass of a batch room. And it's all glued together.
 And we see that we get the exact same result as before. Now for the batch backward pass,
 we'd like to also implement a single formula basically for back propagating through this entire
 operation that is the batch normalization. So in the forward pass previously, we took
 H prebn, the hidden states of the pre-bacterialization and created H preact, which is the hidden states
 just before the activation. In the batch normalization paper, H prebn is X and H preact is Y.
 So in the backward pass, what we'd like to do now is we have D H preact and we'd like to produce
 D H prebn. And we'd like to do that in a very efficient manner. So that's the name of the game.
 Calculate the H prebn given the H preact. And for the purposes of this exercise, we're going to
 ignore gamma and beta and their derivatives because they take on a very simple form in a very similar
 way to what we did up above. So let's calculate this, given that right here. So to help you a
 little bit like I did before, I started off the implementation here on pen and paper. And I took
 two sheets of paper to derive the mathematical formulas for the backward pass. And basically,
 to set up the problem, just write out the mu sigma square variance, Xi hat and Yi exactly as in the
 paper except for the Bessel correction. And then in the backward pass, we have the derivative of
 the loss with respect to all the elements of Y. And remember that Y is a vector. There's there's
 multiple numbers here. So we have all the derivatives with respect to all the Ys. And then there's a
 dima in the beta. And this is kind of like the compute graph. The gamma and the beta, there's the
 X hat. And then the mu and the sigma squared and the X. So we have DL by D YI. And we won't DL by D Xi
 for all the I's in these vectors. So this is the compute graph. And you have to be careful because
 I'm trying to note here that these are vectors. There's many nodes here inside X, X hat and Y.
 But mu and sigma, sorry, sigma square are just individual scalars, single numbers. So you have
 to be careful with that. You have to imagine there's multiple nodes here, or you're going to get your
 math roll. So as an example, I would suggest that you go in the following order, one, two, three,
 four, in terms of the back propagation. So back propagating to X hat, then to sigma square,
 then into mu, and then into X. Just like an anthropological sort in micro grad, we would go
 from right to left. You're doing the exact same thing, except you're doing it with symbols and on
 piece of paper. So for number one, I'm not giving away too much. If you want DL of the XI hat,
 then we just take DL by D YI and multiply by gamma, because of this expression here,
 where any individual YI is just gamma times XI hat plus beta. So it doesn't help you too much
 there. But this gives you basically the derivatives for all the X hats. And so now,
 try to go through this computational graph and derive what is DL by D sigma square.
 And then what is DL by D mu, and then what is DL by D X eventually. So give it a go,
 and I'm going to be revealing the answer one piece at a time. Okay, so to get DL by D sigma square,
 we have to remember again, like I mentioned, that there are many Xs, X hats here. And remember
 that sigma square is just a single individual number here. So when we look at the expression
 for DL by D sigma square, we have that we have to actually consider all the possible paths that
 we basically have that there's many X hats, and they all feed off from the all depend on sigma
 square. So sigma square has a large fan out. There's lots of arrows coming out from sigma square
 into all the X hats. And then there's a back propagating signal from each X hat into sigma square. And
 that's why we actually need to sum over all those eyes from i equal to one to m of the DL by D Xi hat,
 which is the global gradient times the X i hat by D sigma square, which is the local gradient
 of this operation here. And then mathematically, I'm just working it out here, and I'm simplifying
 and you get a certain expression for DL by D sigma square. We're going to be using this
 expression when we back propagate into mu, and then eventually into X. So now let's continue our
 back propagation into mu. So what is DL by D mu? Now again, be careful that mu influences X hat,
 and X hat is actually lots of values. So for example, if our mini batch size is 32, as it is in our
 example that we were working on, then this is 32 numbers and 32 arrows going back to mu. And then
 mu going to sigma square is just a single arrow because sigma square is a scalar. So in total,
 there are 33 arrows emanating from you. And then all of them have gradients coming into mu,
 and they all need to be summed up. And so that's why when we look at the expression for DL by D mu,
 I am summing up over all the gradients of DL by D Xi hat times the Xi hat by D mu.
 So that's this arrow and the 32 arrows here. And then plus the one arrow from here, which is DL by D
 sigma square times the sigma square by D mu. So now we have to work out that expression,
 and let me just reveal the rest of it. Simplifying here is not complicated, the first term, and you
 just get an expression here. For the second term though, there's something really interesting that
 happens. When we look at the sigma square by D mu, and we simplify, at one point, if we assume
 that in a special case where mu is actually the average of Xi's, as it is in this case,
 then if we plug that in, then actually the gradient vanishes and becomes exactly zero.
 And that makes the entire second term cancel. And so these, if you just have a mathematical
 expression like this, and you look at D sigma square by D mu, you would get some mathematical
 formula for how mu impacts sigma square. But if it is the special case that mu is actually equal
 to the average, as it is in the case of rationalization, that gradient will actually vanish and become
 zero. So the whole term cancels, and we just get a fairly straightforward expression here for DL by
 D mu. Okay, and now we get to the craziest part, which is deriving DL by D Xi, which is ultimately
 what we're after. Now, let's count. First of all, how many numbers are there inside X? As I mentioned,
 there are 32 numbers. There are 32 little Xi's. And let's count the number of arrows emanating
 from each Xi. There's an arrow going to mu, an arrow going to sigma square. And then there's an
 arrow going to X hat. But this arrow here, let's group now that a little bit. Each Xi hat is just a
 function of Xi and all the other scalars. So Xi hat only depends on Xi and none of the other
 Xs. And so therefore, there are actually in this single arrow, there are 32 arrows. But those 32
 arrows are going exactly parallel. They don't interfere. They're just going parallel between
 X and X hat. You can look at it that way. And so how many arrows are emanating from each Xi? There
 are three arrows, mu sigma square, and the associated X hat. And so in back propagation, we now need
 to apply the chain rule. And we need to add up those three contributions. So here's what that
 looks like. If I just write that out, we have, we're going through, we're chaining through mu
 sigma square and through X hat. And those three terms are just here. Now we already have three of
 these. We have DL by DXI hat. We have DL by D mu, which we derived here. And we have DL by D
 sigma square, which we derived here. But we need three other terms here. This one, this one, and
 this one. So I invite you to try to derive them. It's not that complicated. You're just looking at
 these expressions here and differentiating with respect to Xi. So give it a shot, but here's the
 result. Or at least what I got. Yeah, I'm just, I'm just differentiating with respect to Xi for
 all of these expressions. And honestly, I don't think there's anything too tricky here. It's
 basic calculus. Now it gets a little bit more tricky is we are now going to plug everything
 together. So all of these terms multiplied with all of these terms and added up according to this
 formula. And that gets a little bit hairy. So what ends up happening is, you get a large expression.
 And the thing to be very careful with here, of course, is we are working with a DL by DXI for
 specific by here. But when we are plugging in some of these terms, like say, this term here,
 DL by DXI squared, you see how DL by DXI squared, I end up with an expression. And I'm iterating
 over little eyes here. But I can't use I as the variable when I plug in here, because this is a
 different eye from this eye. This eye here is just a place or a local variable for for a for loop
 in here. So here, when I plug that in, you notice that I rename the I to a J. Because I need to
 make sure that this J is not that this J is not this I, this J is like a little local iterator over 32
 terms. And so you should be careful with that when you're plugging in the expressions from here
 to here, you may have to rename eyes into J's. You have to be very careful what is actually an I
 with respect to the L by DXI. So some of these are J's, some of these are I's. And then we simplify
 this expression. And I guess like the big thing to notice here is a bunch of terms just gonna come
 out to the front and you can refactor them. There's a sigma squared plus epsilon raised to the power
 of negative three over two. This sigma squared plus epsilon can be actually separated out into
 three terms. Each of them are sigma squared plus epsilon to the negative one over two. So the three
 of them multiplied is equal to this. And then those three terms can go different places,
 because of the multiplication. So one of them actually comes out to the front and will end up
 here outside. One of them joins up with this term and one of them joins up with this other term.
 And then when you simplify the expression, you'll notice that some of these terms that are coming
 out are just the XI hats. So you can simplify just by rewriting that. And what we end up with at the
 end is a fairly simple mathematical expression over here that I cannot simplify further. But
 basically, you'll notice that it only uses the stuff we have and it derives the thing we need.
 So we have dl by dy for all the eyes. And those are used plenty of times here.
 And also in the show what we're using is these XI hats and XJ hats. And they just come from the
 forward pass. And otherwise, this is a simple expression and it gives us dl by d xi for all
 the eyes. And that's ultimately what we're interested in. So that's the end of a Batchnorm
 backward pass analytically. Let's now implement this final result. Okay, so I implemented the
 expression into a single line of code here. And you can see that the max diff is tiny. So this
 is the correct implementation of this formula. Now, I'll just basically tell you that getting
 this formula here from this mathematical expression was not trivial. And there's a lot going on
 packed into this one formula. And this is all exercised by itself. Because you have to consider
 the fact that this formula here is just for a single neuron and a batch of 32 examples. But
 what I'm doing here is I'm actually we actually have 64 neurons. And so this expression has to
 imperil evaluate the Batchnorm backward pass for all those 64 neurons in parallel independently.
 So this has to happen basically in every single column of the inputs here. And in addition to that,
 you see how there are a bunch of sums here. And we need to make sure that when I do those sums
 that they broadcast correctly onto everything else that's here. And so getting this expression is
 just like highly non trivial and I invite you to basically look through it and step through it.
 And it's a whole exercise to make sure that this checks out. But once all the shapes agree,
 and once you convince yourself that it's correct, you can also verify that patchers gets the exact
 same answer as well. And so that gives you a lot of peace of mind that this mathematical formula
 is correctly implemented here and broadcast it correctly and replicated in parallel for all the
 64 neurons inside this Batchnorm layer. Okay, and finally exercise number four asks you to put it
 all together. And here we have a redefinition of the entire problem. So you see that we reinstallize
 the neural net from scratch and everything. And then here, instead of calling a loss that backward,
 we want to have the manual back propagation here as we derived it up above. So go up, copy paste all
 the chunks of code that we've already derived, put them here and derive your own gradients,
 and then optimize this neural net, basically using your own gradients all the way to the
 calibration of the Batchnorm and the evaluation of the loss. And I was able to achieve quite a good
 loss, basically the same loss you would achieve before. And that shouldn't be surprising because
 all we've done is we've really gotten into loss that backward and we've pulled out all the code
 and inserted it here. But those gradients are identical and everything is identical and the
 result are identical. It's just that we have full visibility on exactly what goes on under the hood
 of a lot of backward in this specific case. Okay, and this is all of our code. This is the full
 backward pass using basically the simplified backward pass for the cross entropy loss and the
 Batchnormization. So back propagating through cross entropy, the second layer, the 10-H null
 linearity, the Batchnormization through the first layer and through the embedding. And so you see
 that this is only maybe what is this 20 lines of code or something like that. And that's what
 gives us gradients. And now we can potentially erase loss as backward. So the way I have the code
 set up is you should be able to run this entire cell once you fill this in. And this will run for
 only 100 iterations and then break. And it breaks because it gives you an opportunity to check your
 gradients against PyTorch. So here our gradients we see are not exactly equal. They are approximately
 equal and the differences are tiny, one in negative nine or so. And I don't exactly know where they're
 coming from, to be honest. So once we have some confidence that the gradients are basically correct,
 we can take out the gradient checking. We can disable this breaking statement. And then we can
 basically disable loss the backward. We don't need it anymore. It feels amazing to say that.
 And then here, when we are doing the update, we're not going to use p.grad. This is the old way
 of PyTorch. We don't have that anymore because we're not doing backward. We are going to use
 this update where we you see that I'm iterating over, I've arranged the grads to be in the same
 order as the parameters and I'm zipping them up the gradients and the parameters into p and grad.
 And then here I'm going to step with just the grad that we derived manually. So the last piece
 is that none of this now requires gradients from PyTorch. And so one thing you can do here
 is you can do with Torched.NoGrad and offset this whole code block. And really what you're saying
 is you're telling PyTorch that, "Hey, I'm not going to call backward on any of this." And this
 last PyTorch to be a bit more efficient with all of it. And then we should be able to just run this
 and it's running. And you see that loss of backward is commented out and we're optimizing.
 So we're going to leave this run and hopefully we get a good result. Okay, so I allowed the
 neural out to finish optimization. Then here I calibrated the batch on parameters because I did
 not keep track of the running mean and variance in their training loop. Then here I ran the loss
 and you see that we actually obtained a pretty good loss very similar to what we've achieved
 before. And then here I'm sampling from the model and we see some of the name like gibberish
 that we're sort of used to. So basically the model worked and samples for decent results
 compared to what we were used to. So everything is the same but of course the big deal is that we
 did not use lots of backward. We did not use PyTorch autograd and we estimated our gradients
 ourselves by hand. And so hopefully you're looking at this, the backward pass of this neural net
 and you're thinking to yourself, actually that's not too complicated. Each one of these layers is
 like three lines of code or something like that. And most of it is fairly straightforward,
 potentially with the notable exception of the batch normalization backward pass. Otherwise,
 it's pretty good. Okay, and that's everything I wanted to cover for this lecture. So hopefully
 you found this interesting. And what I liked about it honestly is that it gave us a very nice
 diversity of layers to back propagate through. And I think it gives a pretty nice and comprehensive
 sense of how these backward passes are implemented and how they work. And you'd be able to derive
 them yourself. But of course in practice, you probably don't want to and you want to use the
 PyTorch autograd. But hopefully you have some intuition about how gradients flow backwards
 through the neural net, starting at the loss and how they flow through all the variables and
 all the intermediate results. And if you understood a good chunk of it and if you have a sense of that,
 then you can count yourself as one of these buff doggies on the left, instead of the
 doggies on the right here. Now in the next lecture, we're actually going to go to
 recurrent neural nets, LSTMs, and all the other variants of ARNIS. And we're going to start to
 complexify the architecture and start to achieve better log likelihoods. And so I'm really looking
 forward to that. And I'll see you then.
